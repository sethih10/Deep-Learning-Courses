{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a381d60a-b356-43b6-85ae-3ddac689c47b",
   "metadata": {},
   "source": [
    "## Assignment 7: Exercise 1\n",
    "\n",
    "Deadline 22.05.2025 at 00.00\n",
    "\n",
    "### Text-to-speech (TTS) synthesis with Tacotron 2 and HiFi-GAN\n",
    "\n",
    "In this exercise we duplicate parts of a pre-trained TTS system based on Tacotron 2 and HiFi-GAN. The implementation needs to be compatible with the original implementation and referencing the original source code is advised. Links will be provided where applicable.\n",
    "\n",
    "The intended learning outcomes are to understand the architectures at a high level, navigate complex working codebases, and re-implement the crucial parts related to attention and autoregressive sampling.\n",
    "\n",
    "## Before the Assignment\n",
    "\n",
    "Please give a brief read to the following papers:\n",
    "\n",
    "- [Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions](https://arxiv.org/abs/1712.05884)  \n",
    "  This paper introduces **Tacotron 2**, a neural sequence-to-sequence model that predicts mel-scale spectrograms from text and then uses a modified WaveNet vocoder to synthesize time-domain waveforms.\n",
    "\n",
    "- [HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis](https://arxiv.org/abs/2010.05646)  \n",
    "  HiFi-GAN is a GAN-based neural vocoder designed to convert Mel spectrograms into raw audio waveforms with both high fidelity and real-time efficiency. We will use HiFi_GAN (instead of the original WaveNet vocoder) to synthesize time-domain waveforms from the mel spectrograms produced by the Tacotron2 model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be0bb71-3dda-4998-9404-740026592d73",
   "metadata": {},
   "source": [
    "### Install text-processing libraries -- Asenna tekstinkäsittelykirjastot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2dc1ee-8b33-4fdb-a2de-382d9d0efadd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install unidecode\n",
    "%pip install inflect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc539ed-6ff4-4d9a-a566-5049b9adc3ca",
   "metadata": {},
   "source": [
    "### Importing Tacotron 2 and HiFi-GAN\n",
    "\n",
    "Let's clone the [Tacotron 2](https://github.com/NVIDIA/tacotron2.git) and [HiFi-GAN](https://github.com/jik876/hifi-gan.git) repositories from github. We will also load the pre-trained model weights for the tacotron model from a dedicated [Aalto Gitlab repository](https://version.aalto.fi/gitlab/speech-synthesis-releases/tacotron-models.git). The rest of the section is mostly utility code to manage non-packaged python code. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603fcd82-e986-40df-ae0c-bdfbefad227e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "import IPython.display as ipd\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# HERE WE LOAD THE ARCHITECTURE OF THE TACOTRON MODEL\n",
    "if not os.path.isdir('tacotron2'):\n",
    "    !git clone https://github.com/NVIDIA/tacotron2.git\n",
    "    !touch tacotron2/__init__.py\n",
    "    pass\n",
    "\n",
    "# HERE WE LOAD THE ARCHITECTURE OF THE HIFI GAN MODEL\n",
    "if not os.path.isdir('hifi_gan'):\n",
    "    !git clone https://github.com/jik876/hifi-gan.git\n",
    "    !mv hifi-gan hifi_gan\n",
    "    !touch hifi_gan/__init__.py\n",
    "    pass\n",
    "\n",
    "# HERE WE LOAD THE WEIGHTS OF THE PRETRAINED MODELS\n",
    "if not os.path.isdir('tacotron-models'):\n",
    "    !git clone https://version.aalto.fi/gitlab/speech-synthesis-releases/tacotron-models.git\n",
    "    !touch tacotron2/__init__.py\n",
    "    pass\n",
    "\n",
    "class add_path():\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "\n",
    "    def __enter__(self):\n",
    "        sys.path.insert(0, self.path)\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        try:\n",
    "            sys.path.remove(self.path)\n",
    "        except ValueError:\n",
    "            pass\n",
    "        \n",
    "with add_path(os.path.realpath('.')):\n",
    "\n",
    "    print(\"Working dir:\", os.getcwd())\n",
    "    print(\"Contents:\", os.listdir('./tacotron2'))\n",
    "    \n",
    "    from tacotron2.model import *\n",
    "    from tacotron2.layers import *\n",
    "    from tacotron2.text import *\n",
    "\n",
    "def load_model(hparams):\n",
    "    model = Tacotron2(hparams)\n",
    "    return model\n",
    "\n",
    "def plot_data(data, figsize=(16, 4)):\n",
    "    fig, axes = plt.subplots(1, len(data), figsize=figsize)\n",
    "    for i in range(len(data)):\n",
    "        axes[i].imshow(data[i], aspect='auto', origin='lower', \n",
    "                       interpolation='none')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a65396-771a-4484-aaf3-6173aee15351",
   "metadata": {},
   "source": [
    "### Tacotron 2 configuration parameters\n",
    "\n",
    "The original code uses a deprecated Tensorflow class for storing hyperparameters. Luckily, the functionality is identical to a standard python dataclass. This also gives us a chance to look at all the Tacotron 2 configuration parameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0b1270-9d22-4a9a-bb9b-581e43648c41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hyperparameter dataclass\n",
    "from dataclasses import dataclass\n",
    "n_symbols = len(symbols)\n",
    "\n",
    "@dataclass\n",
    "class HParams:\n",
    "    ################################\n",
    "    # Experiment Parameters        #\n",
    "    ################################\n",
    "    dynamic_loss_scaling=True\n",
    "    fp16_run=False\n",
    "    distributed_run=False\n",
    "    dist_backend=\"nccl\"\n",
    "    dist_url=\"tcp://localhost:54321\"\n",
    "    cudnn_enabled=True\n",
    "    cudnn_benchmark=False\n",
    "    ignore_layers=['embedding.weight']\n",
    "\n",
    "    ################################\n",
    "    # Data Parameters             #\n",
    "    ################################\n",
    "    load_mel_from_disk=False,\n",
    "    training_files='filelists/ljs_audio_text_train_filelist.txt'\n",
    "    validation_files='filelists/ljs_audio_text_val_filelist.txt'\n",
    "    text_cleaners=['english_cleaners']\n",
    "\n",
    "    ################################\n",
    "    # Audio Parameters             #\n",
    "    ################################\n",
    "    max_wav_value=32768.0\n",
    "    sampling_rate=22050\n",
    "    filter_length=1024\n",
    "    hop_length=256\n",
    "    win_length=1024\n",
    "    n_mel_channels=80\n",
    "    mel_fmin=0.0\n",
    "    mel_fmax=8000.0\n",
    "\n",
    "    ################################\n",
    "    # Model Parameters             #\n",
    "    ################################\n",
    "    n_symbols=n_symbols\n",
    "    symbols_embedding_dim=512\n",
    "\n",
    "    # Encoder parameters\n",
    "    encoder_kernel_size=5\n",
    "    encoder_n_convolutions=3\n",
    "    encoder_embedding_dim=512\n",
    "\n",
    "    # Decoder parameters\n",
    "    n_frames_per_step=1  # currently only 1 is supported\n",
    "    decoder_rnn_dim=1024\n",
    "    prenet_dim=256\n",
    "    max_decoder_steps=1000\n",
    "    gate_threshold=0.5\n",
    "    p_attention_dropout=0.1\n",
    "    p_decoder_dropout=0.1\n",
    "\n",
    "    # Attention parameters\n",
    "    attention_rnn_dim=1024\n",
    "    attention_dim=128\n",
    "\n",
    "    # Location Layer parameters\n",
    "    attention_location_n_filters=32\n",
    "    attention_location_kernel_size=31\n",
    "\n",
    "    # Mel-post processing network parameters\n",
    "    postnet_embedding_dim=512\n",
    "    postnet_kernel_size=5\n",
    "    postnet_n_convolutions=5\n",
    "\n",
    "    ################################\n",
    "    # Optimization Hyperparameters #\n",
    "    ################################\n",
    "    use_saved_learning_rate=False\n",
    "    learning_rate=1e-3\n",
    "    weight_decay=1e-6\n",
    "    grad_clip_thresh=1.0\n",
    "    batch_size=64\n",
    "    mask_padding=True  # set model's padded outputs to padded values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b7c28e-2d8a-4f65-a561-d954ff7c1601",
   "metadata": {},
   "source": [
    "### Loading a pre-trained Tacotron 2 model\n",
    "\n",
    "We load the weights of a single speaker model pre-trained on the [LJSpeech dataset](https://pytorch.org/audio/main/generated/torchaudio.datasets.LJSPEECH.html) from the weights directory we downloaded before.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea03f158-cdce-47d8-813a-3e53d6aca2d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hparams = HParams()\n",
    "hparams.sampling_rate = 22050\n",
    "\n",
    "checkpoint_path = os.path.join(os.getcwd(), \"tacotron-models/tacotron-models/tacotron2_statedict.pt\") \n",
    "tacotron = load_model(hparams)\n",
    "state_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "tacotron.load_state_dict(state_dict['state_dict'])\n",
    "\n",
    "# DISPLAY THE ARCHITECTURE OF THE MODEL\n",
    "tacotron.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c098cfb-ccaa-4283-822b-ed589641ab6e",
   "metadata": {},
   "source": [
    "### Implementing the Tacotron 2 inference functions\n",
    "\n",
    "In this section your task is to implement the inference function. You can and should reference the [original implementation](https://github.com/NVIDIA/tacotron2/blob/185cd24e046cc1304b4f8e564734d2498c6e2e6f/model.py) on GitHub.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ab27bb-d869-45dd-ae87-0c22b1562ccc",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "42a6ba79d8622b956d5f71fdb0e8e63f",
     "grade": false,
     "grade_id": "cell-1134800988b3bddc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def attention_forward(self, attention_hidden_state, memory, processed_memory,\n",
    "                    attention_weights_cat, mask):\n",
    "    \"\"\"\n",
    "    PARAMS\n",
    "    ------\n",
    "    attention_hidden_state: attention rnn last output\n",
    "    memory: encoder outputs\n",
    "    processed_memory: processed encoder outputs\n",
    "    attention_weights_cat: previous and cumulative attention weights\n",
    "    mask: binary mask for padded data\n",
    "    \"\"\"\n",
    "    \n",
    "    alignment = self.get_alignment_energies(\n",
    "        attention_hidden_state, processed_memory, attention_weights_cat)\n",
    "\n",
    "    if mask is not None:\n",
    "        alignment.data.masked_fill_(mask, self.score_mask_value)\n",
    "        \n",
    "    # Get attention weights by normalising alignment with softmax\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    # Apply matrix multiplication between attention weights and memory\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return attention_context, attention_weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb479459",
   "metadata": {},
   "source": [
    "### Testing the Attention implementation (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495f2301-43c8-4ebd-b0c4-70b8c6614af2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dc1938359ba167513af5295d1198141f",
     "grade": true,
     "grade_id": "cell-7bf4feab74984c2c",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Attention.forward = attention_forward\n",
    "\n",
    "attention_layer = Attention(\n",
    "            hparams.attention_rnn_dim, hparams.encoder_embedding_dim,\n",
    "            hparams.attention_dim, hparams.attention_location_n_filters,\n",
    "            hparams.attention_location_kernel_size)\n",
    "\n",
    "batch_size = 1\n",
    "num_characters = 44\n",
    "\n",
    "attention_hidden_state = torch.randn(batch_size, hparams.attention_rnn_dim) # (1, 1024)\n",
    "memory = torch.randn(batch_size, num_characters, hparams.encoder_embedding_dim) # (1, 44, 512)\n",
    "processed_memory = torch.randn(batch_size, num_characters,  hparams.attention_dim) # (1, 44, 128)\n",
    "attention_weights_cat = torch.randn(batch_size, 2, num_characters) # (1, 2, 44)\n",
    "\n",
    "mask = None\n",
    "\n",
    "attention_context, attention_weights = attention_layer(attention_hidden_state, memory,\n",
    "                                                       processed_memory, attention_weights_cat, mask)\n",
    "\n",
    "assert attention_context.shape == (batch_size, hparams.encoder_embedding_dim), \\\n",
    "    print(f\"Expected shape {(batch_size, hparams.encoder_embedding_dim)}, but got {attention_context.shape}\")\n",
    "assert attention_weights.shape == (batch_size, num_characters), \\\n",
    "    print(f\"Expected shape {(batch_size, num_characters)}, but got {attention_weights.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27a094f",
   "metadata": {},
   "source": [
    "### Decoding function\n",
    "\n",
    "Let's look a the Tacotron 2 decoding function. We will call this function repeatedly to generate the mel spectrogram frame by frame.\n",
    "\n",
    "No changes required for this section.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89e4960-13df-4c04-b999-07e7c208b8a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def decoder_decode(self, decoder_input):\n",
    "    \"\"\" Decoder step using stored states, attention and memory\n",
    "    PARAMS\n",
    "    ------\n",
    "    decoder_input: previous mel output\n",
    "\n",
    "    RETURNS\n",
    "    -------\n",
    "    mel_output:\n",
    "    gate_output: gate output energies\n",
    "    attention_weights:\n",
    "    \"\"\"\n",
    "    cell_input = torch.cat((decoder_input, self.attention_context), -1)\n",
    "    self.attention_hidden, self.attention_cell = self.attention_rnn(\n",
    "        cell_input, (self.attention_hidden, self.attention_cell))\n",
    "    self.attention_hidden = F.dropout(\n",
    "        self.attention_hidden, self.p_attention_dropout, self.training)\n",
    "\n",
    "    attention_weights_cat = torch.cat(\n",
    "        (self.attention_weights.unsqueeze(1),\n",
    "         self.attention_weights_cum.unsqueeze(1)), dim=1)\n",
    "    self.attention_context, self.attention_weights = self.attention_layer(\n",
    "        self.attention_hidden, self.memory, self.processed_memory,\n",
    "        attention_weights_cat, self.mask)\n",
    "\n",
    "    self.attention_weights_cum += self.attention_weights\n",
    "    decoder_input = torch.cat(\n",
    "        (self.attention_hidden, self.attention_context), -1)\n",
    "    self.decoder_hidden, self.decoder_cell = self.decoder_rnn(\n",
    "        decoder_input, (self.decoder_hidden, self.decoder_cell))\n",
    "    self.decoder_hidden = F.dropout(\n",
    "        self.decoder_hidden, self.p_decoder_dropout, self.training)\n",
    "\n",
    "    decoder_hidden_attention_context = torch.cat(\n",
    "        (self.decoder_hidden, self.attention_context), dim=1)\n",
    "    decoder_output = self.linear_projection(\n",
    "        decoder_hidden_attention_context)\n",
    "\n",
    "    gate_prediction = self.gate_layer(decoder_hidden_attention_context)\n",
    "    return decoder_output, gate_prediction, self.attention_weights\n",
    "\n",
    "# add the new method to the Decoder class\n",
    "Decoder.decode = decoder_decode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a385e2d",
   "metadata": {},
   "source": [
    "### Decoder inference function\n",
    "\n",
    "In this section we implement the decoder inference function. This function will call the decoder repeatedly to generate the mel spectrogram frame by frame.\n",
    "\n",
    "Check the original source code for reference.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dde992-0f7f-4fdf-89a1-e1180871be62",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6e2e1ed9493aafd183cb781fcc80ef25",
     "grade": false,
     "grade_id": "cell-a3a6f2e861546be6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def decoder_inference(self, memory):\n",
    "        \"\"\" Decoder inference\n",
    "        PARAMS\n",
    "        ------\n",
    "        memory: Encoder outputs\n",
    "\n",
    "        RETURNS\n",
    "        -------\n",
    "        mel_outputs: mel outputs from the decoder\n",
    "        gate_outputs: gate outputs from the decoder\n",
    "        alignments: sequence of attention weights from the decoder\n",
    "        \"\"\"\n",
    "        decoder_input = self.get_go_frame(memory)\n",
    "\n",
    "        self.initialize_decoder_states(memory, mask=None)\n",
    "\n",
    "        mel_outputs, gate_outputs, alignments = [], [], []\n",
    "        # run the decoder loop, generate mel outputs one frame at a time\n",
    "        while True:\n",
    "            decoder_input = self.prenet(decoder_input)\n",
    "            mel_output, gate_output, alignment = self.decode(decoder_input)\n",
    "\n",
    "            # collect outputs in lists\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "\n",
    "            # check stopping criterion and break if needed (use self.gate_threshold)\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "\n",
    "            # stop if max decoder steps is reached (use self.max_decoder_steps)\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "\n",
    "            decoder_input = mel_output\n",
    "\n",
    "        # convert lists to tensors\n",
    "        mel_outputs, gate_outputs, alignments = self.parse_decoder_outputs(\n",
    "            mel_outputs, gate_outputs, alignments)\n",
    "        \n",
    "        return mel_outputs, gate_outputs, alignments\n",
    "    \n",
    "# add the new method to the Decoder class\n",
    "Decoder.inference = decoder_inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e13fdd",
   "metadata": {},
   "source": [
    "### Tacotron2 whole model inference\n",
    "\n",
    "In this section we implement infrence for the whole Tacotron 2 model. Encoder works in a single pass for the entire sequence, but the decoder needs to be called repeatedly to generate the mel spectrogram frame by frame.\n",
    "\n",
    "Reference the original source code for the Tacotron 2 model for implementation details.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e755ae22-85d8-40e0-82ab-9be36e88edb0",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579c7862-23ef-4c11-a06d-327bbedc3fa2",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c244f1511088894fc1415acc02c73aa8",
     "grade": false,
     "grade_id": "cell-c63d1a020eacc303",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def inference(self, inputs):\n",
    "    \"\"\" \n",
    "    Args:\n",
    "        inputs: (batch, n_chars)\n",
    "\n",
    "    Returns:\n",
    "        mel_outputs: (batch, n_mel_channels, T_out)\n",
    "        mel_outputs_postnet: (batch, n_mel_channels, T_out)\n",
    "        gate_outputs: (batch, T_out)\n",
    "        alignments: (batch, T_out, n_chars)\n",
    "        embedded_inputs: (batch, n_chars, symbols_embedding_dim)\n",
    "        encoder_outputs: (batch, n_chars, encoder_embedding_dim)\n",
    "\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return mel_outputs, mel_outputs_postnet, gate_outputs, alignments, embedded_inputs, encoder_outputs \n",
    "\n",
    "# set custom forward function\n",
    "Tacotron2.inference = inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84be73e-ee1a-420f-9f76-602f12533193",
   "metadata": {},
   "source": [
    "### Mel-spectrogram synthesis and attention alignment\n",
    "\n",
    "Let's synthesize an utterance and inspect the the mel-spectrogram and attention plots. We will use a waveform synthesis model later to actually listen to the output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b801439b-b6b8-4701-9bc4-463a67828d8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from tacotron2.text import text_to_sequence   # adjust import if you changed paths\n",
    "\n",
    "def plot_data(data, figsize=(16, 4)):\n",
    "    fig, axes = plt.subplots(1, len(data), figsize=figsize)\n",
    "    for i, ax in enumerate(axes):\n",
    "        im = ax.imshow(data[i],\n",
    "                       aspect='auto',\n",
    "                       origin='lower',\n",
    "                       interpolation='none')\n",
    "        if i == 0:\n",
    "            ax.set_title(\"Mel Spectrogram\")\n",
    "            ax.set_xlabel(\"Audio Frames\")\n",
    "            ax.set_ylabel(\"Filterbank Band Index\")\n",
    "        else:\n",
    "            ax.set_title(\"Alignment\")\n",
    "            ax.set_xlabel(\"Audio Frames\")\n",
    "            ax.set_ylabel(\"Character Position\")\n",
    "            ax.yaxis.tick_right()\n",
    "            ax.yaxis.set_label_position(\"right\")\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "text     = \"The quick brown fox jumps over the lazy dog.\"\n",
    "seq      = np.array(text_to_sequence(text, ['english_cleaners']))[None, :]\n",
    "sequence = torch.LongTensor(seq)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "mel_outputs, mel_outputs_postnet, gate_outputs, alignments, embedded_inputs, encoder_outputs = \\\n",
    "    tacotron.inference(sequence)\n",
    "\n",
    "fig = plot_data((\n",
    "    mel_outputs[0].detach().cpu().numpy(),\n",
    "    alignments[0].detach().cpu().numpy().T\n",
    "))\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5aef43",
   "metadata": {},
   "source": [
    "### Checking that the model output is correct (3 sections, 1 point each)\n",
    "\n",
    "The following code checks that the model output is correctly shaped.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761aac96-880d-4bec-916e-eacedd8a6684",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7e62d47a0b6f92ba80fd03e5e4f8fbfa",
     "grade": true,
     "grade_id": "cell-f75e80821cdd089a",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "mel_outputs, mel_outputs_postnet, gate_outputs, alignments, embedded_inputs, encoder_outputs  = tacotron.inference(sequence)\n",
    "\n",
    "n_frames = mel_outputs.shape[2]\n",
    "n_chars = sequence.shape[1]\n",
    "batch_size = 1\n",
    "\n",
    "print(f\"Mel outputs shape {mel_outputs.shape}\")\n",
    "assert mel_outputs.shape[0] == batch_size, \\\n",
    "    f\"Expected shape {batch_size}, but got {mel_outputs.shape[0]}\"\n",
    "assert mel_outputs.shape[1] == hparams.n_mel_channels, \\\n",
    "    f\"Expected shape {hparams.n_mel_channels}, but got {mel_outputs.shape[1]}\"\n",
    "\n",
    "print(\"Tests ok\")\n",
    "\n",
    "print(f\"Mel outputs postnet shape {mel_outputs_postnet.shape}\")\n",
    "assert mel_outputs_postnet.shape[0] == batch_size, \\\n",
    "    f\"Expected shape {batch_size}, but got {mel_outputs_postnet.shape[0]}\"\n",
    "assert mel_outputs_postnet.shape[1] == hparams.n_mel_channels, \\\n",
    "    f\"Expected shape {hparams.n_mel_channels}, but got {mel_outputs_postnet.shape[1]}\"\n",
    "assert mel_outputs_postnet.shape[2] == n_frames, \\\n",
    "    f\"Expected shape {n_frames}, but got {mel_outputs_postnet.shape[2]}\"\n",
    "print(\"Tests ok\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d90f608-b343-481d-8e98-260ee46bdbd6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5ed53a388ce8e498055e16b45e3f90d2",
     "grade": true,
     "grade_id": "cell-17e5399e4e9b9b34",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "mel_outputs, mel_outputs_postnet, gate_outputs, alignments, embedded_inputs, encoder_outputs  = tacotron.inference(sequence)\n",
    "\n",
    "n_frames = mel_outputs.shape[2]\n",
    "n_chars = sequence.shape[1]\n",
    "batch_size = 1\n",
    "\n",
    "print(f\"Gate outputs shape {gate_outputs.shape}\")\n",
    "assert gate_outputs.shape[0] == batch_size, \\\n",
    "    f\"Expected shape {batch_size}, but got {gate_outputs.shape[0]}\"\n",
    "assert gate_outputs.shape[1] == n_frames, \\\n",
    "    f\"Expected shape {n_frames}, but got {gate_outputs.shape[1]}\"\n",
    "print(\"Tests ok\")\n",
    "\n",
    "print(f\"Alignments shape {alignments.shape}\")\n",
    "assert alignments.shape[0] == batch_size, \\\n",
    "    f\"Expected shape {batch_size}, but got {alignments.shape[0]}\"\n",
    "assert alignments.shape[1] == n_frames, \\\n",
    "    f\"Expected shape {n_frames}, but got {alignments.shape[1]}\"\n",
    "assert alignments.shape[2] == n_chars, \\\n",
    "    f\"Expected shape {n_chars}, but got {alignments.shape[2]}\"\n",
    "print(\"Tests ok\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1689bf9-2434-49f2-93d4-73b60bf3165e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "544f5ebaacd524a7c43261a13f2a4e17",
     "grade": true,
     "grade_id": "cell-c6c9fae9196b084c",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "mel_outputs, mel_outputs_postnet, gate_outputs, alignments, embedded_inputs, encoder_outputs  = tacotron.inference(sequence)\n",
    "\n",
    "n_frames = mel_outputs.shape[2]\n",
    "n_chars = sequence.shape[1]\n",
    "batch_size = 1\n",
    "\n",
    "print(f\"Embedded inputs shape {embedded_inputs.shape}\")\n",
    "assert embedded_inputs.shape[0] == batch_size\n",
    "assert embedded_inputs.shape[1] == hparams.symbols_embedding_dim\n",
    "assert embedded_inputs.shape[2] == n_chars\n",
    "print(\"Tests ok\")\n",
    "\n",
    "print(f\"Encoder outputs shape {encoder_outputs.shape}\")\n",
    "assert encoder_outputs.shape[0] == batch_size\n",
    "assert encoder_outputs.shape[1] == n_chars\n",
    "assert encoder_outputs.shape[2] == hparams.encoder_embedding_dim\n",
    "print(\"Tests ok\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8488645b-c8bf-494f-94f5-608a7e4cee5d",
   "metadata": {},
   "source": [
    "### HiFi-GAN waveform generator\n",
    "\n",
    "Let's load a HiFi-GAN neural vocoder model. This model is speaker-specific, it has been trained on the LJSpeech dataset and fine-tuned use with Tacotron 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81406910-fa4d-42a8-8e63-c040a644fad6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "matplotlib_backend = matplotlib.get_backend()\n",
    "\n",
    "\n",
    "print(sys.path)\n",
    "if 'utils' in sys.modules:  \n",
    "    del sys.modules[\"utils\"]\n",
    "with add_path(os.path.realpath('./hifi_gan')):\n",
    "    hifi_gan = __import__('hifi_gan')\n",
    "    from hifi_gan.models import Generator as HiFiGAN\n",
    "    from hifi_gan.inference import load_checkpoint\n",
    "    from hifi_gan.env import AttrDict\n",
    "\n",
    "matplotlib.use(matplotlib_backend)\n",
    "\n",
    "\n",
    "model_dir = os.path.join(os.getcwd(), 'tacotron-models/hifigan-models/LJ_FT_T2_V3/')\n",
    "model_file = os.path.join(model_dir, 'generator_v3')\n",
    "config_file = os.path.join(model_dir, 'config.json')\n",
    "\n",
    "state_dict = load_checkpoint(model_file, device='cpu')\n",
    "\n",
    "with open(config_file) as f:\n",
    "    data = f.read()\n",
    "\n",
    "json_config = json.loads(data)\n",
    "h = AttrDict(json_config)\n",
    "\n",
    "print(state_dict.keys())\n",
    "\n",
    "hifigan = HiFiGAN(h)\n",
    "hifigan.load_state_dict(state_dict['generator'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605a95c3-bf9f-4f1f-ab59-98aa5f2891cd",
   "metadata": {},
   "source": [
    "### HiFi-GAN generator forward pass\n",
    "\n",
    "The HiFi-GAN generator model takes a frame-rate mel spectrogram and progressively upsamples it to a waveform at an audio sample rate.\n",
    "\n",
    "In this section, your task is to implement the forward pass for the generator. In addition to the original functionality, you should collect the intermediate upsampled representations and return them in a list.\n",
    "\n",
    "The forward pass needs to be compatible with the [original implementation](\n",
    "https://github.com/jik876/hifi-gan/blob/4769534d45265d52a904b850da5a622601885777/models.py#L75).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd67db1-3aa7-4c6c-9e28-3e7fee7f5e30",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5dbc9a21c55686224263a021726c2c7c",
     "grade": false,
     "grade_id": "cell-7aaf78134ac9ee8e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "LRELU_SLOPE = 0.1\n",
    "def hifigan_gen_forward(self, x):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: (batch, n_mels, n_frames)\n",
    "\n",
    "    Returns:\n",
    "        x: (batch, 1, n_samples)\n",
    "        x_resolution_outputs: list of output tensors at each temporal resolution\n",
    "    \"\"\"\n",
    "    x_resolution_outputs = []\n",
    "    # Hint: add a way of storing the intermediate representations here\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return x, x_resolution_outputs\n",
    "\n",
    "HiFiGAN.forward = hifigan_gen_forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01a4b12-929f-4320-a941-90c62bddaf0e",
   "metadata": {},
   "source": [
    "### Test for correct sizes and listen to the sample (2 points)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09510c45-3812-449a-8ef9-52ce702a3602",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "71f27b42348dfd51467ec810acd7fbf4",
     "grade": true,
     "grade_id": "cell-6ddc6a54cf52a366",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    mel_outputs, mel_outputs_postnet, stop_gate_outputs, alignments, _, _ = tacotron.inference(sequence)\n",
    "    waveform, resolution_outputs = hifigan(mel_outputs)\n",
    "\n",
    "upsample_rates = hifigan.h.upsample_rates\n",
    "\n",
    "n_samples = waveform.shape[-1]\n",
    "n_frames = mel_outputs.shape[-1]\n",
    "\n",
    "assert n_samples == n_frames * hparams.hop_length\n",
    "\n",
    "upsample_rate_total = 1\n",
    "for x, upsample_rate in zip(resolution_outputs, upsample_rates):\n",
    "    upsample_rate_total *= upsample_rate\n",
    "\n",
    "    x_samples = x.shape[-1]\n",
    "    assert x_samples == n_frames * upsample_rate_total, \\\n",
    "        f\"Expected {n_frames * upsample_rate_total} samples, got {x_samples} samples\"\n",
    "\n",
    "import IPython\n",
    "IPython.display.Audio(waveform.squeeze() ,rate=22050)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b0e892-d75c-4363-9dc7-ec04cca9b0c1",
   "metadata": {},
   "source": [
    "### HiFi-GAN Upsampling stages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd58226-f46a-46e7-910b-b22f443331f0",
   "metadata": {},
   "source": [
    "To better understand what is going on in each step of the forward pass of the HiFi-GAN Vocoder we take a listen to the mean of the intermediate feature maps that we collected after each upsampling block. Let's take a function that uses the resolution_outputs array and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0e4711-d7bf-4944-8ff2-0c127986683d",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8b7996403ac2d0b9088fd9e4f6b5be36",
     "grade": false,
     "grade_id": "cell-8f4b933d9e0af35a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def get_mean_intermediate_representation(idx):\n",
    "    \n",
    "    # Hint: use resolution_outputs that you returned in the \"hifi_gan_forward\" function\n",
    "    # Hint: return the audio file as a numpy array with 1 dimension, apply the correct transformations\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    mean_np = mean_waveform.detach().cpu().numpy()\n",
    "    print(\"Final NumPy array shape:\", mean_np.shape)\n",
    "    \n",
    "    return mean_np\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f66e55a-bad1-45d0-94ad-aa4e92df6b0c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "29baadeed2faac5b626f59d07cf7fb14",
     "grade": true,
     "grade_id": "cell-a94d3d7e9ca81d85",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "num_res = len(resolution_outputs)\n",
    "print(f\"Found {num_res} upsampling stages in HiFi-GAN\\n\")\n",
    "\n",
    "for res_idx in range(num_res):\n",
    "    print(f\"--- Stage {res_idx+1} of {num_res} ---\")\n",
    "\n",
    "    mean_waveform = get_mean_intermediate_representation(res_idx)\n",
    "\n",
    "    assert isinstance(mean_waveform, np.ndarray), f\"Expected a NumPy array, got {type(mean_waveform)}\"\n",
    "    assert mean_waveform.ndim == 1, f\"Expected 1D array, got {mean_waveform.ndim}D\"\n",
    "    \n",
    "    \n",
    "    expected_len = resolution_outputs[res_idx].shape[-1]\n",
    "    \n",
    "    assert mean_waveform.shape[0] == expected_len, f\"Expected length {expected_len}, got {mean_waveform.shape[0]}\"\n",
    "    assert np.all(np.isfinite(mean_waveform)), \"Found non-finite values\"\n",
    "    assert np.max(np.abs(mean_waveform)) <= 1.0, \"Amplitude out of range [-1, 1]\"\n",
    "\n",
    "    print(\"▶️ Playing averaged audio at this stage...\\n\")\n",
    "    display(Audio(mean_waveform, rate=22050))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383e606e-3474-40e6-afc8-1b6e0ea64e57",
   "metadata": {},
   "source": [
    "### Experiment with the synthesiser\n",
    "\n",
    "This section is purely for playing with the synthesizer\n",
    "\n",
    "- Try tongue twisters (https://www.ef.com/wwen/english-resources/tongue-twisters-english/)\n",
    "- Try multiple runs on the same prompt\n",
    "- Try to break the attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d756c610-538f-49bb-a25b-1084755b561c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "text = \"Potato\"\n",
    "\n",
    "sequence = np.array(text_to_sequence(text, ['english_cleaners']))[None, :]\n",
    "sequence = torch.Tensor(sequence).long()\n",
    "\n",
    "with torch.no_grad():\n",
    "    mel_outputs, mel_outputs_postnet, stop_gate_outputs, alignments, _, _ = tacotron.inference(sequence)\n",
    "    waveform, resolution_outputs = hifigan(mel_outputs)\n",
    "\n",
    "IPython.display.Audio(waveform.squeeze(), rate=22050)\n",
    "\n",
    "### Why does this happen??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8299e0-2607-4d0f-8385-fcd5dd90dec5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot generated mel-spectrogram and attention alignment\n",
    "fig = plot_data((mel_outputs.float().data.cpu().numpy()[0],\n",
    "           alignments.float().data.cpu().numpy()[0].T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217ee3ba-14fc-4a3f-833f-f216d8cafdf8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
