{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c6ea1b-d4c3-4ce2-9f9e-7b78e18ed665",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "skip_training = False  # Set this flag to True before validation and submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a6ba2a-e4ab-452d-b5b0-cdaf09ff7f3e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a3a83d9b9c5541f81165a9a05aa38f61",
     "grade": true,
     "grade_id": "cell-f7ace4d177d1ba70",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tools, warnings\n",
    "warnings.showwarning = tools.customwarn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7646f794-19c7-4c5e-b2ce-63a6f6788451",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tools\n",
    "import tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f251f9ff-549d-4b52-873f-49bc77af2da0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = tools.select_data_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78226e45-4395-4249-8244-d1e4b0d86af0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select the device for training (use GPU if you have one)\n",
    "#device = torch.device('cuda:0')\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae46652-80ef-4aaa-a1e3-936f556db7e8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a6df1132be030e6d1710ac059c93acca",
     "grade": false,
     "grade_id": "cell-fc86b9cd8e65b149",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The models are always evaluated on CPU\n",
    "if skip_training:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c64d552-c735-431e-b0fe-9b46094d5e73",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5448d60c1a07be904376c4ed913dbd85",
     "grade": false,
     "grade_id": "cell-5971a22eb75f2abd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b66d954-b431-44da-9d48-5e8f4c12aed3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "827ec26604f5a5b61a7ac458e6d6f336",
     "grade": false,
     "grade_id": "cell-40903d78f014a08f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainset = tools.MNIST(data_dir, train=True)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d8e722-5182-4ea7-9ed4-e521043914e5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "81dca4bdf8b8363ac0e0d058072b2a9a",
     "grade": false,
     "grade_id": "cell-4e392fc1af8ce52b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "images, labels, idxs = next(iter(trainloader))\n",
    "print(\"Maximum pixel value:\", images.flatten().max().item())\n",
    "print(\"Minimum pixel value:\", images.flatten().min().item())\n",
    "tools.show_images(images[:8], ncol=4, cmap='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88a98f3-8eb3-44d1-94d6-2e74219a9c30",
   "metadata": {},
   "source": [
    "## Deep Latent Variable Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a48ee9-abea-4c46-8aeb-6bfa22e96051",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "58252c2ed61f6db9e7abe3cc6273297a",
     "grade": false,
     "grade_id": "cell-eefb98830cb5df7a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### (Non-Amortized) Variational Inference\n",
    "\n",
    "In this exercise, we will implement variational inference (VI) to learn the parameters of the following generative model:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{z} \\sim p_\\theta(\\boldsymbol{z}) = \\mathcal{N}(\\boldsymbol{0}, I_L)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\boldsymbol{x} \\mid \\boldsymbol{z} \\sim p_\\theta(\\boldsymbol{x} \\mid \\boldsymbol{z}) = \\mathcal{N}(\\boldsymbol{\\mu}_\\theta(\\boldsymbol{z}), \\boldsymbol{\\sigma}_\\theta^2I_D)\n",
    "$$\n",
    "where $\\boldsymbol{z} \\in \\mathbb{R}^L$ is $L$ dimensional latent variable vector of a single sample $\\boldsymbol{x} \\in \\mathbb{R}^D$ and $\\boldsymbol{\\mu}_\\theta(\\cdot)$ is a neural network, i.e. \"decoder\", with parameters $\\theta$. For simplicity, we will assume $\\boldsymbol{\\sigma}_\\theta \\in \\mathbb{R}^D$ is a vector of learnable scalars as a part of decoder parameters.\n",
    "\n",
    "\n",
    "![VI Graphical Model](vi_graphical.png) \n",
    "\n",
    "We are interested in the posterior distribution of latent variables:\n",
    "\n",
    "$$\n",
    "p_\\theta(\\boldsymbol{z} \\mid \\boldsymbol{x}) = \\frac{p_\\theta(\\boldsymbol{x} \\mid \\boldsymbol{z}) p_\\theta(\\boldsymbol{z})}{p_\\theta(\\boldsymbol{x})}\n",
    "$$\n",
    "\n",
    "which is intractable since the evidence term, $p_\\theta(\\boldsymbol{x})$, cannot be computed analitically. We will use VI to approximate $p_\\theta(\\boldsymbol{z} \\mid \\boldsymbol{x})$ by a variational distribution $q_\\psi(z)$ with variational parameters denoted by $\\psi$. Then our objective becomes minimizing the reverse Kullback-Leibler (KL) divergence $\\text{KL}(q_\\psi(\\boldsymbol{z}) \\mid \\mid p_\\theta(\\boldsymbol{z} \\mid \\boldsymbol{x}))$, which is equivalent to maximizing evidence lower bound (ELBO):\n",
    "\n",
    "$$\n",
    "\\log p_\\theta(\\boldsymbol{x}) \\geq \\underbrace{\\mathbb{E}_{q_\\psi(\\boldsymbol{z})}\\left[\\log p_\\theta(\\boldsymbol{x} \\mid \\boldsymbol{z}) \\right]}_{\\text{expected log likelihood}} - \\underbrace{\\text{KL}\\left(q_\\psi(\\boldsymbol{z}) \\mid \\mid p_\\theta(\\boldsymbol{z})\\right)}_{\\text{KL of the posterior from prior}}\n",
    "$$\n",
    "\n",
    "where the first term measures the reconstruction loss and the second term can be interpreted as prior regularization.\n",
    "\n",
    "\n",
    "In this task, we assume the following variational distribution per observation:\n",
    "\n",
    "$$q_{\\psi_n}(\\boldsymbol{z_n}) = \\mathcal{N}(\\boldsymbol{\\mu}_n, \\boldsymbol{\\sigma}_n^2 I_L)$$\n",
    "\n",
    "\n",
    "For $N$ observations the joint distribution factorizes across samples:\n",
    "\n",
    "\\begin{align*}\n",
    "p_\\theta(\\boldsymbol{x}_{1:N}, \\boldsymbol{z}_{1:N}) &= p_\\theta(\\boldsymbol{x}_{1:N} \\mid \\boldsymbol{z}_{1:N}) p_\\theta(\\boldsymbol{z}_{1:N}) \\\\\n",
    "&= \\prod_{n = 1}^N p_\\theta(\\boldsymbol{x}_{n} \\mid \\boldsymbol{z}_{n}) p_\\theta(\\boldsymbol{z}_{n})\n",
    "\\end{align*}\n",
    "\n",
    "Moreover, we assume mean-field approximation as the variational family:\n",
    "\n",
    "$$\n",
    "q(\\boldsymbol{z}_{1:N}) = \\prod_{n = 1}^N q_{\\psi_n}(\\boldsymbol{z}_n)\n",
    "$$\n",
    "\n",
    "Hence, we have ELBO that also factorizes as follows:\n",
    "\n",
    "$$\\log p_\\theta(\\boldsymbol{x}_{1:N}) \\geq \\text{ELBO}(\\theta, \\psi_{1:N} \\mid \\boldsymbol{x}_{1:N}) = \\sum_{n = 1}^N \\text{ELBO}(\\theta, \\psi_{n} \\mid \\boldsymbol{x}_{n})$$\n",
    "\n",
    "In the coding part, you will minimize the negative of ELBO (- ELBO) which is equivalent to maximizing ELBO.\n",
    "\n",
    "$$\n",
    "- \\text{ELBO}(\\theta, \\psi_{n} \\mid \\boldsymbol{x}_{n}) = - \\mathbb{E}_{q_\\psi(\\boldsymbol{z}_n)}\\left[\\log p_\\theta(\\boldsymbol{x}_n \\mid \\boldsymbol{z}_n) \\right] + \\text{KL}\\left(q_\\psi(\\boldsymbol{z}_n) \\mid \\mid p_\\theta(\\boldsymbol{z}_n)\\right)\n",
    "$$\n",
    "\n",
    "- Negative log-likelihood: $- \\log p_\\theta(\\boldsymbol{x} \\mid \\boldsymbol{z})$\n",
    "- Regularization: $\\text{KL}\\left(q_\\psi(\\boldsymbol{z}) \\mid \\mid p_\\theta(\\boldsymbol{z})\\right)$\n",
    "\n",
    "where we drop the sample index $n$ for simplicity.\n",
    "\n",
    "You will derive negative log-likelihood in the following task. Derivation for KL of two Gaussians is provided in the lecture slides.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb43d354-8eb8-4cca-a128-08d15322c5a5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b369959b2c3f6fd54a1efd617654e02c",
     "grade": false,
     "grade_id": "cell-40ec97f3fde3ccb9",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    },
    "tags": []
   },
   "source": [
    "#### Derivation: Negative log-likelihood (1 point)\n",
    "\n",
    "Derive $- \\log p_\\theta(\\boldsymbol{x} \\mid \\boldsymbol{z})$ here. Hints:\n",
    "\n",
    "- Make use of diagonal covariance to ease your work.\n",
    "- Omit the constant term $\\frac{D}{2} \\log(2 \\pi)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c4c36c-cf08-4788-b54c-c4696d85cd60",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6075d9ca2e75a485beb4129ccb23c03a",
     "grade": true,
     "grade_id": "cell-27a4a9cad5a120f3",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe95ab9-4616-456b-8efd-96eab3b0bdda",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2d09120bed110feadd13dbd7d15865b6",
     "grade": false,
     "grade_id": "cell-6a1fb95610578013",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "#### Coding: Decoder\n",
    "\n",
    "$$p_\\theta(\\boldsymbol{x} \\mid \\boldsymbol{z}) = \\mathcal{N}(\\boldsymbol{\\mu}_\\theta(\\boldsymbol{z}), \\boldsymbol{\\sigma}_\\theta^2I_D)$$\n",
    "We will use the following architecture for our decoder (generative network):\n",
    "* Fully-connected layer with 250 output features, followed by ReLU\n",
    "* Fully-connected layer with 250 input features, followed by ReLU\n",
    "* `ConvTranspose2d` layer with kernel size 5 with 16 input channels, followed by ReLU\n",
    "* `ConvTranspose2d` layer with kernel size 5 with 6 input channels.\n",
    "\n",
    "**Important note about learning $\\boldsymbol{\\sigma}^2_\\theta$:**\n",
    "\n",
    "In many probabilistic models, we need to ensure that variances remain strictly positive. Moreover, directly predicting variance can cause numerical issues if the model tries to produce values near zero or negative. To ensure positivity and stability, we will parameterize the variance of predictions in log space and restrict it to have a minimum value as follows:\n",
    "```\n",
    "self.register_buffer('min_logvar', -6 * torch.ones(1))\n",
    "```\n",
    "\n",
    "We need to use `register_buffer` to ensure that the variable is on the same device as the trained parameters of the model. \n",
    "\n",
    "* You need to initialize a `trainable` set of parameters as `self._logvar` to tensor consting of **0s** in the `__init__` method of your decoder class.\n",
    "* You will implement a method `get_logvar` to get the logvar parameters using the following code:\n",
    "\n",
    "```\n",
    "logvar = self.min_logvar + F.softplus(self._logvar - self.min_logvar)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1222f5fc-4ea2-4c9a-8108-c236321e9dab",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "66b3b647505bbf034799999be757c022",
     "grade": false,
     "grade_id": "cell-7bfa8fdd36979064",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          latent_dim (int): Dimensionality of the latent space.\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        self._logvar = None\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def get_logvar(self):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          z of shape (batch_size, latent_dim): Tensor of latent variables.\n",
    "\n",
    "        Returns:\n",
    "          x_hat of shape (batch_size, n_channels=1, width, height): Tensor of reconstructed images.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39833a3-594e-4d27-9020-a7f4ee2f1b7f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e4343c2fc6551c9e69fb40ab5ed96fce",
     "grade": true,
     "grade_id": "cell-affa3dcf9b2d696c",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_Decoder_shapes():\n",
    "    latent_dim = 10\n",
    "    decoder = Decoder(latent_dim=latent_dim)\n",
    "\n",
    "    z = torch.randn(3, latent_dim)\n",
    "    x_hat = decoder(z)\n",
    "    x_hat_shape = torch.Size([3, 1, 28, 28])\n",
    "    \n",
    "    logvar = decoder.get_logvar()\n",
    "    logvar_shape = torch.Size([1, 28, 28])\n",
    "    _logvar_val = np.zeros(28 * 28)\n",
    "    assert x_hat.shape == x_hat_shape, \"Bad shape of x_hat: x_hat.shape={}\".format(x_hat.shape)\n",
    "    assert logvar.shape == logvar_shape, \"Bad shape of logvar: logvar.shape={}\".format(logvar.shape)\n",
    "    assert decoder._logvar.requires_grad, \"Logvar must be trainable\"\n",
    "    assert all(decoder._logvar.detach().flatten().numpy() == _logvar_val), \"Logvar must be initialized to 0\"\n",
    "    print('Success')\n",
    "    \n",
    "test_Decoder_shapes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff0c4e2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cdfa40661616a08728a81bf692c931b9",
     "grade": false,
     "grade_id": "cell-7248a6384bae5b56",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "#### Coding: Negative log-likelihood loss (1 point)\n",
    "\n",
    "Implement the expected NLL that you derived previously.\n",
    "\n",
    "**Notes:** \n",
    "\n",
    "* Your implementation **should not** include constant term $\\frac{28 * 28}{D} \\log 2\\pi$.\n",
    "* Make use of **diagonal covariance** structure to simplify computations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6d4ce6-f17d-4328-af7c-5dd8848b855b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "30e7a7ee8ecffe5e15491bb6cbd79566",
     "grade": false,
     "grade_id": "cell-a0f36f56f8ba8d52",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def nll_loss(x, x_hat, logvar):\n",
    "    \"\"\"\n",
    "     Args:\n",
    "        x of shape (batch_size, 1, 28, 28): Training samples.\n",
    "        x_hat of shape (batch_size, 1, 28, 28): Predictive mean of the generated sample.\n",
    "        logvar of shape (1, 28, 28): Log-variance of the generated pixels.\n",
    "    \n",
    "    Returns:\n",
    "        nll (torch scalar): Negative log-likelihood loss, averaged over the batch.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c74e2b8-860c-4725-99db-7a623928b311",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "88f4c14afb839e7523083e34164044a7",
     "grade": true,
     "grade_id": "cell-e7a00db9717d3756",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_nll_loss():\n",
    "    x_hat = torch.ones(3, 1, 28, 28) \n",
    "    logvar = torch.log(2*torch.ones(1, 28, 28))\n",
    "    logvar[:,:14,:] = torch.log(torch.ones(1, 14, 28))\n",
    "\n",
    "    x = torch.zeros(3, 1, 28, 28)\n",
    "    x[:,:,:14,:] = torch.ones(1, 14, 28)\n",
    "\n",
    "    nll = nll_loss(x, x_hat, logvar)\n",
    "    expected = torch.tensor(233.8568572998047)\n",
    "    \n",
    "    print('loss:', nll)\n",
    "    print('expected:', expected)\n",
    "    assert torch.allclose(nll, expected), \"nll does not match expected value.\"\n",
    "    print('Success')\n",
    "\n",
    "test_nll_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f0b1d7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ed765dc80f7bc4eb048573b002bb8a8b",
     "grade": false,
     "grade_id": "cell-e09076e4d05dcf75",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "#### Coding: Kullback-Leibler Divergence (1 point)\n",
    "\n",
    "Implement KL divergence:\n",
    "\n",
    "$$\n",
    "\\frac{1}{B} \\sum_{n = 1}^B \\text{KL}\\left(q_\\psi(\\boldsymbol{z}_n) \\mid \\mid p_\\theta(\\boldsymbol{z}_n)\\right)\n",
    "$$\n",
    "\n",
    "where $B$ is the batch size and\n",
    "\n",
    "\\begin{align*}\n",
    "q_{\\psi_n}(\\boldsymbol{z_n}) &= \\mathcal{N}(\\boldsymbol{\\mu}_n, \\boldsymbol{\\sigma}_n^2 I_L)\\\\\n",
    "p_\\theta(\\boldsymbol{z}_n) &= \\mathcal{N}(\\boldsymbol{0}, I_L)\n",
    "\\end{align*}\n",
    "\n",
    "**Notes:**\n",
    "* Please do **not** use functions from the `torch.distributions` module. \n",
    "* Make use of **diagonal covariance** structure to simplify computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160d5030-e4fb-4902-bf62-14a41f676302",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bb496d27814e7aae6a841e10d22cd420",
     "grade": false,
     "grade_id": "cell-bee7afe4fc487036",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def kl_loss(z_mu, z_logvar):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      z_mu of shape (batch_size, latent_dim): Means of the approximate distributions of the latent variables.\n",
    "      z_logvar of shape (batch_size, latent_dim): Log-variance of the approximate distributions of the latent variables.\n",
    "    \n",
    "    Returns:\n",
    "      loss (torch scalar): Kullback-Leibler divergence, averaged over the batch.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d24554-53e9-4931-9332-2fae5b6c0e7c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "655d55c7acbf60eb09555e0f7e542245",
     "grade": true,
     "grade_id": "cell-228e0b77c6e50a28",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_loss_kl():\n",
    "    latent_dim = 3\n",
    "    z_mu = torch.zeros(3, latent_dim)\n",
    "    z_logvar = torch.log(2*torch.ones(3, latent_dim))\n",
    "    kl = kl_loss(z_mu, z_logvar)\n",
    "    expected = torch.tensor(0.4602792263031006)\n",
    "    print('kl:', kl.item())\n",
    "    print('expected:', expected.item())\n",
    "    assert torch.allclose(kl, expected, atol=1e-5), \"loss does not match expected value.\"\n",
    "    print('Success')\n",
    "\n",
    "test_loss_kl()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2033fa",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "aa48d70e6d8c78ba987592b41e4bd3b3",
     "grade": false,
     "grade_id": "cell-9b5b241b372f9248",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "#### Coding: Variational distribution\n",
    "\n",
    "In the following cell, you need to initialize variational distribution per observation.\n",
    "\n",
    "\\begin{align*}\n",
    "q(\\boldsymbol{z}_{1:N}) &= \\prod_{n = 1}^N q_{\\psi_n}(\\boldsymbol{z}_n)\\\\\n",
    "q_{\\psi_n}(\\boldsymbol{z}_n) &= \\mathcal{N}(\\boldsymbol{\\mu}_n, \\boldsymbol{\\sigma}_n^2 I_L)\n",
    "\\end{align*}\n",
    "\n",
    "**Important note about learning $\\boldsymbol{\\sigma}^2_n$:**\n",
    "\n",
    "As mentioned previously, we need to ensure positivity on variances. Again, we will achieve this by parameterizing variance in the log space, $\\log \\boldsymbol{\\sigma}^2_n$. Hence, variational parameters become $\\psi_n = \\{\\boldsymbol{\\mu}_n, \\log \\boldsymbol{\\sigma}^2_n\\}$. We can obtain the positive variance by just exponentiating log-variances, $\\boldsymbol{\\sigma}^2_n = \\exp(\\log \\boldsymbol{\\sigma}^2_n)$. \n",
    "\n",
    "* **Do not set a minimum value** for the variance in this case.\n",
    "\n",
    "* Use 0 as the initial value for $\\boldsymbol{\\mu}$.\n",
    "* Use 1 as the initial value for $\\log \\boldsymbol{\\sigma}^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc642b84-392d-4bc7-b955-2fea53d76db1",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f5b4a7d8838e659ab2d62107c94d7d6d",
     "grade": false,
     "grade_id": "cell-da94773f9844bed5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def init_variational_parameters(N, latent_dim):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        N (int): Number of training samples.\n",
    "        latent_dim (int): Dimensionality of the latent space.\n",
    "    Returns:\n",
    "        mu_psi of shape (N, latent_dim): Means of the approximate distributions of the latent variables.\n",
    "        logvar_psi of shape (N, latent_dim): Log-variance of the approximate distributions of the latent variables.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e9f100-44a2-4516-9cc9-f0dd6e2ff631",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1055c2d9350234420855b73253e91f34",
     "grade": true,
     "grade_id": "cell-b77552d701b5c9bf",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_variational_parameters():\n",
    "    mu_psi, logvar_psi = init_variational_parameters(100, 3)\n",
    "    param_shape = torch.Size([100, 3])\n",
    "    assert mu_psi.shape == param_shape, \"Bad shape of mu_psi: mu_psi.shape={}\".format(mu_psi.shape)\n",
    "    assert logvar_psi.shape == param_shape, \"Bad shape of logvar_psi: logvar_psi.shape={}\".format(logvar_psi.shape)\n",
    "    assert mu_psi.requires_grad, \"mu_psi must be trainable\"\n",
    "    assert logvar_psi.requires_grad, \"logvar_psi must be trainable\"\n",
    "    print('Success')\n",
    "\n",
    "test_variational_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4959a571",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "62d445a07540ab6054ca7c055b69643f",
     "grade": false,
     "grade_id": "cell-57328785af58d26d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "#### Coding: Reparameterization trick for sampling\n",
    "\n",
    "We have the variational distribution with parameters $$\\psi = \\{\\boldsymbol{\\mu}, \\log \\boldsymbol{\\sigma}^2\\}$$ and we need to sample latent variables from the distribution. We will do this as follows:\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "\\boldsymbol{\\epsilon} &\\sim \\mathcal{N}(\\boldsymbol{0}, I_L)\\\\\n",
    "\\boldsymbol{\\sigma} &= \\sqrt{\\exp (\\log \\boldsymbol{\\sigma}^2)}\\\\\n",
    "\\boldsymbol{z} &= \\boldsymbol{\\mu} + \\text{diag}(\\boldsymbol{\\sigma}) \\boldsymbol{\\epsilon}\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46567dd-cfa0-451c-9d8e-945cea1b6b29",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bad7af12ab2ddafa9119f2066726d966",
     "grade": false,
     "grade_id": "cell-3da82736be24c4b8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sample_latent(z_mu, z_logvar):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        z_mu of shape (batch_size, latent_dim): Means of the approximate distributions of the latent variables.\n",
    "        z_logvar of shape (batch_size, latent_dim): Log-variance of the approximate distributions of the latent variables.\n",
    "    \n",
    "    Returns:\n",
    "        z of shape (batch_size, latent_dim): Sampled latent variables.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b42971e-5479-46d0-a4e9-54c202be474d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cbacf382f7d5116beda97667aedd66e3",
     "grade": true,
     "grade_id": "cell-859654af3b2d499a",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_sample():\n",
    "    latent_dim = 10\n",
    "    z_mu = torch.zeros(3, latent_dim)\n",
    "    z_logvar = torch.log(2*torch.ones(3, latent_dim))\n",
    "    z = sample_latent(z_mu, z_logvar)\n",
    "    assert z.shape == z_mu.shape, f\"Bad z.shape: {z.shape}\"\n",
    "    print('Success')\n",
    "\n",
    "test_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dca5dd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5fd48ce162fe7e79f74f6724d882a402",
     "grade": false,
     "grade_id": "cell-707002f761dd655d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "#### Coding: Packed VI model (1 point)\n",
    "\n",
    "We will put all components together to reduce repetitive code. Implement the `loss` and `forward` functions of the VI class in the cell below. \n",
    "\n",
    "* `loss` function should return - ELBO averaged for the given samples.\n",
    "* `forward` function (i) fetches the local variational parameters of the training samples, (ii) samples latent variables and (iii) decodes them to predict images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073f09bb",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9299f952f7e2792d23ba11fbbe728e53",
     "grade": false,
     "grade_id": "cell-8a5ac970e8347e65",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VI(nn.Module):\n",
    "    def __init__(self, N, latent_dim):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          N (int): Number of observations in the dataset.\n",
    "          latent_dim (int): Dimensionality of the latent space.\n",
    "        \"\"\"\n",
    "        super(VI, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.decoder = Decoder(latent_dim)\n",
    "        self.mu_psi, self.logvar_psi = init_variational_parameters(N, latent_dim)\n",
    "\n",
    "    def loss(self, x, x_hat, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          x of shape (batch_size, n_channels=1, width=28, height=28): Training samples.\n",
    "          x_hat of shape (batch_size, n_channels=1, width=28, height=28): Predictive mean of the images.\n",
    "          index of shape (batch_size): Index of the training samples.\n",
    "\n",
    "        Returns:\n",
    "          loss (torch scalar): Loss (- ELBO) averaged over the batch.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def forward(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          index of shape (batch_size): Index of the training samples.\n",
    "\n",
    "        Returns:\n",
    "          x_hat of shape (batch_size, n_channels=1, width=28, height=28): Predictive mean of the images.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9afe84f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "db6392247c708ad2a38fd12be3ac73d7",
     "grade": true,
     "grade_id": "cell-5fb883e3bfbb9cd8",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_VI_ELBO():\n",
    "    N = 3\n",
    "    latent_dim = 10\n",
    "    tes_vi = VI(N, latent_dim)\n",
    "\n",
    "    x_hat = torch.ones(3, 1, 28, 28)\n",
    "\n",
    "    x = torch.zeros(3, 1, 28, 28)\n",
    "    x[:,:,:14,:] = torch.ones(1, 14, 28)\n",
    "    \n",
    "    index = torch.tensor([0, 1, 2])\n",
    "    loss = tes_vi.loss(x, x_hat, index)\n",
    "\n",
    "    expected = torch.tensor(200.07723999023438)\n",
    "    \n",
    "    print('-ELBO:', loss)\n",
    "    print('expected:', expected)\n",
    "    assert torch.allclose(loss, expected), \"loss does not match expected value.\"\n",
    "    print('Success')\n",
    "test_VI_ELBO()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a6508f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2fda39e9f26371ab8a4d6892f269e906",
     "grade": false,
     "grade_id": "cell-deefaf543e32026f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "#### Training setup for VI model\n",
    "\n",
    "We will use a small subset of the MNIST dataset for the VI model training since optimizing local parameters individually for the entire dataset is a challenging task. We will make use of 1000 random samples from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3f29e4-1ce8-4b77-87c4-a1302c05f516",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "689f65a8706acff63dd28c1d258678dd",
     "grade": false,
     "grade_id": "cell-b26fa4e6e7c1e53a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "N = 1000\n",
    "vi_trainset = tools.MNIST(data_dir, train=True, N=N)\n",
    "vi_trainloader = torch.utils.data.DataLoader(vi_trainset, batch_size=32, shuffle=True)\n",
    "L = 10\n",
    "vi_model = VI(N, L)\n",
    "vi_model = vi_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90894be6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ebcb4ce0b0e130e2d789d68023fc2311",
     "grade": false,
     "grade_id": "cell-c1d868cce11c2450",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "#### Coding: Training loop (2 points)\n",
    "\n",
    "The `vi_trainloader` provides images, labels, and indexes of the samples in the batch. \n",
    "\n",
    "Implement the training loop in the cell below. The recommended hyperparameters:\n",
    "* Adam optimizer with a learning rate of 0.001\n",
    "* Number of epochs: 1000\n",
    "\n",
    "Hints:\n",
    "- The loss after 1000 epochs should be less than -1050."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dca332f-318e-44c1-b39e-ea8b125359c8",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5a476b128ea53dfe68d5c100fbed45e8",
     "grade": false,
     "grade_id": "cell-198f969e3b8f39cf",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "if not skip_training:\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544565fb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7c19ade62622cf4d271c5d45d284180c",
     "grade": false,
     "grade_id": "cell-0782c27c3d0514c8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "    tools.save_model(vi_model, '2_vi_model.pth', confirm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1523cc9c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5b7cf17e363028c5a3881a0a421f4748",
     "grade": false,
     "grade_id": "cell-57006d6a258fb14b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if skip_training:\n",
    "    vi_model = VI(N, L)\n",
    "    tools.load_model(vi_model, '2_vi_model.pth', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5532f08-1cc5-4b83-b586-0d46f115f65d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b3d1b21f9a1898f02f4bc1c653741c33",
     "grade": true,
     "grade_id": "cell-eace3d53181feab0",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tests.visualize_embeddings(vi_model.mu_psi, vi_trainloader, n_samples=1000, device=device, avi = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e681b589",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "575bc104da79e5605e51076ba7607b59",
     "grade": true,
     "grade_id": "cell-3463275d5ecf8494",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    dataiter = iter(vi_trainloader)\n",
    "    images, _, index = next(dataiter)\n",
    "    images = images.to(device)\n",
    "    x_hat = vi_model(index)\n",
    "    tools.show_images(images.cpu()[:8], ncol=4, cmap='binary')\n",
    "    tools.show_images(x_hat.cpu()[:8], ncol=4, cmap='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2d71b6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4e2eabadf95153d7e7911572728a31b6",
     "grade": true,
     "grade_id": "cell-554d5627db70f0bb",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This cell tests your model. Do not remove."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59527023-0079-4477-bd56-28356895265c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d8eff097456d69dd4b11d2579f2c4720",
     "grade": false,
     "grade_id": "cell-384d4201140ea4f4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### Amortized Variational Inference\n",
    "\n",
    "In standard VI we have local variational parameters per data point, which allows us to be flexible and even find the global parameters of the used variational family (mean-field). However, we face the following problems:\n",
    "\n",
    "1. We might have a large number of observations, $N$, which means we would have a large number of free parameters that need to be optimized separately. To achieve optimal parameters we should perform many training epochs which can take quite a lot of time.\n",
    "2. Optimized parameters are restricted to observations. When there is a separate test set, which is not observed during the training, we have no information about the posterior distribution. Hence, VI cannot generalize to unseen data points.\n",
    "\n",
    "Instead of learning the local variational parameters of each sample separately, we can learn a **global** function to predict local variational parameters:\n",
    "\n",
    "$$\\psi_n = f_{\\phi}(\\boldsymbol{x}_n)$$\n",
    "\n",
    "where $f_\\phi(\\cdot)$ called the recognition network or simply encoder that provides:\n",
    "\n",
    "1. Faster convergence since each update improves inference for **all** data via a shared encoder.\n",
    "2. Generalization to the unseen data. We can learn the approximate posterior of unseen data points by providing the data to the encoder network.\n",
    "3. Fixed number of trainable parameters that are independent of the dataset size.\n",
    "\n",
    "![AVI Graphical Model](avi_graphical.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2c459c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9b8d1cf5d1c242374a4744d9f75b89ae",
     "grade": false,
     "grade_id": "cell-7ddce8423f125522",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "#### Coding: Encoder\n",
    "$$\\psi_n = \\{\\boldsymbol{\\mu}_n, \\log \\boldsymbol{\\sigma}^2_n\\} = f_{\\phi}(\\boldsymbol{x}_n)$$\n",
    "We will use the following architecture for our encoder (recognition network):\n",
    "* `Conv2d` layer with kernel size 5 with 6 output channels, followed by ReLU\n",
    "* `Conv2d` layer with kernel size 5 with 16 output channels, followed by ReLU\n",
    "* Fully-connected layer with 250 output features, followed by ReLU\n",
    "* Two heads: each is a fully-connected layer with `latent_dim` elements.\n",
    "\n",
    "The two heads are needed to produce two outputs of the encoder:\n",
    "* means $\\boldsymbol{\\mu}_n$ of the approximate distribution of the latent varibles.\n",
    "* log-variances $\\log \\boldsymbol{\\sigma}_n^2$ of the approximate distribution of the latent variables.\n",
    "\n",
    "To guarantee that the variance is positive, we parameterize it as $\\boldsymbol{\\sigma}_n^2 = \\exp(\\log \\boldsymbol{\\sigma}_n^2)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b64e59c-c8f3-4e1b-a9ba-a263c1f7a3fe",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c292e7cd84d01ea8d1848252309c05be",
     "grade": false,
     "grade_id": "cell-605428905a59b595",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          latent_dim (int): Dimensionality of the latent space\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          x of shape (batch_size, n_channels=1, width, height): Examples to encode.\n",
    "\n",
    "        Returns:\n",
    "          z_mu of shape (batch_size, latent_dim): Means of the approximate distributions of the codes.\n",
    "          z_logvar of shape (batch_size, latent_dim): Log-variances of the approximate distributions of the codes.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615bf190",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6802e79e4a1bbe376124ea13300ecfe8",
     "grade": false,
     "grade_id": "cell-1f9adffaf4dd5824",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "#### Coding: Packed AVI model (1 point)\n",
    "\n",
    "We will put all components together to reduce repetitive code. Implement the `loss` and `forward` functions of the AVI class in the cell below. \n",
    "\n",
    "* `loss` function should return - ELBO averaged for the given samples.\n",
    "* `forward` function (i) encodes the given training samples and obtains variational parameters, (ii) samples latent variables and (iii) decodes them to predict images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db585f24",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "16ed42c96cfb1112963d223cc1e4ef35",
     "grade": false,
     "grade_id": "cell-0f6e0e708cc2aa42",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AVI(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          latent_dim (int): Dimensionality of the latent space.\n",
    "        \"\"\"\n",
    "        super(AVI, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.encoder = Encoder(latent_dim)\n",
    "        self.decoder = Decoder(latent_dim)\n",
    "\n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "    \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def loss(self, x, x_hat, z_mu, z_logvar):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          x of shape (batch_size, n_channels=1, width=28, height=28): Training samples.\n",
    "          x_hat of shape (batch_size, n_channels=1, width=28, height=28): Predictive mean of the images.\n",
    "          z_mu of shape (batch_size, latent_dim): Means of the approximate distributions of the latent variables.\n",
    "          z_logvar of shape (batch_size, latent_dim): Log-variance of the approximate distributions of the latent variables.\n",
    "\n",
    "        Returns:\n",
    "          loss (torch scalar): Loss (- ELBO) averaged over the batch.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          x of shape (batch_size, n_channels=1, width=28, height=28): Training samples.\n",
    "\n",
    "        Returns:\n",
    "          x_hat of shape (batch_size, n_channels=1, width=28, height=28): Predictive distribution of the images.\n",
    "          z_mu of shape (batch_size, latent_dim): Means of the approximate distributions of the latent variables.\n",
    "          z_logvar of shape (batch_size, latent_dim): Log-variance of the approximate distributions of the latent variables.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354634d8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ac45c433b585f51186d4271b0879ac74",
     "grade": true,
     "grade_id": "cell-c16c04b923ca1dd5",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_AVI_ELBO():\n",
    "    latent_dim = 10\n",
    "    tes_avi = AVI(latent_dim)\n",
    "\n",
    "    x_hat = torch.ones(3, 1, 28, 28)\n",
    "    x = torch.zeros(3, 1, 28, 28)\n",
    "    x[:,:,:14,:] = torch.ones(1, 14, 28)\n",
    "\n",
    "    z_mu = torch.zeros(3, latent_dim)\n",
    "    z_logvar = torch.log(2*torch.ones(3, latent_dim))\n",
    "    \n",
    "    loss = tes_avi.loss(x, x_hat, z_mu, z_logvar)\n",
    "    expected = torch.tensor(198.0200958251953)\n",
    "    \n",
    "    print('-ELBO:', loss)\n",
    "    print('expected:', expected)\n",
    "    assert torch.allclose(loss, expected), \"loss does not match expected value.\"\n",
    "    print('Success')\n",
    "test_AVI_ELBO()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7250eb1d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "12c873c76f37806f276e7e648060b55b",
     "grade": false,
     "grade_id": "cell-e709aa22596b5df4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "L = 10\n",
    "avi_model = AVI(L)\n",
    "avi_model = avi_model.to(device)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761d39bb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7108a431a8e7eb30eee7566083f636b9",
     "grade": false,
     "grade_id": "cell-1843610f7335699a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "#### Coding: Training loop\n",
    "\n",
    "The `trainloader` provides images, labels, and indexes of the samples in the batch. You can ignore indexes in AVI training.\n",
    "\n",
    "Implement the training loop in the cell below. The recommended hyperparameters:\n",
    "* Adam optimizer with a learning rate of 0.001\n",
    "* Number of epochs: 10\n",
    "\n",
    "Hints:\n",
    "- The loss after 10 epochs should be less than -1140."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a496bc8a-a928-4dc3-9ab7-88ef91e94b74",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "57a83f92de8806cba58bc380e4c27fcf",
     "grade": false,
     "grade_id": "cell-3fa22998d5528958",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543e03f1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c2c544acf7335291237d6f7104824136",
     "grade": false,
     "grade_id": "cell-181e5ca88dbe4e09",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "    tools.save_model(avi_model, '2_avi_model.pth', confirm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce41dd1b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c17ee2fb2fb5d6c908285c8c66265e20",
     "grade": false,
     "grade_id": "cell-f03fb1b9807f2e09",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if skip_training:\n",
    "    avi_model = AVI(L)\n",
    "    tools.load_model(avi_model, '2_avi_model.pth', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d146f347-dcf5-4fc1-9fe4-d1d7a10234ac",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "680e5905f7181c98b9607b0e007dc9db",
     "grade": true,
     "grade_id": "cell-6c3fac11b7244bf0",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tests.visualize_embeddings(lambda x: avi_model.encode(x)[0], trainloader, n_samples=1000, device=device, avi = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d32cf5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fb80bd3112fd1e840a1a5bfcd7367c5e",
     "grade": true,
     "grade_id": "cell-918ed529be0a03f4",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    dataiter = iter(trainloader)\n",
    "    images, _, _ = next(dataiter)\n",
    "    images = images.to(device)\n",
    "    x_hat, _, _ = avi_model(images)\n",
    "    tools.show_images(images.cpu()[:8], ncol=4, cmap='binary')\n",
    "    tools.show_images(x_hat.cpu()[:8], ncol=4, cmap='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3b44a5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c626293664bcfff2bf3e0e41f7dcb94f",
     "grade": false,
     "grade_id": "cell-c7421750b5762576",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "#### Test the quality of the produced embeddings of unobserved data (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a37862",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2c85392b7a15623b91896abbf814ff4a",
     "grade": false,
     "grade_id": "cell-01ea5a266ee53513",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "testset = tools.MNIST(data_dir, train=False)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4442dd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "608649371e52e4b041b114e8036e55e4",
     "grade": true,
     "grade_id": "cell-948eaca3807f70a6",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    dataiter = iter(testloader)\n",
    "    images, _, _ = next(dataiter)\n",
    "    images = images.to(device)\n",
    "    x_hat, _, _ = avi_model(images)\n",
    "    tools.show_images(images.cpu()[:8], ncol=4, cmap='binary')\n",
    "    tools.show_images(x_hat.cpu()[:8], ncol=4, cmap='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7293b7a2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "780b7261b8a5fb077f54520c6ad21a51",
     "grade": true,
     "grade_id": "cell-7e9612002c5ea37d",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Encode data samples using the AVI encoder\n",
    "@torch.no_grad()\n",
    "def encode(dataset, encoder):\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=100, shuffle=False)\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "    for images, labels_, _ in dataloader:\n",
    "        mu, logsigma = encoder(images.to(device))\n",
    "        embeddings.append(mu)\n",
    "        labels.append(labels_)\n",
    "\n",
    "    embeddings = torch.cat(embeddings, dim=0)\n",
    "    labels = torch.cat(labels, dim=0)\n",
    "    return embeddings, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ad747d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0a5a066cf6311ec71b6888d60d45ae50",
     "grade": true,
     "grade_id": "cell-e8a8aa8b80ec4fad",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save and submit the AVI embeddings\n",
    "if not skip_training:\n",
    "    traincodes, trainlabels = encode(trainset, avi_model.encoder)  # traincodes is (60000, 10)\n",
    "    testcodes, testlabels = encode(testset, avi_model.encoder)  # testcodes is (10000, 10)\n",
    "    torch.save([traincodes, trainlabels, testcodes, testlabels], '2_avi_embeddings.pth')\n",
    "else:\n",
    "    traincodes, trainlabels, testcodes, testlabels = torch.load('2_avi_embeddings.pth', map_location=lambda storage, loc: storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1cf6be",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1f2835b4f32587eef60ee4fa39d80e44",
     "grade": true,
     "grade_id": "cell-0cd63c3a854e7211",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train a simple linear classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(C=1e5, solver='lbfgs', multi_class='multinomial', max_iter=400)\n",
    "logreg.fit(traincodes.cpu(), trainlabels.cpu())\n",
    "\n",
    "predicted_labels = logreg.predict(testcodes.cpu())  # (10000,)\n",
    "\n",
    "# Compute accuracy of the linear classifier\n",
    "accuracy = np.sum(testlabels.cpu().numpy() == predicted_labels) / predicted_labels.size\n",
    "print('Accuracy with a linear classifier: %.2f%%' % (accuracy*100))\n",
    "assert accuracy > .85, \"Poor accuracy of the embeddings: classification accuracy is %.2f%%\" % (accuracy*100)\n",
    "print('Success')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddefdc9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e0901166868f0ad5298a8aa26e5312c4",
     "grade": false,
     "grade_id": "cell-25a9be66b37700ac",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    },
    "tags": []
   },
   "source": [
    "### Question: Three latent characters (3 points)\n",
    "\n",
    "In a bustling generative modeling lab, three essential latent characters$z_1, z_2$, and $z_3$work together to generate lifelike images. Each character is responsible for a distinct artistic feature: $z_1$ controls the overall structure (the blueprint), $z_2$ paints the textures (the palette), and $z_3$ adds the final touches (the shading). The true creative process is captured by the joint distribution:\n",
    "\n",
    "$$p(z_1, z_2, z_3)$$\n",
    "\n",
    "which embodies all the complicated dependencies among these variables. However, the team decided to use a factorized approximation to simplify computations and speed up generation,\n",
    "\n",
    "$$q(z_1, z_2, q_3) = q(z_1) q(z_2) q(z_3)$$\n",
    "\n",
    "which assumes that each latent variable acted independently. The team must select $q(z_1)$, $q(z_2)$, and $q(z_3)$ carefully such that the overall divergence from the true process is minimized.\n",
    "\n",
    "$$\n",
    "\\text{KL}(p \\mid \\mid q) = \\int p(z_1, z_2, z_3) \\log \\frac{p(z_1, z_2, z_3)}{q(z_1, z_2, z_3)} d\\boldsymbol{z}\n",
    "$$\n",
    "\n",
    "**Your task** is to help the team find $q(z_i)$ for $i \\in \\{1,2,3\\}$ such that forward KL divergence is minimized by providing a mathematical proof. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cf9813-57f8-47c9-aef2-e7247b0ba27a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2fddfd02db89a2184a37aeaeaec8446b",
     "grade": false,
     "grade_id": "cell-df8095201544ea96",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34442e0",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "22b299ae9495de358e96bcb7adcd6ef9",
     "grade": true,
     "grade_id": "cell-b0f69eff340dcd24",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb1cf53",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7ea133d6b7024d3f3a93c7bb013246d1",
     "grade": true,
     "grade_id": "cell-143f743a387eff30",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert skip_training, \"Set skip_training = True before submitting the assignment.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af90d17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
