{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f645c65b-7a67-490b-9f76-91ed4b7f2020",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "292eb8ff2a8709151f45a0d63c10a9f5",
     "grade": false,
     "grade_id": "cell-4e348378b219c1f2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import heapq\n",
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import sentencepiece as spm\n",
    "\n",
    "def set_seeds(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  # For CUDA\n",
    "    torch.cuda.manual_seed_all(seed)  # If you are using multi-GPU.\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seeds(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8662695-b2f9-4f7d-abe4-ed3008afa48b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "skip_training = False  # Set this flag to True before validation and submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770d75be-e482-42cd-b9d4-3221db46cb93",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c520edd0550544732a1224c716b67c1c",
     "grade": true,
     "grade_id": "cell-7f7445a41652de60",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# During evaluation, this cell sets skip_training to True\n",
    "# skip_training = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92de0119-73c0-43e0-b66b-67141e347927",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "871c622644ab8c86ecf5d40171eeea7d",
     "grade": false,
     "grade_id": "cell-433d00d2bde428e1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Exercise 3: n-gram language modeling (2p)\n",
    "\n",
    "In this exercise, we will fit a simple n-gram language model to the Shakespeare dataset. The simplified BPE tokenizer implemented in the previous exercise does not handle whitespace properly. A correct BPE tokenizer should avoid tokens spanning multiple words. Therefore, we will use the standard SentencePiece library to build a proper BPE tokenizer.\n",
    "\n",
    "First, let's train the tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52944f6-3035-4a87-b6b7-7479ec7cc79e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "499bdf885ec8b196eae30942cc05c907",
     "grade": false,
     "grade_id": "cell-1d791257edb6f0af",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check if the file already exists\n",
    "if not os.path.exists(\"input.txt\"):\n",
    "    !wget https://raw.githubusercontent.com/karpathy/char-rnn/refs/heads/master/data/tinyshakespeare/input.txt\n",
    "else:\n",
    "    print(\"input.txt already exists. Skipping download.\\n\")\n",
    "\n",
    "# Define parameters\n",
    "model_prefix = \"shakespeare_tokenizer\"\n",
    "vocab_size = 512\n",
    "filename = \"input.txt\"\n",
    " \n",
    "# Train SentencePiece tokenizer\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input=filename,\n",
    "    model_prefix=f\"{model_prefix}_{vocab_size}\",\n",
    "    vocab_size=vocab_size,\n",
    "    model_type=\"bpe\",\n",
    "    max_sentence_length=4096,\n",
    "    minloglevel=2,\n",
    "    hard_vocab_limit=False,\n",
    "    normalization_rule_name=\"nmt_nfkc\",\n",
    "    remove_extra_whitespaces=True,\n",
    "    character_coverage=1.0,\n",
    "    num_threads=1, \n",
    ")\n",
    "\n",
    "print(f\"Tokenizer creation complete. Model files generated: {model_prefix}_{vocab_size}.model, {model_prefix}_{vocab_size}.vocab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5a5fed-9c5b-4ac6-87ba-5fc7c71fcfb3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dda0f9a5aea4f82527cfa4246265700d",
     "grade": false,
     "grade_id": "cell-f2b65df6b27293af",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Let's verify that the tokenizer tokenizes words reasonably:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5d9581-c5b4-4a29-87f4-5ee8757e6a11",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2d1233427734ad47228b50a1cd172a1c",
     "grade": false,
     "grade_id": "cell-6b9c076dd3523cda",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sp_model = spm.SentencePieceProcessor()\n",
    "sp_model.load(f\"shakespeare_tokenizer_{vocab_size}.model\")\n",
    "print(f\"Tokenizer loaded successfully from shakespeare_tokenizer_{vocab_size}.model\\n\")\n",
    "\n",
    "text = \"This is a test sentence for tokenization.\"\n",
    "print(f\"Let us test how the following sentence gets tokenized:\\n{text}\")\n",
    "tokens = sp_model.encode(text, out_type=str)  # Get tokenized output as strings\n",
    "print(\"\\nTokenized output:\", tokens)\n",
    "\n",
    "# Decode back to original text\n",
    "decoded_text = sp_model.decode(tokens)\n",
    "print(\"Decoded text:\", decoded_text)\n",
    "\n",
    "print()\n",
    "print(\"Vocabulary size:\", sp_model.get_piece_size())\n",
    "\n",
    "for i in range(10):  # Show first 10 tokens\n",
    "    print(f\"Token {i}: {sp_model.id_to_piece(i)}\")\n",
    "\n",
    "oov_word = \"unseenword\"\n",
    "tokens = sp_model.encode(oov_word, out_type=str)\n",
    "print(f\"Tokenized OOV word ({oov_word}):\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7e78ed-07c2-4250-b505-40cc3148f79b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0f110013a84d09a4feed924fff68cf87",
     "grade": false,
     "grade_id": "cell-3733e21c69153da0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Now, letâ€™s load and tokenize the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021c20ab-3fec-459e-85f8-2b55b62bf9a0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0204b03acf3dea7c4496a8949373133d",
     "grade": false,
     "grade_id": "cell-be9bd06dab81f2bc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_dataset(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()  # Read entire dataset as one large string\n",
    "\n",
    "# Tokenize entire dataset continuously\n",
    "def tokenize_text(text, sp_model):\n",
    "    return sp_model.encode(text, out_type=str)  # Get tokens as a single list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bc34e2-6c60-4bcd-8e30-f4f76967c12f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "98ea6bf820e61066965d71b1c7ffdd74",
     "grade": false,
     "grade_id": "cell-583b433303ed4431",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Next, we implement the n-gram language model. Start by defining build_ngram_counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5077e5a5-457b-4975-b783-1628c5757dcb",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f0987e4d6adc4fdac1e6d71a65a6be6e",
     "grade": false,
     "grade_id": "cell-369b2daf82c231e1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_ngram_counts(token_stream, n):\n",
    "    \"\"\"\n",
    "    Builds frequency counts for n-grams and their contexts from a sequence of tokens.\n",
    "\n",
    "    This function scans through the given token stream and collects:\n",
    "      - Counts of each n-gram (sequence of n consecutive tokens)\n",
    "      - Counts of (n-1)-gram contexts (used for calculating probabilities)\n",
    "      - The set of unique tokens (vocabulary), useful for smoothing\n",
    "\n",
    "    Parameters:\n",
    "        token_stream (list or iterable): A sequence of tokens (e.g., token IDs or words).\n",
    "        n (int): Size of the n-gram (e.g., 2 for bigrams, 3 for trigrams).\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - ngram_counts (dict): Maps each n-gram (tuple) to its frequency.\n",
    "            - context_counts (dict): Maps each (n-1)-gram context to its frequency.\n",
    "            - vocabulary (set): The set of unique tokens in the input stream.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d0c7ec-b0b4-47d6-afac-395c53b35a39",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2930d6b23768702f215cf779be170524",
     "grade": true,
     "grade_id": "cell-1740a047314a2c76",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "token_stream = [1, 2, 3, 1]\n",
    "n = 2\n",
    "ngram_counts, context_counts, vocabulary = build_ngram_counts(token_stream, n)\n",
    "assert ngram_counts == {(1, 2): 1, (2, 3): 1, (3, 1): 1}, \"Ngram counts from build_ngram_counts are incorrect for test with n=2\"\n",
    "# Note that the final one is not counted in the context counts\n",
    "assert context_counts == {(1,): 1, (2,): 1, (3,): 1}, \"Context counts from build_ngram_counts are incorrect for test with n=2\"\n",
    "assert vocabulary == {1, 2, 3}, \"The vocabulary returned by build_ngram_counts is incorrect for test with n=2\"\n",
    "\n",
    "token_stream = [4, 5, 4, 5, 6]\n",
    "n = 3\n",
    "ngram_counts, context_counts, vocabulary = build_ngram_counts(token_stream, n)\n",
    "assert ngram_counts == {\n",
    "    (4, 5, 4): 1,\n",
    "    (5, 4, 5): 1,\n",
    "    (4, 5, 6): 1\n",
    "}, \"Ngram counts from build_ngram_counts are incorrect for test with n=3\"\n",
    "assert context_counts == {\n",
    "    (4, 5): 2,\n",
    "    (5, 4): 1\n",
    "}, \"Context counts from build_ngram_counts are incorrect for test with n=3\"\n",
    "assert vocabulary == {4, 5, 6}, \"The vocabulary returned by build_ngram_counts is incorrect for test with n=3\"\n",
    "\n",
    "token_stream = [7, 8, 9, 7]\n",
    "n = 1\n",
    "ngram_counts, context_counts, vocabulary = build_ngram_counts(token_stream, n)\n",
    "assert ngram_counts == {\n",
    "    (7,): 2,\n",
    "    (8,): 1,\n",
    "    (9,): 1\n",
    "}, \"Ngram counts from build_ngram_counts are incorrect for test with n=1\"\n",
    "assert context_counts == {\n",
    "    (): 4  # Four tokens total, so the empty context occurs 4 times\n",
    "}, \"Context counts from build_ngram_counts are incorrect for test with n=1\"\n",
    "assert vocabulary == {7, 8, 9}, \"The vocabulary returned by build_ngram_counts is incorrect for test with n=1\"\n",
    "\n",
    "print(\"Tests pass - success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d95f717-cac4-4821-8066-44ed97013781",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ba87dfd7dbf7a0aed94ad69cf34d37c6",
     "grade": false,
     "grade_id": "cell-acda7d46d5b430b4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Then, define compute_ngram_probs_laplace to apply Laplace smoothing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe50296-d991-4f96-8450-b37f397a227f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "38d9020ae30dced9f342c136c3a85426",
     "grade": false,
     "grade_id": "cell-20ae2539df4a317d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_ngram_probs_laplace(ngram_counts, context_counts, vocabulary, n):\n",
    "    \"\"\"\n",
    "    Computes Laplace-smoothed probabilities for n-grams.\n",
    "\n",
    "    This function calculates the probability of each n-gram using Laplace (add-one) \n",
    "    smoothing to handle unseen events. For each n-gram, 1 is added to its count, \n",
    "    and the size of the vocabulary (V) is added to the context count in the denominator.\n",
    "\n",
    "    Parameters:\n",
    "        ngram_counts (dict): Maps n-grams to their counts.\n",
    "        context_counts (dict): Maps (n-1)-gram contexts to their counts.\n",
    "        vocabulary (set): Unique tokens (used for vocabulary size).\n",
    "        n (int): Size of the n-grams.\n",
    "\n",
    "    Returns:\n",
    "        dict: Maps each n-gram to its Laplace-smoothed probability.\n",
    "\n",
    "    Formula, where w stands for token:\n",
    "        P(w_n | w_1...w_{n-1}) = (count(w_1...w_n) + 1) / (count(w_1...w_{n-1}) + V)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4ec8c2-32f2-4c2e-a613-9d0b00e23cbf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3d1082e3c784618138448d3993c5ce69",
     "grade": true,
     "grade_id": "cell-088254a1f34c6360",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# n=2 test\n",
    "tokens = [1, 2, 3, 1]\n",
    "n = 2\n",
    "ngram_counts = {(1, 2): 1, (2, 3): 1, (3, 1): 1}\n",
    "context_counts = {(1,): 1, (2,): 1, (3,): 1}\n",
    "vocabulary == {1, 2, 3}\n",
    "\n",
    "ngram_probs = compute_ngram_probs_laplace(ngram_counts, context_counts, vocabulary, n)\n",
    "expected_ngram_probs = {\n",
    "    (1, 2): 0.5,\n",
    "    (2, 3): 0.5,\n",
    "    (3, 1): 0.5\n",
    "}\n",
    "assert ngram_probs == expected_ngram_probs, \"Expected ngram probs differ from what the function compute_ngram_probs_laplace returns for n=2\"\n",
    "\n",
    "# n=3 test\n",
    "tokens = [4, 5, 4, 5, 6]\n",
    "n = 3\n",
    "ngram_counts = {\n",
    "    (4, 5, 4): 1,\n",
    "    (5, 4, 5): 1,\n",
    "    (4, 5, 6): 1,\n",
    "}\n",
    "context_counts = {\n",
    "    (4, 5): 2,\n",
    "    (5, 4): 1\n",
    "}\n",
    "vocabulary = {4, 5, 6}\n",
    "\n",
    "ngram_probs = compute_ngram_probs_laplace(ngram_counts, context_counts, vocabulary, n)\n",
    "expected_ngram_probs = {\n",
    "    (4, 5, 4): 0.4,\n",
    "    (5, 4, 5): 0.5,\n",
    "    (4, 5, 6): 0.4,\n",
    "}\n",
    "\n",
    "assert ngram_probs == expected_ngram_probs, \"Expected ngram probs differ from what the function compute_ngram_probs_laplace returns for n=3\"\n",
    "\n",
    "# n=1 test\n",
    "tokens = [7, 8, 9, 7]\n",
    "n = 1\n",
    "ngram_counts = {\n",
    "    (7,): 2,\n",
    "    (8,): 1,\n",
    "    (9,): 1\n",
    "}\n",
    "context_counts = {\n",
    "    (): 4  # Empty tuple as context for unigrams\n",
    "}\n",
    "vocabulary = {7, 8, 9}\n",
    "\n",
    "ngram_probs = compute_ngram_probs_laplace(ngram_counts, context_counts, vocabulary, n)\n",
    "expected_ngram_probs = {\n",
    "    (7,): 3/7,\n",
    "    (8,): 2/7,\n",
    "    (9,): 2/7,\n",
    "}\n",
    "\n",
    "assert ngram_probs == expected_ngram_probs, \"Expected ngram probs differ from what the function compute_ngram_probs_laplace returns for n=1\"\n",
    "print(\"Tests pass - success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e3d870-21c4-4150-ba91-7060792a4048",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f1ea35d711be5a4ea33d455e95d1f0c1",
     "grade": false,
     "grade_id": "cell-28c01758ba3a156d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "We evaluate our model using a simplified perplexity function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5d6eaa-87d5-4ae4-99bc-c8b05edb417c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9d8177468729084d7ca0d9e112b6a9d2",
     "grade": false,
     "grade_id": "cell-f1737c6d38f0da29",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_perplexity(token_stream, ngram_probs, vocabulary, n):\n",
    "    \"\"\"\n",
    "    Computes perplexity given n-gram probabilities.\n",
    "\n",
    "    Parameters:\n",
    "        token_stream (list): Sequence of tokens.\n",
    "        ngram_probs (dict): Maps n-grams to probabilities.\n",
    "        vocabulary (set): Vocabulary set, for unseen tokens.\n",
    "        n (int): n-gram order.\n",
    "\n",
    "    Returns:\n",
    "        float: Perplexity score.\n",
    "    \"\"\"\n",
    "    log_prob_sum = 0\n",
    "    num_ngrams = 0\n",
    "    V = len(vocabulary)\n",
    "\n",
    "    for i in range(len(token_stream) - n + 1):\n",
    "        ngram = tuple(token_stream[i : i + n])\n",
    "        prob = ngram_probs.get(ngram, 1 / V)  # Unseen n-grams get a small probability\n",
    "        log_prob_sum += np.log(prob)\n",
    "        num_ngrams += 1\n",
    "\n",
    "    perplexity = np.exp(-log_prob_sum / num_ngrams) if num_ngrams > 0 else float(\"inf\")\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344dd73e-6f4e-450f-9691-9ec0a7a615ed",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e812aa9e123d662bff5450e3c20d0424",
     "grade": false,
     "grade_id": "cell-50de929d8b337024",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Then, we train and evaluate our n-gram models. We train multiple n-gram models for higher quality sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd758a9f-76ad-4050-8603-c5eb3615846c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d8f69d77288879f1ea3b7e7b30386e59",
     "grade": true,
     "grade_id": "cell-ea5280ecf71e1530",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sp_model = spm.SentencePieceProcessor(model_file=f\"shakespeare_tokenizer_{vocab_size}.model\")\n",
    "\n",
    "n_max = 5\n",
    "\n",
    "text = load_dataset(\"input.txt\")\n",
    "tokens = tokenize_text(text, sp_model)\n",
    "\n",
    "split_idx = int(0.98 * len(tokens))\n",
    "train_tokens = tokens[:split_idx]\n",
    "test_tokens = tokens[split_idx:]\n",
    "\n",
    "# Train n-gram models for all n in [1, 2, ..., n_max]\n",
    "all_ngram_probs = {}\n",
    "vocabulary = set(train_tokens)  # Shared vocab across models\n",
    "\n",
    "print(f\"Total tokens: {len(train_tokens)}\")\n",
    "print(\"Building n-gram model on data...\")\n",
    "\n",
    "for n in range(1, n_max + 1):\n",
    "    print(f\"  - Training {n}-gram model...\")\n",
    "    ngram_counts, context_counts, _ = build_ngram_counts(train_tokens, n)\n",
    "    ngram_probs = compute_ngram_probs_laplace(ngram_counts, context_counts, vocabulary, n)\n",
    "    all_ngram_probs[n] = ngram_probs\n",
    "\n",
    "perplexity = compute_perplexity(test_tokens, all_ngram_probs[n_max], vocabulary, n_max)\n",
    "print(f\"Perplexity for vocab size {vocab_size}, n={n_max}: {perplexity:.2f}\")\n",
    "assert np.abs(perplexity - 390.80) < 5, f\"Expected perplexity ~390.80, got {perplexity:.2f}\"\n",
    "print(\"Tests pass - success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c80a89-aed4-45fc-9a7f-c5d8c12241af",
   "metadata": {
    "tags": []
   },
   "source": [
    "For improved generation quality, we implement stupid backoff. The idea is that if the n-gram hasn't been seen in the training dataset, we are resorting down to n-1-gram level to avoid having to perform uniform sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04901da1-3e2a-4228-b787-0d38e2f27836",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_backoff_candidates(context, all_ngram_probs, n, alpha=0.4):\n",
    "    \"\"\"\n",
    "    Finds candidate tokens using stupid backoff from n-gram down to unigram.\n",
    "    \"\"\"\n",
    "    candidates = {}\n",
    "    for backoff_n in range(n, 0, -1):\n",
    "        context_ngram = context[-(backoff_n - 1):] if backoff_n > 1 else ()\n",
    "        probs = all_ngram_probs.get(backoff_n, {})\n",
    "\n",
    "        for ngram, prob in probs.items():\n",
    "            if ngram[:-1] == context_ngram:\n",
    "                token = ngram[-1]\n",
    "                discounted_prob = prob * (alpha ** (n - backoff_n))\n",
    "                candidates[token] = max(candidates.get(token, 0), discounted_prob)\n",
    "\n",
    "        if candidates:\n",
    "            break  # Prefer higher-order context when available\n",
    "\n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce28f8e-f8aa-483f-93f1-2d1f7eb0b98f",
   "metadata": {},
   "source": [
    "You can experiment with a couple of different sampling strategies. Each strategy takes a list of possible tokens and their probabilities as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f218d88a-4c9d-4acd-a6a7-3a2c78ed2539",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sample_random(candidates, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Applies temperature scaling to the probability distribution.\n",
    "    Returns a dictionary mapping tokens to adjusted probabilities.\n",
    "    \"\"\"\n",
    "    if temperature <= 0:\n",
    "        raise ValueError(\"Temperature must be greater than 0.\")\n",
    "    \n",
    "    scaled_probs = {token: prob ** (1.0 / temperature) for token, prob in candidates.items()}\n",
    "    total_prob = sum(scaled_probs.values())\n",
    "    normalized_probs = {token: prob / total_prob for token, prob in scaled_probs.items()}\n",
    "    \n",
    "    return normalized_probs\n",
    "\n",
    "def sample_top_k(candidates, top_k):\n",
    "    \"\"\"Top-k sampling: selects from the k most probable tokens.\"\"\"\n",
    "    top_candidates = heapq.nlargest(top_k, candidates.items(), key=lambda x: x[1])\n",
    "    return dict(top_candidates)\n",
    "\n",
    "def sample_top_p(candidates, top_p):\n",
    "    \"\"\"Top-p (nucleus) sampling: selects from the smallest set of tokens whose cumulative probability exceeds p.\"\"\"\n",
    "    sorted_candidates = sorted(candidates.items(), key=lambda x: x[1], reverse=True)\n",
    "    cumulative_prob = 0.0\n",
    "    top_candidates = []\n",
    "    for token, prob in sorted_candidates:\n",
    "        top_candidates.append((token, prob))\n",
    "        cumulative_prob += prob\n",
    "        if cumulative_prob >= top_p:\n",
    "            break\n",
    "    return dict(top_candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbaeaee-4e49-4530-8c86-344e5c94670b",
   "metadata": {},
   "source": [
    "Finally, define sample_text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f175ef37-cb3e-4930-8ef6-6a3b28aec1dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sample_text(test_tokens, ngram_probs, n, seed=None, max_tokens=50, strategy='random', top_k=5, top_p=0.9, temperature=1.0, alpha=0.4):\n",
    "    \"\"\"\n",
    "    Generates text autoregressively from the trained n-gram model with different sampling strategies.\n",
    "\n",
    "    Args:\n",
    "        test_tokens: Initializations for sampling\n",
    "        ngram_probs: Dictionary of n-gram probabilities.\n",
    "        n: Order of the n-gram model (e.g., 3 for trigrams).\n",
    "        seed: Optional seed sequence (list of tokens) to start the generation.\n",
    "        max_tokens: Maximum number of tokens to generate.\n",
    "        strategy: Sampling strategy ('random', 'top_k', 'top_p').\n",
    "        top_k: Number of top candidates to sample from (used in top-k sampling).\n",
    "        top_p: Cumulative probability threshold for nucleus sampling (used in top-p sampling).\n",
    "        temperature: Temperature parameter for random sampling.\n",
    "\n",
    "    Returns:\n",
    "        A generated sequence as a list of tokens.\n",
    "    \"\"\"\n",
    "    # If no seed is provided, pick a random (n-1)-length window from test_tokens\n",
    "    if seed is None:\n",
    "        if len(test_tokens) < n - 1:\n",
    "            raise ValueError(\"Not enough test tokens to create a seed of length (n-1).\")\n",
    "        start_idx = random.randint(0, len(test_tokens) - (n - 1))\n",
    "        seed = test_tokens[start_idx : start_idx + (n - 1)]\n",
    "\n",
    "    generated_tokens = list(seed)  # Start with the seed context\n",
    "\n",
    "    for _ in range(max_tokens):\n",
    "        context = tuple(generated_tokens[-(n - 1):])\n",
    "\n",
    "        # Search for backoff candidates from n-gram down to unigram\n",
    "        candidates = get_backoff_candidates(context, all_ngram_probs, n, alpha)\n",
    "\n",
    "        if not candidates:\n",
    "            break  # No valid next-token candidates\n",
    "        \n",
    "        if strategy == 'top_k':\n",
    "            token_probs = sample_top_k(candidates, top_k)\n",
    "        elif strategy == 'top_p':\n",
    "            token_probs = sample_top_p(candidates, top_p)\n",
    "        else:\n",
    "            token_probs = sample_random(candidates, temperature)\n",
    "        \n",
    "        tokens, probs = zip(*token_probs.items())\n",
    "        next_token = random.choices(tokens, weights=probs)[0]\n",
    "        \n",
    "        generated_tokens.append(next_token)\n",
    "    \n",
    "    return generated_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf81f7e2-9de3-4773-8558-e2f3fce73edd",
   "metadata": {},
   "source": [
    "Let's generate continuations from our test set using top-k sampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f9b5ee-e1c6-4e0f-84e4-0662e40720bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "    for i in range(3):\n",
    "        generated_text = sample_text(test_tokens, all_ngram_probs, n, max_tokens=100, strategy='top_k', top_k=10, temperature=0.8, alpha=0.4)\n",
    "        print(\"Generated Text:\", \"\".join(generated_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677b0dfd-0797-4931-9839-05fc7d88680d",
   "metadata": {},
   "source": [
    "The generated text generally exhibits correct grammar but lacks logical coherence due to the very limited context window inherent to n-gram models. It can mimic local linguistic patterns and surface-level grammar but struggles with long-range dependencies or deeper semantic understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e7e5bd-0670-4833-aa19-72c3fc0ae741",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
