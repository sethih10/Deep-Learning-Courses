{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a477e64-7f62-4f97-8a6a-e4b13173d34f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f8ee64f82f096b8e1dd17017622c5d22",
     "grade": false,
     "grade_id": "cell-0b5d1fe7034bd914",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cfabdb-605b-4176-97f6-7f030278be72",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d5b563d982d6c3124763e8384bcca4f3",
     "grade": false,
     "grade_id": "cell-43586f27a7840afc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Exercise 1. Total variation\n",
    "\n",
    "In parametric statistics we are interested in estimating parameters of the distribution. In order to do that, we need to define an appropriate distance metric between the distributions. Such a common metric in classical statistics is total variation, which is defined as $TV(P, Q) = \\max_{A \\in E}{|P(A) - Q(A)|}$, where $E$ is the union of domains of $P$ and $Q$.  Total variation is a proper distance, meaning that it satisfies the following properties:\n",
    "$$\n",
    "TV(P, Q) = TV(Q,P) \\ (\\text{symmetric}) \\\\\n",
    "TV(P, Q) \\geq 0 \\ (\\text{positive}) \\\\\n",
    "\\text{if} \\ TV(P, Q) = 0, \\ \\text{then} \\  P = Q \\ (\\text{definite}) \\\\\n",
    "TV(P, Q) \\leq TV(P, K) + TV(K, Q) \\ (\\text{triangle inequality})\n",
    "$$\n",
    "\n",
    "$\\textbf{Your task is to prove these properties} \\\\ $\n",
    "Write your solutions in LateX or attach a picture in the answer cell provided below. You can add a picture using the command ```![1](imagename_in_the_folder.jpg)```. Latex in here works similarly as you would write it normally!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0904caeb-6122-4a73-8b7a-65a4d3eca67c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "561d3be0758fd7159f91394f21f2d72e",
     "grade": true,
     "grade_id": "cell-c216e65158be1397",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ffe3ad-2f28-42be-81c2-09fefdcd6d2c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ae4bf91f1769ce221184963bad403517",
     "grade": false,
     "grade_id": "cell-5726e621df67db46",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "When both $P$ and $Q$ are either discrete or continuous, TV can be computed as $\\frac{1}{2}{\\sum_{x \\in E}}{|P(x) - Q(x)|}$ or $\\frac{1}{2}{\\int_{x \\in E}}{|P(x) - Q(x)| dx}$. $ \\\\ $ However, despite the intuative definition, TV has a major disadvantage - we cannot easily construct an estmator out of it, when we have access only to samples of target distribution $P$ and our goal is to infer parameters of $Q$. Hence, we need to define more appealing metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c97f1e-da44-4053-a70b-ae6bcd2c894d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "75d7bb0cedc48377cc1f2f4512711dd5",
     "grade": false,
     "grade_id": "cell-8feb9fbd3b784e8d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Exercise 2. KL divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08998309-6bb2-4cd0-96d5-a475e839af86",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2def3fb5304ac030b8b26c95a03474ee",
     "grade": false,
     "grade_id": "cell-224a5a0553bb5202",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "KL divergence is, probably, the most popular statistical metric for measuring how $P$ is close to $Q$. Mathematically KL is not a distance, in general it does not satisfy properties of symmetry and triangle inequality, however, it does not really affect the applicability of this metric. KL is still positive and definite and positiviness is proved by employing Jensen's inequality. This inequality appears a lot when deriving lower bounds for latent variable models (next lectures), so lets focus on it a little bit.\n",
    "\n",
    "### Jensen’s inequality\n",
    "\n",
    "if $X$ is a random variable and $f$ is a convex function, then\n",
    "$$\n",
    "f(\\mathbb{E}[f(X)]) \\leq \\mathbb{E}[f(X)]\n",
    "$$\n",
    "\n",
    "A differentiable real-valued function f of a single variable is called convex if the first order Taylor approximation of f is an underestimate of the function, that is\n",
    "$$\n",
    "f(c) + (x-c) f'(c) \\leq f(x)\n",
    "$$\n",
    "for every $c$ and $x$.\n",
    "\n",
    "This definition of the convexity differs from the canonical one, however is more useful for proving inequality. $\\textbf{Your task is to prove Jensen’s inequality}$. (Hint: choose $c$ as $\\mathbb{E}[X])$. Write your solutions in LateX or attach a picture in the answer cell provided below. You can add a picture using the command ```![1](imagename_in_the_folder.jpg)```. Latex in here works similarly as you would write it normally!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da05043-d0d3-475b-be9f-416d326e4729",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9dfe4dbfa27fa190521e9c0810a8a35a",
     "grade": true,
     "grade_id": "cell-be35d4fa223fbc4e",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc38b036-8334-4110-9b3c-cc7afccb86e7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9610fd55dee2585de6850eea260980a9",
     "grade": false,
     "grade_id": "cell-b88274ac8d9d9309",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "To start exploring KL, we will consider a situation, where we want to minimise KL between two distributions: the target distribution $p$ is known to us and for the approximation distribution $q$ we know its parametric form. For simplicity, we will assume that both distributions belong to the Gaussian family. $\\textbf{Your task is to optimise parameters of $q$ using the gradient descent}$. Below N(a, b) corresonds to normal distribution with mean $a$ and variance $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805203e1-4846-4943-9fa9-24aca2885887",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "071c371aa0ee5f88b0a34eb88442a41f",
     "grade": false,
     "grade_id": "cell-dd1a9a8caebac96b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def KL(mean, log_var, mean_target, log_var_target):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        mean: mean of normal distribution q which we optimise\n",
    "        log_var: log of variance of normal distribution q which we optimise\n",
    "        mean_target: mean of target normal distribution p which is known\n",
    "        log_var_target: log of variance of target normal distribution p which is known\n",
    "        \n",
    "    Returns:\n",
    "        KL between p and q\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return kl\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4490d637-2693-47de-8738-b9f924bf5a1d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b847db2fc57379a97b8ef85d37c6dd3a",
     "grade": true,
     "grade_id": "cell-8e3b2637477e5bea",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "res = KL(torch.tensor(0),torch.tensor(1).log(),torch.tensor(0),torch.tensor(3).log()).item()\n",
    "eps = 0.001\n",
    "\n",
    "diff = abs(res - 0.4506938)\n",
    "assert diff < eps, \\\n",
    "print(f\"KL between N(0,1) and N(0, 3) do not allign, the absolute value difference is {diff}\")\n",
    "print(\"Success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0549e0de-8d19-418d-b784-c99e1ae654af",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "694daea4b13f3a4706cef89f13a4d71f",
     "grade": true,
     "grade_id": "cell-04ef22e750c7dc7d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Here is a hidden test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3fe19c-bd1b-4c30-b7cc-26b4c442f0d2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8456bafdb1c79c552ea551c938d0e2d7",
     "grade": false,
     "grade_id": "cell-889f617f7aaab589",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mean = torch.nn.Parameter(torch.randn(1))\n",
    "log_var = torch.nn.Parameter(torch.randn(1))\n",
    "\n",
    "mean_target = torch.tensor(5)\n",
    "log_var_target = torch.log(torch.tensor(2.0))\n",
    "\n",
    "num_iterations = 5000\n",
    "optimiser = torch.optim.Adam([mean, log_var], lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8f7196-e4a4-439e-bbb2-b7c8d40fda05",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "32eebec03bea6994d5a6f303c894ea62",
     "grade": false,
     "grade_id": "cell-48014dd9a2c54b90",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(num_iterations):\n",
    "    optimiser.zero_grad()\n",
    "    loss = KL(mean, log_var, mean_target, log_var_target)\n",
    "    if i % 100 == 0:\n",
    "        print('Iter %d/%d - KL Loss: %.3f' % (i, num_iterations, loss.item()), flush=True)\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "\n",
    "print(\"optimised mean: \", mean.item())\n",
    "\n",
    "print(\"optimised variance: \", log_var.exp().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c69da38-b0ea-4cd4-bdce-fb7b69c646e9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "53e47656af17dc6fd78f99e18628bbd1",
     "grade": true,
     "grade_id": "cell-896d9b5857d6c669",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "eps = 0.1\n",
    "assert abs(mean.detach().item() - mean_target) < eps, \\\n",
    "print(f\"Means do not allign, the absolute value difference is {abs(mean.detach().item() - mean_target)}\")\n",
    "assert abs(log_var.detach().item() - log_var_target) < eps,\\\n",
    "print(f\"log variances do not allign, the absolute value difference is {abs(log_var.detach().item() - log_var_target)}\")\n",
    "\n",
    "print(\"Success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aea7ddf-0d0d-43b2-94a2-113f85308e25",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5b1c9f09f3adbb536204cc6e6e707114",
     "grade": false,
     "grade_id": "cell-188d1d60bfaebfe4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Lets draw the resulting densities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8efc73e-389b-42b5-9072-0fc12b49a23b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "708aa5a32bca0600b4b98f95f54c00e1",
     "grade": false,
     "grade_id": "cell-0974bdaa069c749b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = np.linspace(0,10,1000)\n",
    "target_pdf = scipy.stats.norm(mean_target, log_var_target.exp().sqrt().numpy()).pdf(x)\n",
    "fit_pdf = scipy.stats.norm(mean.detach().numpy(), log_var.exp().sqrt().detach().numpy()).pdf(x)\n",
    "\n",
    "plt.plot(x, target_pdf, label=\"target\")\n",
    "plt.plot(x, fit_pdf, label=\"fit\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7f0d58-2715-400e-bcc3-3145a975e4e1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "81c6e8d11e036fbd586ec2975eaf601c",
     "grade": false,
     "grade_id": "cell-f2cccf077361331d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "In most real cases, of course, we are not provided with the target distribution, but rather can observe samples from it. However, we can still define a parametric form for $q$ and minimise KL between emperical distribution of the data and $q$. Assume that $q$ belongs to the Gaussian family again. $\\textbf{Your task is to optimise parameters of $q$ using the gradient descent}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10be03f-4c73-405f-b08a-e14f83cb105f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2292d966edb0e94ca921e5123a9c5618",
     "grade": false,
     "grade_id": "cell-c461e21698e44dcb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss_function(mean, log_var, samples, N):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        mean: mean of normal distribution q which we optimise\n",
    "        log_var: log of variance of normal distribution q which we optimise\n",
    "        samples: samples from the unknown distribution p\n",
    "        N: number of samples\n",
    "        \n",
    "    Returns:\n",
    "        objective for minimising KL\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e3cac9-0e91-43f3-a7b3-b3d8a1b0eb1e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c88bd103e597785dddb89a338812c802",
     "grade": false,
     "grade_id": "cell-d28dc7967d72d2c2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mean = torch.nn.Parameter(torch.randn(1))\n",
    "log_var = torch.nn.Parameter(torch.randn(1))\n",
    "optimiser = torch.optim.Adam([mean, log_var], lr=1e-2)\n",
    "N = 50000\n",
    "samples = torch.randn(N) * torch.sqrt(log_var_target.exp()) + mean_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4b25e3-2945-46c0-b73f-04082d2bbe82",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ba81acf46a9e9ad06c479ec10751b462",
     "grade": false,
     "grade_id": "cell-7c4900f9d95d13fb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(num_iterations):\n",
    "    optimiser.zero_grad()\n",
    "    loss = loss_function(mean, log_var, samples, N)\n",
    "    if i % 100 == 0:\n",
    "        print('Iter %d/%d - KL Loss: %.3f' % (i, num_iterations, loss.item()), flush=True)\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "\n",
    "print(\"optimised mean: \", mean.item())\n",
    "\n",
    "print(\"optimised variance: \", log_var.exp().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2244b47d-c5c1-4c3c-8cd1-1f4092329023",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4dcb333a5961db3f7420b8481efc3b21",
     "grade": true,
     "grade_id": "cell-960e5b011962fa1d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "eps = 0.1\n",
    "assert abs(mean.detach().item() - mean_target) < eps, \\\n",
    "print(f\"Means do not allign, the absolute value difference is {abs(mean.detach().item() - mean_target)}\")\n",
    "assert abs(log_var.detach().item() - log_var_target) < eps,\\\n",
    "print(f\"log variances do not allign, the absolute value difference is {abs(log_var.detach().item() - log_var_target)}\")\n",
    "\n",
    "print(\"Success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051f43a7-eccd-497e-84c8-c476de4d7427",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d6ef895fe2ec26f725a924ac7f5eee5d",
     "grade": false,
     "grade_id": "cell-a8a04f19a8e0c35b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Lets draw the resulting densities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03985295-d1d9-41a1-8170-d217de3134f2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "739d92c1772694a0103d56be8e435020",
     "grade": false,
     "grade_id": "cell-cc53186929620625",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_pdf = scipy.stats.norm(mean_target, log_var_target.exp().sqrt().numpy()).pdf(x)\n",
    "fit_pdf = scipy.stats.norm(mean.detach().numpy(), log_var.exp().sqrt().detach().numpy()).pdf(x)\n",
    "\n",
    "plt.plot(x, target_pdf, label=\"target\")\n",
    "plt.plot(x, fit_pdf, label=\"fit\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232533ac-0289-455d-85fc-f50ff7c20b55",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "13d198d4be87b5ce59b24b79b5668f33",
     "grade": false,
     "grade_id": "cell-923f8bdfa9c92ebd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Exercise 3. Jensen-Shannon divergence\n",
    "\n",
    "KL is not the only divergence for estimating difference between probability distributions. Another example is Jensen-Shannon divergence (JSD), which is defined as\n",
    "$$\n",
    "\\text{JS}(P, Q) = \\frac{1}{2}\\text{KL}(P, M) + \\frac{1}{2}\\text{KL}(Q, M),\n",
    "$$\n",
    "where $M = \\frac{1}{2}(P + Q)$. Despite being symmetric, even for gaussians it is analytically intractable, hence numerical solutions are used. One option is to estimate corresponding KL terms by sampling from P and Q. $\\textbf{Your task is to compute JSD between two gaussians by employing sampling}$. You can use scipy.stats package here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58b4392-baaf-48ed-ace0-a300f945c231",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5865013452b6e905a0ae27ec9b15de33",
     "grade": false,
     "grade_id": "cell-891d74046522d31e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def JSD(mean1, sigma1, mean2, sigma2, N):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        mean1: mean of normal distribution p\n",
    "        sigma1: std of the normal distribution p \n",
    "        mean2: mean of normal distribution q\n",
    "        sigma2: std of the normal distribution q\n",
    "        N: number of simulations\n",
    "        \n",
    "    Returns:\n",
    "        jsd\n",
    "    \"\"\"\n",
    "    samples1 = np.random.randn(N) * sigma1 + mean1\n",
    "    samples2 = np.random.randn(N) * sigma2 + mean2\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return jsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e10d027-eeda-4177-9f3b-9c75cca60655",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9816ddffa3417f6407589d8eac1e4cf2",
     "grade": true,
     "grade_id": "cell-23f43029084b0c82",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "eps = 0.028\n",
    "jsd = 0.545333\n",
    "\n",
    "diff = abs(JSD(3,6, 10,0.5, 10000) - jsd)\n",
    "assert diff < eps, \\\n",
    "print(f\"jsd do not allign, the absolute value difference is {diff}\")\n",
    "\n",
    "print(\"Success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a9d5d5-5646-4320-8d31-0cbc8c1d948e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a580f76750de2ba837bf6f5c83647688",
     "grade": true,
     "grade_id": "cell-8cbe65fb6b67b6c7",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Here is a hidden test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0d9fbf-9be9-4aec-9492-949be5c8184b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "611a3213b3bf419e592b3225f8cb1774",
     "grade": false,
     "grade_id": "cell-3f36d7d170989696",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Exercise 4. Monte-Carlo for area estimation\n",
    "\n",
    "Monte-Carlo approaches are used for various tasks. One of them is area estimation, when the shape is complex. For this task, your are given figure that satisfies the following inequalities:\n",
    "\n",
    "$$\n",
    "\\frac{x^2}{2} + \\frac{y^2}{3} < 1 \\\\\n",
    "x + y > -1 \\\\\n",
    "x - y < 0\n",
    "$$\n",
    "\n",
    "$\\textbf{Your task is to estimate its area}$. Sample $x$ and $y$ from uniform distributions over $[-2, 2]$ and compute the Monte-Carlo estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f64984-2613-4a29-a3c0-d96ece46a597",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f2b707655fdb6a8c7e5be8687ccfe091",
     "grade": false,
     "grade_id": "cell-7c70180dd48ac8ce",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def area(N):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        N: number of samples from uniform distributions\n",
    "    Returns:\n",
    "        x: sampled x values\n",
    "        y: sampled y values\n",
    "        mask: boolean array of size N where mask[i] is True if corresponding x_i and y_i satisfy inequalities\n",
    "        area: estimated area\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return x,y,mask,area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27188253-6bec-48c9-a662-130d40f8c2ab",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e5a5093c02440d627d6322551f851e9a",
     "grade": true,
     "grade_id": "cell-94df4b76316a62eb",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Here is a hidden test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effedf24-5474-42ac-bda1-920116316e9a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f20e64d1ce922094b819d5b7ea5179b4",
     "grade": false,
     "grade_id": "cell-49c8d5afb74ab4f0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x,y,mask,area_ = area(1000)\n",
    "\n",
    "plt.scatter(x,y, label=\"all points\")\n",
    "plt.scatter(x[mask], y[mask], label=\"points in the region\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
