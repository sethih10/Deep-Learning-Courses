{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fa4b88d-b0e4-4a21-9e74-0287fa3e7395",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b1e586ad026883f5b8e78750fde3772d",
     "grade": false,
     "grade_id": "cell-f00dea7bcdc4a14b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "**This exercise requires computing power. The assignment is solvable using a JupyterHub CPU. If implemented efficiently, the training duration will be approximately 15 minutes. However, we recommend you to use a GPU if you have access to one.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "8a197a83-b77f-45e7-b9a0-a6f7139814c3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "559659f2d3856488f8b1823a5d2873f4",
     "grade": false,
     "grade_id": "cell-5a789e23d2939295",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import heapq\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import sentencepiece as spm\n",
    "from tqdm import tqdm\n",
    "\n",
    "def set_seeds(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  # For CUDA\n",
    "    torch.cuda.manual_seed_all(seed)  # If you are using multi-GPU.\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seeds(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "dc4595a7-cb3b-480f-a25f-ef6677321fca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "skip_training = True  # Set this flag to True before validation and submission\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d8b3aa40-7516-4682-9cb9-2a22be2e132c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c53b8da72965174f8fe4d4b9ad60460f",
     "grade": true,
     "grade_id": "cell-410adc87eea15265",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# During evaluation, this cell sets skip_training to True\n",
    "# skip_training = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9f8b0c-ae75-41f7-babc-3fb21daecded",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ee917ae7aa300b91497e7516baf62ad8",
     "grade": false,
     "grade_id": "cell-46d194e46aeec3c4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Exercise 4 - Autoregressive Language Modeling (4p)\n",
    "\n",
    "In the previous exercise, we explored n-gram models, which, despite their simplicity, quickly reach their limits in modeling long-range language dependencies. In this final assignment, your goal is to implement and train a Transformer-based autoregressive language model to predict the next token given a sequence of previous tokens. Your model should achieve low perplexity on a held-out test set from the Shakespeare dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bbd943-d6bc-4c44-b998-6ee24c777767",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5f155b6b379f4147b5f7046ce0a43dfb",
     "grade": false,
     "grade_id": "cell-db4696f367b2d45e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Tokenization and Data Preparation\n",
    "\n",
    "Use the previously trained SentencePiece tokenizer to tokenize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4b014df8-209f-48c5-87d4-4961cd7932c2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "590ba54576ba16f701cd1760c72632b0",
     "grade": false,
     "grade_id": "cell-7ab99e895840584f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input.txt already exists. Skipping download.\n",
      "\n",
      "Tokenizer loaded successfully from shakespeare_tokenizer_512.model\n",
      "\n",
      "Total tokens in dataset: 476719\n"
     ]
    }
   ],
   "source": [
    "# Check if the file already exists\n",
    "if not os.path.exists(\"input.txt\"):\n",
    "    !wget https://raw.githubusercontent.com/karpathy/char-rnn/refs/heads/master/data/tinyshakespeare/input.txt\n",
    "else:\n",
    "    print(\"input.txt already exists. Skipping download.\\n\")\n",
    "\n",
    "# Define parameters\n",
    "model_prefix = \"shakespeare_tokenizer\"\n",
    "vocab_size = 512\n",
    "filename = \"input.txt\"\n",
    "\n",
    "if not os.path.exists(f\"{model_prefix}_{vocab_size}.model\"):\n",
    "    # Train SentencePiece tokenizer\n",
    "    spm.SentencePieceTrainer.train(\n",
    "        input=filename,\n",
    "        model_prefix=f\"{model_prefix}_{vocab_size}\",\n",
    "        vocab_size=vocab_size,\n",
    "        model_type=\"bpe\",\n",
    "        max_sentence_length=4096,\n",
    "        minloglevel=2,\n",
    "        hard_vocab_limit=False,\n",
    "        normalization_rule_name=\"nmt_nfkc\",\n",
    "        remove_extra_whitespaces=True,\n",
    "        character_coverage=1.0,\n",
    "        num_threads=1, \n",
    "    )\n",
    "    print(f\"Tokenizer creation complete. Model files generated: {model_prefix}_{vocab_size}.model, {model_prefix}_{vocab_size}.vocab\")\n",
    "else:\n",
    "    sp_model = spm.SentencePieceProcessor()\n",
    "    sp_model.load(f\"shakespeare_tokenizer_{vocab_size}.model\")\n",
    "    print(f\"Tokenizer loaded successfully from shakespeare_tokenizer_{vocab_size}.model\\n\")\n",
    "\n",
    "def tokenize_text(text, sp_model):\n",
    "    return sp_model.encode(text, out_type=int)\n",
    "\n",
    "def load_dataset(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "text = load_dataset(\"input.txt\")\n",
    "tokens = tokenize_text(text, sp_model)\n",
    "print(f\"Total tokens in dataset: {len(tokens)}\")\n",
    "\n",
    "block_size = 128  # context window for each input sequence\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08348a7-7814-49bc-a931-cfa727830a1d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "33001caed62dd9243d5cfb2a687b8fc9",
     "grade": false,
     "grade_id": "cell-285ca905296e23f5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Creating the Dataset\n",
    "\n",
    "We divide the dataset into fixed-length sequences (chunks) to train the model. To introduce variability and improve model robustness when performing multi-epoch training, we add a random offset each epoch to shift the dataset slightly to keep the mini-batches different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "9952b6ae-9b26-4ce4-a9c5-f8c3309f8cd1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "990df054bfec0abf0e83345a83aa2a4f",
     "grade": false,
     "grade_id": "cell-0b6b08fe28c59d8d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ShakespeareDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Creates fixed-length sequences from a continuous token stream for autoregressive training.\n",
    "    \"\"\"\n",
    "    def __init__(self, tokens, block_size):\n",
    "        self.tokens = tokens\n",
    "        self.block_size = block_size\n",
    "        self.offset = 0\n",
    "        self._update_chunks()\n",
    "\n",
    "    def _update_chunks(self):\n",
    "        # Trim the tokens using the current offset\n",
    "        adjusted_tokens = self.tokens[self.offset:]\n",
    "        total_len = len(adjusted_tokens)\n",
    "        self.num_chunks = (total_len - 1) // self.block_size\n",
    "        self.chunks_start = [i * self.block_size + self.offset for i in range(self.num_chunks)]\n",
    "\n",
    "    def set_epoch(self, epoch=None):\n",
    "        # Set a new random offset at the beginning of each epoch\n",
    "        self.offset = random.randint(0, self.block_size - 1)\n",
    "        self._update_chunks()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_chunks\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start = self.chunks_start[idx]\n",
    "        end = start + self.block_size + 1\n",
    "        chunk = self.tokens[start:end]\n",
    "\n",
    "        x = torch.tensor(chunk[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(chunk[1:], dtype=torch.long)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9443898-34a6-4961-b0e7-80799b05b97a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a53c535360c119a9a7008627c342afa3",
     "grade": false,
     "grade_id": "cell-db0600dffd18ff43",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "We split the dataset into training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "064c4dc3-eaba-43d3-90f2-4204fe7ed96b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3b193b3581c17d4577610817fcc5e7c2",
     "grade": false,
     "grade_id": "cell-9f1092ef980e0f2f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "split_idx = int(0.98 * len(tokens))\n",
    "train_data = tokens[:split_idx]\n",
    "test_data = tokens[split_idx:]\n",
    "\n",
    "train_dataset = ShakespeareDataset(train_data, block_size)\n",
    "test_dataset = ShakespeareDataset(test_data, block_size)\n",
    "\n",
    "# train_loader created later\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98a6be7-dc7e-4b1a-b857-7b4a0739d30b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5756be6166debf2088f718534bc96b6e",
     "grade": false,
     "grade_id": "cell-d6d28360fbda25c6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Transformer Architecture\n",
    "\n",
    "You will implement a decoder-only transformer model suitable for autoregressive language modeling. We give you the original sinusoidal positional encoding below, which you may use but are not obligated to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "faff08e4-71ad-4df8-96ef-a927ac02c5ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=1000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbb6249-e4ac-464f-a6a1-75e0aec0df56",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "97a3901546a5af77ba612fdb08ca4d54",
     "grade": false,
     "grade_id": "cell-f70981138b7cb0c5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### TransformerBlock\n",
    "\n",
    "Your task is to implement a TransformerBlock. A transformer block takes input vectors with shape (batch_size, seq_len, embed_dim) and processes them in two main stages:\n",
    "\n",
    "Causal Self-Attention:\n",
    "Apply layer normalization, followed by multi-head self-attention with a causal mask to ensure each token attends only to itself and previous tokens. The attention output is added back to the input (residual connection).\n",
    "\n",
    "Feedforward Network:\n",
    "Again apply layer normalization, followed by a feedforward network (two linear layers with a ReLU activation in between). The output is added back via a second residual connection.\n",
    "\n",
    "We provide this typical transformer description as guidance. The exact implementation details are flexible. We only test correctness of input/output shapes and causal masking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "562dc47f-6c8c-4e9a-881f-142b77d0be51",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1f0c95f30f81b1feee96e3fdd18efbd5",
     "grade": false,
     "grade_id": "cell-8294964706f5cea6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A single decoder-only transformer block. Can, for instance, consist of:\n",
    "    - LayerNorm followed by multi-head self-attention with residual connection.\n",
    "    - LayerNorm followed by a feedforward network with residual connection.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    embed_dim : int\n",
    "        Dimension of the input embeddings and the hidden representations.\n",
    "    num_heads : int\n",
    "        Number of attention heads in the multi-head attention layer.\n",
    "    ff_hidden_dim : int\n",
    "        Hidden layer size for the feedforward network.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, ff_hidden_dim):\n",
    "        super().__init__()\n",
    "        # YOUR CODE HERE\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.mha = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads,batch_first=True)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 4*embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*embed_dim, embed_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        #raise NotImplementedError()\n",
    "        \n",
    "    def forward(self, x, attn_mask):\n",
    "        \"\"\"\n",
    "        Forward pass through the transformer block.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : Tensor\n",
    "            Input tensor of shape (batch_size, seq_len, embed_dim).\n",
    "        attn_mask : Tensor\n",
    "            Boolean tensor of shape (seq_len, seq_len) used to mask future tokens.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        Tensor\n",
    "            Output tensor of the same shape as input (batch_size, seq_len, embed_dim).\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        batch_size, seq_len, embed_dim = x.shape\n",
    "        residual = x\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        attn_out, attn_weights = self.mha(x,x,x,attn_mask = attn_mask)\n",
    "        x = residual + attn_out\n",
    "        \n",
    "        residual = x\n",
    "        x = self.norm(x)\n",
    "        x = self.ffn(x)\n",
    "        x = x+residual\n",
    "        \n",
    "        \n",
    "        return x\n",
    "        \n",
    "        #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeeaf9d3-0c87-4bc7-bea9-4d3984576205",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f7ed93abf204a165808b181f28e460dd",
     "grade": false,
     "grade_id": "cell-bd5e49114dbdf852",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### TransformerDecoderOnlyLM\n",
    "\n",
    "Next, implement the full language model using your TransformerBlock. The model is a decoder-only transformer for autoregressive language modeling, mapping token indices (batch_size, seq_len) to logits (batch_size, seq_len, vocab_size).\n",
    "\n",
    "Typically, this model performs the following steps:\n",
    "\n",
    "Embedding + Positional Encoding: Embed tokens into vectors and add positional information.\n",
    "\n",
    "Stacked Transformer Blocks: Pass embeddings through several transformer blocks, each using causal self-attention.\n",
    "\n",
    "Final Projection: Apply a final layer normalization and project the output vectors to logits over the vocabulary.\n",
    "\n",
    "The exact architecture (number of layers, dimensions) is not tested. We only require correct handling of tensor shapes, causal masking, and final output dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "013e8eeb-353d-4093-93f0-974f183c6dff",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5ea833fdc2cabff918f673607c144376",
     "grade": false,
     "grade_id": "cell-b3dfc2d11fec7a9a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TransformerDecoderOnlyLM(nn.Module):\n",
    "    \"\"\"\n",
    "    A full decoder-only Transformer language model.\n",
    "\n",
    "    This model maps a sequence of token indices to a sequence of output logits \n",
    "    over the vocabulary using causal self-attention, suitable for autoregressive \n",
    "    language modeling (i.e., predicting the next token given previous ones).\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    vocab_size : int\n",
    "        Size of the vocabulary (number of unique tokens).\n",
    "    embed_dim : int\n",
    "        Dimension of the embedding vectors and model hidden states.\n",
    "    num_heads : int\n",
    "        Number of attention heads in each Transformer block.\n",
    "    num_layers : int\n",
    "        Number of stacked Transformer blocks.\n",
    "    ff_hidden_dim : int\n",
    "        Hidden layer size inside the feedforward network of each block.\n",
    "    max_len : int\n",
    "        Maximum sequence length supported by the positional encoding.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, ff_hidden_dim, max_len=1024):\n",
    "        super().__init__()\n",
    "        # YOUR CODE HERE\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_encodings = PositionalEncoding(d_model = embed_dim, max_len=max_len)\n",
    "        self.trf = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.trf.append(TransformerBlock(embed_dim = embed_dim, num_heads = num_heads, ff_hidden_dim = ff_hidden_dim))\n",
    "            \n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.output_projection = nn.Linear(embed_dim, vocab_size)\n",
    "        \n",
    "        \n",
    "        #raise NotImplementedError()\n",
    "\n",
    "    def generate_causal_mask(self, seq_len, device):\n",
    "        \"\"\"\n",
    "        Creates a causal mask to prevent attention to future tokens.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        attn_mask : Tensor\n",
    "            Boolean tensor of shape (seq_len, seq_len) where True values are masked.\n",
    "        \"\"\"\n",
    "        return torch.triu(torch.ones(seq_len, seq_len, device=device), diagonal=1).bool()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the Transformer language model.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : Tensor\n",
    "            Input tensor of token indices with shape (batch_size, seq_len).\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        logits : Tensor\n",
    "            Output tensor of shape (batch_size, seq_len, vocab_size), containing\n",
    "            unnormalized scores for each token in the vocabulary.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        batch_size, seq_len = x.shape\n",
    "        x_emb = self.embeddings(x)\n",
    "        x = self.pos_encodings(x_emb)\n",
    "        \n",
    "        attn_mask = self.generate_causal_mask(seq_len = seq_len, device = device)\n",
    "        for block in self.trf:\n",
    "            x = block(x, attn_mask=attn_mask)\n",
    "        x = self.norm(x)\n",
    "        x = self.output_projection(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "        #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "7decf777-ec64-4337-b40d-82fa3753ff7d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1d04fe0cb0786ec25990f45c9514fd1a",
     "grade": true,
     "grade_id": "cell-58057663523e0c69",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test: TransformerBlock output shape...\n",
      "Running test: TransformerDecoderOnlyLM output shape...\n",
      "Running test: Causal mask effectiveness...\n",
      "Tests pass - success!\n"
     ]
    }
   ],
   "source": [
    "# Test Config\n",
    "embed_dim = 16\n",
    "num_heads = 4\n",
    "ff_hidden_dim = 64\n",
    "seq_len = 10\n",
    "batch_size = 2\n",
    "num_layers = 2\n",
    "\n",
    "block = TransformerBlock(embed_dim, num_heads, ff_hidden_dim)\n",
    "model = TransformerDecoderOnlyLM(vocab_size, embed_dim, num_heads, num_layers, ff_hidden_dim, max_len=seq_len)\n",
    "\n",
    "print(\"Running test: TransformerBlock output shape...\")\n",
    "x = torch.randn(batch_size, seq_len, embed_dim)\n",
    "attn_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
    "out = block(x, attn_mask)\n",
    "assert out.shape == (batch_size, seq_len, embed_dim), \"TransformerBlock output shape mismatch\"\n",
    "\n",
    "print(\"Running test: TransformerDecoderOnlyLM output shape...\")\n",
    "tokens = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "logits = model(tokens)\n",
    "assert logits.shape == (batch_size, seq_len, vocab_size), \"TransformerDecoderOnlyLM output shape mismatch\"\n",
    "\n",
    "print(\"Running test: Causal mask effectiveness...\")\n",
    "x1 = torch.randint(0, vocab_size, (1, seq_len))\n",
    "x2 = x1.clone()\n",
    "change_start = seq_len // 2\n",
    "x2[0, change_start:] = (x2[0, change_start:] + 1) % vocab_size\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits1 = model(x1)\n",
    "    logits2 = model(x2)\n",
    "\n",
    "tolerance = 1e-4\n",
    "diff = (logits1[:, :change_start] - logits2[:, :change_start]).abs().max().item()\n",
    "assert diff < tolerance, f\"Causal mask failed — output changed by {diff:.4f}\"\n",
    "\n",
    "print(\"Tests pass - success!\")\n",
    "\n",
    "# Reset after tests\n",
    "block_size = 128 \n",
    "batch_size = 32\n",
    "vocab_size = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf77a523-6cc6-4b00-b475-768bd895909e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7b4b9a845e5266561d4d5202fe6b9b95",
     "grade": false,
     "grade_id": "cell-717fe885ce3283d6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Training Instructions\n",
    "\n",
    "You will train the TransformerDecoderOnlyLM model to achieve a test loss below 3.6, corresponding to a good language model on this dataset.\n",
    "\n",
    "Hints for training:\n",
    "- Gradient clipping significantly stabilizes training (nn.utils.clip_grad_norm_).\n",
    "- AdamW optimizer usually outperforms plain Adam.\n",
    "- For better results, you can consider using learning rate warm-up and cosine decay schedules.\n",
    "\n",
    "In practice, training a moderately-sized Transformer (e.g., embed_dim=512) for around 3 epochs should take approximately 15 minutes on JupyterHub CPU and under one minute on a Colab GPU and lead to sufficient performance for passing the tests. Training for longer can yield significantly improved results. Modify the cell below to define the hyperparameters, the desired optimizer and training duration, and any other optional components. Do not modify the model creating function, our tests assume this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "11d8b462-26f0-4c84-80f2-d6490da67b66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embed_dim = 512\n",
    "ff_hidden_dim = 512*4\n",
    "num_heads = 8\n",
    "num_layers = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "20a2829c-78ae-44f3-af80-5a869a2c7b6c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b7ed12ba30c46c38924cd7c24441785f",
     "grade": true,
     "grade_id": "cell-de531eff78133eea",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This cell sets some stuff for TA use\n",
    "\n",
    "model = TransformerDecoderOnlyLM(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=embed_dim,\n",
    "    num_heads=num_heads,\n",
    "    num_layers=num_layers,\n",
    "    ff_hidden_dim=ff_hidden_dim,\n",
    "    max_len=block_size,\n",
    ").to(device)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "911c8a35-81f0-4ce8-a783-30e473c27f61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import LambdaLR, CosineAnnealingLR\n",
    "import math\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-2)\n",
    "epochs = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "54e98152-c0e2-4e32-ae6b-8294b1bf4ee5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3e12163a689ee9ff1f4b8689ee34a28c",
     "grade": true,
     "grade_id": "cell-07cda67a882024db",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This cell sets some stuff for TA use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "b7bd541c-8144-46d5-9253-b4b949553db5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learnable parameters: 16,282,624\n"
     ]
    }
   ],
   "source": [
    "def count_learnable_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Learnable parameters: {count_learnable_params(model):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "a8151ab2-e070-44e6-8009-4f9fb0c1cd7a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5415e4d5bc45684e1fe13cbe8573c9a8",
     "grade": false,
     "grade_id": "cell-4c0d3e1e3942411d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "    for epoch in range(epochs):\n",
    "        train_dataset.set_epoch(epoch)\n",
    "        # re-created before each epoch because we shift the dataset epoch to get an offset and some variance in the batches\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)  \n",
    "        \n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for x, y in tqdm(train_loader, desc=f\"Epoch {epoch+1} [Train]\"):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            # YOUR CODE HERE\n",
    "            optimizer.zero_grad()\n",
    "            out = model(x)\n",
    "            out = out.view(-1, vocab_size)\n",
    "            y = y.view(-1)\n",
    "            loss = criterion(out,y)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            #raise NotImplementedError()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch + 1}, Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # --- Evaluation ---\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in test_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                output = model(x)\n",
    "                loss = criterion(output.view(-1, vocab_size), y.view(-1))\n",
    "                test_loss += loss.item()\n",
    "\n",
    "        avg_test_loss = test_loss / len(test_loader)\n",
    "        print(f\"Epoch {epoch + 1}, Test Loss: {avg_test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a04c3c02-9ff2-418a-80c7-bdbe7d79af47",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e2e90872fac65d97b364b5515839389b",
     "grade": false,
     "grade_id": "cell-f15a944e05fad104",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_model(model, filename):\n",
    "    try:\n",
    "        do_save = input('Do you want to save the model (type yes to confirm)? ').lower()\n",
    "        if do_save == 'yes':\n",
    "            torch.save(model.state_dict(), filename)\n",
    "            print('Model saved to %s.' % (filename))\n",
    "        else:\n",
    "            print('Model not saved.')\n",
    "    except:\n",
    "        raise Exception('The notebook should be run or validated with skip_training=True.')\n",
    "\n",
    "\n",
    "def load_model(model, filename, device):\n",
    "    model.load_state_dict(torch.load(filename, map_location=lambda storage, loc: storage))\n",
    "    print('Model loaded from %s.' % filename)\n",
    "    model.to(device)\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "201f4871-3964-461c-ba3f-45a7ffaeb2b7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "633880942c13658c15eabb0511b56f48",
     "grade": false,
     "grade_id": "cell-c3b0f5fd40b60f03",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from shakespeare_decoder.pth.\n"
     ]
    }
   ],
   "source": [
    "# Save the model to disk (the pth-files will be submitted automatically together with your notebook)\n",
    "if not skip_training:\n",
    "    save_model(model, 'shakespeare_decoder.pth')\n",
    "else:\n",
    "    model = TransformerDecoderOnlyLM(\n",
    "        vocab_size=vocab_size,\n",
    "        embed_dim=embed_dim,\n",
    "        num_heads=num_heads,\n",
    "        num_layers=num_layers,\n",
    "        ff_hidden_dim=ff_hidden_dim,\n",
    "        max_len=block_size,\n",
    "    )\n",
    "    load_model(model, 'shakespeare_decoder.pth', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a8585b-597b-4fd8-ae42-251c23d869d2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ef723f3619d2d8809b341d69912d2130",
     "grade": false,
     "grade_id": "cell-124910d4c2368dd4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Evaluation and Testing\n",
    "\n",
    "We evaluate using average test perplexity (corresponds to test loss), including a separate calculation for the loss specifically at the final token of various sequence lengths. This ensures that your causal attention masking is correctly implemented.\n",
    "\n",
    "Passing criteria:\n",
    "- Causal mask correctness: Final-token average loss must be below 4.4. (1 point)\n",
    "- Acceptable model: Average test loss must be below 4.0. (1 additional point)\n",
    "- Good model (target): Average test loss must be below 3.6. (1 additional point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "3c9aa538-f40e-4fbf-8fb5-7ea91a9f054c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5e109ae62dca721339cc0767ed00d9e2",
     "grade": false,
     "grade_id": "cell-5316e8969a5a83ad",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_perplexity(model, test_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # ---- Compute Perplexity ----\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            output = model(x)\n",
    "            loss = criterion(output.view(-1, model.output_projection.out_features), y.view(-1))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    print(f\"Avg loss: {avg_loss:.2f}\")\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    print(f\"Perplexity on test set: {perplexity:.2f}\")\n",
    "    return avg_loss\n",
    "\n",
    "def evaluate_perplexity_at_token(model, test_loader, token_idx, device, verbose=False):\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    losses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            # Cut all sequences at token_idx\n",
    "            input_cut = x[:, :token_idx]  # Shape: [batch_size, token_idx]\n",
    "            target_tokens = y[:, token_idx-1]  # Shape: [batch_size]\n",
    "\n",
    "            output = model(input_cut)  # Shape: [batch_size, token_idx, vocab_size]\n",
    "            logits = output[:, -1, :]  # Shape: [batch_size, vocab_size]\n",
    "\n",
    "            loss = criterion(logits, target_tokens)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    avg_loss = sum(losses) / len(losses)\n",
    "    if verbose:\n",
    "        print(f\"Avg loss at token {token_idx}: {avg_loss:.2f}\")\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    if verbose:\n",
    "        print(f\"Per-token perplexity at token {token_idx}: {perplexity:.2f}\")\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "94ac2a0c-41b7-47dc-844b-e0e1292b89a5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "464d6ff8b794aac1ebf25e891e1cd2f3",
     "grade": false,
     "grade_id": "cell-6fc71be0513affc1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 3.51\n",
      "Perplexity on test set: 33.43\n",
      "Average loss at the final token for different length inputs: 3.39\n"
     ]
    }
   ],
   "source": [
    "test_loss = evaluate_perplexity(model, test_loader, device)\n",
    "\n",
    "losses_at_final_token = []\n",
    "for tk_index in [32, 64, 96, 128]:\n",
    "    losses_at_final_token.append(evaluate_perplexity_at_token(model, test_loader, token_idx=tk_index, device=device, verbose=False))\n",
    "losses_at_final_token = sum(losses_at_final_token)/len(losses_at_final_token)\n",
    "print(f\"Average loss at the final token for different length inputs: {losses_at_final_token:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "da5d3409-2759-45f4-af70-92843e8df9c3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c87e79d64ea2baa482996a15952573ad",
     "grade": true,
     "grade_id": "cell-dc238e333050c7ce",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert losses_at_final_token < 4.4, \"If the test loss during training was low but this test fails, most likely the causal mask has not been applied correctly\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "391a71b9-4e06-450c-8e80-030a2065e6f4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "72e833df9956bcf4b11592d7c4b2ea58",
     "grade": true,
     "grade_id": "cell-3af8c9a0831fb114",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert losses_at_final_token < 4.4, \"If the test loss during training was low but this test fails, most likely the causal mask has not been applied correctly\"\n",
    "assert test_loss < 4, \"The model does not perform very well\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "2beacdb5-7ef6-49d0-a443-08120a2be081",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8f5edf21a89b98637160da88e0ab29a3",
     "grade": true,
     "grade_id": "cell-0e9080966e8c759c",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests pass - success!\n"
     ]
    }
   ],
   "source": [
    "assert losses_at_final_token < 4.4, \"If the test loss during training was low but this test fails, most likely the causal mask has not been applied correctly\"\n",
    "assert test_loss < 3.6, \"The model performance could still be improved\"\n",
    "print(\"Tests pass - success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba21ff1-5a1b-484b-92e5-aa3dfc107938",
   "metadata": {},
   "source": [
    "## Text Generation and Sampling\n",
    "\n",
    "Finally, we sample from the trained model. You can experiment with various sampling strategies:\n",
    "- Temperature controls the randomness: lower values (e.g., 0.7 or lower) yield more deterministic and coherent text; values close to 1.0 produce more diverse but potentially noisy outputs.\n",
    "- Top-k sampling restricts the choices to the k most likely tokens at each step, improving coherence.\n",
    "\n",
    "You should observe that combining moderately low temperatures (around 0.7–0.8) with top-k sampling typically yields high-quality, coherent text samples with perplexity well below 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "04330130-ab4c-44f2-af14-00185b0999da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sample_next_token(logits, temperature=1.0, top_k=0):\n",
    "    logits = logits / temperature\n",
    "\n",
    "    if top_k > 0:\n",
    "        # Top-k sampling: keep only top k tokens with highest logits\n",
    "        top_k = min(top_k, logits.size(-1))  # safety\n",
    "        values, indices = torch.topk(logits, top_k)\n",
    "        logits_filtered = torch.full_like(logits, float('-inf'))\n",
    "        logits_filtered.scatter_(1, indices, values)\n",
    "        logits = logits_filtered\n",
    "\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    return torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "\n",
    "def sample(model, test_loader, sp_model, device, num_samples=5, max_new_tokens=100, temperature=1.0, top_k=0):\n",
    "    # ---- Collect num_samples prompts ----\n",
    "    print(f\"=== Sampling {num_samples} test examples ===\\n\")\n",
    "    collected_prompts = []\n",
    "    criterion = nn.CrossEntropyLoss(reduction='none')  # Per-token loss\n",
    "\n",
    "    for x, _ in test_loader:\n",
    "        for sample in x:\n",
    "            if len(collected_prompts) < num_samples:\n",
    "                collected_prompts.append(sample[None].to(device))\n",
    "            else:\n",
    "                break\n",
    "        if len(collected_prompts) >= num_samples:\n",
    "            break\n",
    "\n",
    "    # ---- Batched Sampling ----\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        total_tokens = 0\n",
    "        for i in range(0, len(collected_prompts), batch_size):\n",
    "            batch_prompts = collected_prompts[i:i + batch_size]\n",
    "            generated = torch.cat(batch_prompts, dim=0)\n",
    "\n",
    "            for _ in range(max_new_tokens):\n",
    "                input_chunk = generated[:, -block_size:]\n",
    "                output = model(input_chunk)\n",
    "                next_token_logits = output[:, -1, :]  # shape: [B, vocab_size]\n",
    "                next_tokens = sample_next_token(next_token_logits, temperature=temperature, top_k=top_k)\n",
    "                generated = torch.cat((generated, next_tokens), dim=1)\n",
    "                \n",
    "                loss = criterion(next_token_logits, next_tokens.squeeze(-1))  # [batch_size]\n",
    "                total_loss += loss.sum().item()\n",
    "                total_tokens += next_tokens.size(0)\n",
    "\n",
    "            for j, (prompt_tensor, full_output) in enumerate(zip(batch_prompts, generated)):\n",
    "                prompt_tokens = prompt_tensor.tolist()\n",
    "                generated_tokens = full_output.tolist()\n",
    "\n",
    "                prompt_text = sp_model.decode(prompt_tokens)[0]\n",
    "                generated_text = sp_model.decode(generated_tokens)\n",
    "\n",
    "                print(f\"[Sample {i + j + 1}]\")\n",
    "                print(\"Prompt:\")\n",
    "                print(prompt_text)\n",
    "                print(\"\\nGenerated continuation:\")\n",
    "                print(generated_text[len(prompt_text):])\n",
    "                print(\"=\" * 60)\n",
    "                \n",
    "        avg_loss = total_loss / total_tokens\n",
    "        perplexity = math.exp(avg_loss)\n",
    "        print(f\"\\n=== Perplexity of generated text: {perplexity:.2f} ===\")\n",
    "        return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "63fa2047-469b-48ec-86cf-54aedc4d6223",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "    print(\"Sampling with default settings: no top_k, temperature=1.0\")\n",
    "    sample(model, test_loader, sp_model, device, num_samples=3, max_new_tokens=100, temperature=1)\n",
    "    print(\"\\n\\nClose-to-argmax sampling: temperature = 0.01\")\n",
    "    sample(model, test_loader, sp_model, device, num_samples=3, max_new_tokens=100, temperature=0.01)\n",
    "    print(\"\\n\\nSampling with top_k: temperature = 0.75, top_k = 30\")\n",
    "    sample(model, test_loader, sp_model, device, num_samples=3, max_new_tokens=100, temperature=0.75, top_k=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ad4b1a-32f2-4f7e-b3cb-21d4829592c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
