{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0a605de66c23f13422b14d663bac548e",
     "grade": false,
     "grade_id": "cell-b247a7c1a13a2809",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Deadline:</b> March 3, 2025 (Wednesday) 23:00\n",
    "</div>\n",
    "\n",
    "# Exercise 1. Few-shot learning with Prototypical Networks\n",
    "\n",
    "The goal of the exercise is to get familiar with methods that can solve few-shot classification tasks. In this noteboook, we will implement Prototypical Networks. We recommend you to read the original paper by [Snell et al, (2017)](https://arxiv.org/pdf/1703.05175.pdf) before doing this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "92cb6fe65e5885e47aa273f08f29844d",
     "grade": false,
     "grade_id": "cell-876af25c4c417ca0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We will use interactive figures in this notebook\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "skip_training = False  # Set this flag to True before validation and submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "183440b39c7428817d72a6af88fb618c",
     "grade": true,
     "grade_id": "cell-f357feeef0e44248",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# During evaluation, this cell sets skip_training to True\n",
    "# skip_training = True\n",
    "\n",
    "import tools, warnings\n",
    "warnings.showwarning = tools.customwarn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c01a459c596256e51959916bc5cb86dd",
     "grade": false,
     "grade_id": "cell-68d82bb6c9d904b2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import tools\n",
    "import tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data directory is /coursedata\n"
     ]
    }
   ],
   "source": [
    "# When running on your own computer, you can specify the data directory by:\n",
    "# data_dir = tools.select_data_dir('/your/local/data/directory')\n",
    "data_dir = tools.select_data_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select the device for training (use GPU if you have one)\n",
    "#device = torch.device('cuda:0')\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b10c40a5a2c0dd319ea8797e07aabfc5",
     "grade": false,
     "grade_id": "cell-dd5ea972af42eeac",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if skip_training:\n",
    "    # The models are always evaluated on CPU\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fbe2ce55acaab3de8e2c5922d90cd032",
     "grade": false,
     "grade_id": "cell-6dde5ca26c51bf91",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Omniglot data\n",
    "\n",
    "We will use Omniglot data for training. Omniglot is a collection of 19280 images of 964 characters from 30 alphabets. There are 20 images for each of the 964 characters in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "371ef3feec7da10179525bd70b54a079",
     "grade": false,
     "grade_id": "cell-1c347de01112e2c1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "dataset = torchvision.datasets.Omniglot(root=data_dir, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8167b324529ee679168c93ebbab751b9",
     "grade": false,
     "grade_id": "cell-749dfef0478c4f4d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 105, 105]) 0\n"
     ]
    }
   ],
   "source": [
    "# Let us plot some samples from the dataset.\n",
    "x, y = dataset[0]  # x is the image, y is the label (character)\n",
    "print(x.shape, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9dec021fa4fb764a4ea01493db623c87",
     "grade": false,
     "grade_id": "cell-8040506306c53bed",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f3c226b7160>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58b7955d7f2f42a4b693f79355843844",
       "version_major": 2,
       "version_minor": 0
      },
      "image/png": "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",
      "text/html": [
       "\n",
       "            <div style=\"display: inline-block;\">\n",
       "                <div class=\"jupyter-widgets widget-label\" style=\"text-align: center;\">\n",
       "                    Figure\n",
       "                </div>\n",
       "                <img src='data:image/png;base64,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' width=300.0/>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(3, 3))\n",
    "ax.matshow(1-x[0], cmap=plt.cm.Greys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7a7f12995ac60af4290a53d696aeb513",
     "grade": false,
     "grade_id": "cell-6b4f5d99575d2215",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Custom data loader for the few-shot learning task\n",
    "\n",
    "The task of few-shot learning is to learn a classification task from a few training examples. In this notebook, we will consider $n$-way $k$-shot classification problems, when each classification problem has $n$ clases with $k$ examples per class in the training dataset.\n",
    "\n",
    "We take the meta-learning approach, in which we learn how to learn new ($N$-way $k$-shot classification) tasks using multiple training examples of tasks. Thus, in the meta-learning approach, a single \"training example\" is one learning (e.g. classification) task which comes from a distribution of tasks that we create using the Omniglot dataset. \n",
    "\n",
    "We perform meta-learning using **episodic training**. In each episode, we process one training task or a mini-batch of tasks. Each tasks contains two datasets:\n",
    "* *support set*, which is used to build a classifier,\n",
    "* *query set*, which is used to test the accuracy of the built classifier.\n",
    "\n",
    "In order to load such training examples in the training loop, we build a custom dataloader on top of the `Omniglot` dataset available in pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1023c761bc86304ea13d5688ed2e5e66",
     "grade": false,
     "grade_id": "cell-6c59dacd2e44f93a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class OmniglotFewShot(Dataset):\n",
    "    \"\"\"Omniglot data set for few-shot learning.\n",
    "\n",
    "    Args:\n",
    "      root (string): Root directory to put the data.\n",
    "      n_support (int): Number of support samples in each training task.\n",
    "      n_query (int): Number of query samples in each training task.\n",
    "      transform (callable): Transforms applied to Omniglot images. We rescale them to 28x28,\n",
    "          convert to tensors and invert so that image backround is encoded as 0 (original Omniglot images have\n",
    "          background encoded as 1).\n",
    "      mix: If True, all examples can be used either as support or query examples. If False, the first\n",
    "          n_support images are always used as support examples and the following n_query images are used\n",
    "          as query examples.\n",
    "      train: If True, use training set. If False, use test set.\n",
    "    \"\"\"\n",
    "    def __init__(self, root, n_support, n_query,\n",
    "                 transform=transforms.Compose([\n",
    "                     transforms.Resize(28),\n",
    "                     transforms.ToTensor(),\n",
    "                     transforms.Lambda(lambda x: 1-x),\n",
    "                 ]),\n",
    "                 mix=False,  # Mix support and query examples\n",
    "                 train=True\n",
    "                ):\n",
    "\n",
    "        assert n_support + n_query <= 20, \"Omniglot contains only 20 images per character.\"\n",
    "        self.n_support = n_support\n",
    "        self.n_query = n_query\n",
    "        self.mix = mix\n",
    "        self.train = train  # training set or test set\n",
    "\n",
    "        self._omniglot = torchvision.datasets.Omniglot(root=root, download=True, transform=transform)\n",
    "\n",
    "        self.character_classes = character_classes = np.array([\n",
    "            character_class for _, character_class in self._omniglot._flat_character_images\n",
    "        ])\n",
    "\n",
    "        n_classes = max(character_classes)\n",
    "        self.indices_for_class = {\n",
    "            i: np.where(character_classes == i)[0].tolist()\n",
    "            for i in range(n_classes)\n",
    "        }\n",
    "\n",
    "        np.random.seed(1)\n",
    "        rp = np.random.permutation(n_classes)\n",
    "        if train:\n",
    "            self.used_classes = rp[:770]\n",
    "        else:\n",
    "            self.used_classes = rp[770:]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "          support_query of shape (n_support+n_query, 1, height, width):\n",
    "                      support_query[:n_support] is the support set\n",
    "                      support_query[n_support:] is the query set\n",
    "        \"\"\"\n",
    "        class_ix = self.used_classes[index]\n",
    "        indices = self.indices_for_class[class_ix]\n",
    "        if self.mix:\n",
    "            indices = np.random.permutation(indices)\n",
    "\n",
    "        indices = indices[:self.n_support+self.n_query]  # First support, then query\n",
    "        support_query = torch.stack([self._omniglot[ix][0] for ix in indices])\n",
    "\n",
    "        return support_query\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.used_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9b96c798274aa9434ce303c3e46ed42e",
     "grade": false,
     "grade_id": "cell-75bbc1cd6cdb5a67",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "One sample from the dataset represents one class which consists of `n_support` support samples and `n_query` query samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "12f7b9602280ff9d82ca9645b880ca92",
     "grade": false,
     "grade_id": "cell-8f9f6c2d4092564a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "torch.Size([4, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "dataset = OmniglotFewShot(root=data_dir, n_support=1, n_query=3, train=True)\n",
    "support_query = dataset[0]\n",
    "print(support_query.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4c36176e85bb5a81f7537811b0efac26",
     "grade": false,
     "grade_id": "cell-6b4a12058eb3d6bf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We can now build data for $n$-way $k$-shot classification tasks using the following data loader. Each mini-batch that this data loader produces is one $n$-way $k$-shot classification task. In principles, we could include more tasks into each mini-batch but we do not do it in this notebok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "29c609bc344f41c3c536de7cb932a3f0",
     "grade": false,
     "grade_id": "cell-64b9da22f0738265",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 4, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "n_way = 5\n",
    "trainloader = DataLoader(dataset=dataset, batch_size=n_way, shuffle=True, pin_memory=True)\n",
    "\n",
    "for support_query in trainloader:\n",
    "    print(support_query.shape)\n",
    "    # support_query is (n-way, n_support+n_query, 1, 28, 28)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e5211d58dfbab8bcc4c7e7a2da76a678",
     "grade": false,
     "grade_id": "cell-cd341221139edaeb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Prototypical networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "abcd173c953dfb449da80801382b2494",
     "grade": false,
     "grade_id": "cell-877fcfccf1b541b3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## The embedding CNN\n",
    "\n",
    "We first build a convolutional neural network that embeds images into a lower-dimensional space.\n",
    "\n",
    "The exact architecture is not important in this exercise but the following architecture worked for us:\n",
    "* Four blocks with the following layers:\n",
    "    * `Conv2d` layer with kernel size 3 and 64 output channels, followed by `BatchNorm2d`, ReLU and 2d max pooling (with kernel 2 and stride 2).\n",
    "* A fully-connected layer with 64 output features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6dfed17ff38fd3944aae2c3e60a4829a",
     "grade": false,
     "grade_id": "CNN",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    # YOUR CODE HERE\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, padding = 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        \n",
    "        self.fcn = nn.Linear(64, 64)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fcn(x)\n",
    "        return x\n",
    "    #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5d0114d24b38aa98aa0cbd63db54bbcb",
     "grade": false,
     "grade_id": "cell-6036d06f90d0f6b4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "def test_CNN_shapes():\n",
    "    net = CNN()\n",
    "\n",
    "    x = torch.randn(2, 1, 28, 28)\n",
    "    y = net(x)\n",
    "    assert y.shape == torch.Size([2, 64]), f\"Wrong y.shape: {y.shape}\"\n",
    "    print('Success')\n",
    "\n",
    "test_CNN_shapes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cfe6eeac5e4f5061cfa5c506c62d2865",
     "grade": false,
     "grade_id": "cell-3588e7f10c2b8e10",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## One episode of training\n",
    "\n",
    "In the cell below, you need to implement building a computational graph for one episode of training of Prototypical Networks.\n",
    "\n",
    "The required steps:\n",
    "* Use the provided network to embed both support and query examples.\n",
    "* Compute one prototype per class using the support set. The prototypes are the mean values of the embeddings of the samples from the same class.\n",
    "* Compute the log-probabilities that the query samples belong to one of the n classes.\n",
    "  The probabilities are softmax of the negative squared Euclidean distance from an embedded sample to a class prototype.\n",
    "* Compute the negative log-likelihood loss using the query samples.\n",
    "\n",
    "Notes:\n",
    "* Try to avoid using for-loops. This will result in faster training and (possibly) better accuracy.\n",
    "* One reason why for-loops can affect training is batch normalization. If you compute the embeddings in a for-loop, the running estimates of the batch norm statistics will be different compared to computing the embeddings with one call of the CNN forward function.\n",
    "* **Your implementation should work for any values of `n_way`, `n_support`, `n_query` and for input images of any resolution.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "710a75bf71fa8b8a2d73446844069f7d",
     "grade": false,
     "grade_id": "episode_pn",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def episode_pn(net, support_query, n_support):\n",
    "    \"\"\"Build a computational graph for one episode of training of prototypical networks.\n",
    "\n",
    "    Args:\n",
    "      net: An embedding network which takes as inputs tensors of shape (batch_size, n_channels, height, width)\n",
    "           and which outputs a tensor of shape (batch_size, n_features).\n",
    "      support_query of shape (n_way, n_support+n_query, 1, height, width):\n",
    "                      support_query[:, :n_support] is the support set\n",
    "                      support_query[:, n_support:] is the query set\n",
    "\n",
    "    Returns:\n",
    "      loss (scalar tensor): The negative log-likelihood loss.\n",
    "      accuracy (float): The classification accuracy on the given example (needed for tracking the progress).\n",
    "      outputs of shape (n_way, n_query, n_way): Logits (log-softmax) of the probabilities of query classes\n",
    "          belonging to one of the n classes. The first dimension corresponds to the true class, the last\n",
    "          dimension corresponds to predicted classes.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    n_way, total, _, _, _ = support_query.shape\n",
    "    # Computing embedding of support and query examples\n",
    "    support_query_emb = net(support_query.reshape(n_way*total, *support_query.shape[2:]))\n",
    "    support_query_emb = support_query_emb.view(n_way, total, -1)\n",
    "\n",
    "    support_emb = support_query_emb[:,:n_support,:]\n",
    "    query_emb = support_query_emb[:, n_support: , :]\n",
    "    \n",
    "    # Computing prototype per class using the support set\n",
    "    prototypes = torch.mean(support_emb, dim = 1)\n",
    "    \n",
    "    # Computing log probabilites that the query samples belong to one of the n classes\n",
    "    # Euclidean distance\n",
    "    \n",
    "    distance = torch.cdist(query_emb.reshape(query_emb.shape[0]*query_emb.shape[1], -1), prototypes, p=2) # shape(n_way*n_support, n_way)\n",
    "    logits = F.log_softmax(-distance, dim = 1)\n",
    "    \n",
    "    \n",
    "    # Compute negative log-likelihood\n",
    "    targets = torch.arange(n_way).repeat_interleave(total - n_support)\n",
    "    # targets = torch.arange(n_way, device=logits.device).unsqueeze(1).expand(-1, total - n_support).reshape(-1)\n",
    "\n",
    "    loss = F.nll_loss(logits, targets).mean()\n",
    "    \n",
    "    outputs = logits.reshape(n_way, total - n_support, -1)\n",
    "    \n",
    "    preds = logits.argmax(dim=-1)\n",
    "    accuracy = (preds == targets).float().mean().item()\n",
    "    \n",
    "    \n",
    "    return loss, accuracy, outputs\n",
    "    \n",
    "    \n",
    "    #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9e5e9720479f4add088bb248ef261fcc",
     "grade": false,
     "grade_id": "cell-dfdb73fde9a8ea66",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "def test_episode_pn_shapes():\n",
    "    n_support = 2\n",
    "    n_query = 4\n",
    "    n_way = 5\n",
    "    support_query = torch.randn(n_way, n_support+n_query, 1, 28, 28)\n",
    "\n",
    "    net = CNN()\n",
    "    loss, accuracy, outputs = episode_pn(net, support_query, n_support)\n",
    "    assert loss.shape == torch.Size([]), \"Bad loss.shape\"\n",
    "    assert 0. <= float(accuracy) <= 1., \"accuracy should be a scalar between 0 and 1.\"\n",
    "    assert outputs.shape == torch.Size([n_way, n_query, n_way]), f\"Bad outputs.shape: {outputs.shape}\"\n",
    "    print('Success')\n",
    "\n",
    "test_episode_pn_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b1089cd4a970dfd530dc0d51de20a477",
     "grade": true,
     "grade_id": "test_episode_pn",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs:\n",
      " tensor([[[-3.3552e-04, -8.0003e+00, -1.6000e+01, -2.4000e+01, -3.2000e+01],\n",
      "         [-8.0007e+00, -6.7080e-04, -8.0007e+00, -1.6001e+01, -2.4001e+01]],\n",
      "\n",
      "        [[-8.0007e+00, -6.7080e-04, -8.0007e+00, -1.6001e+01, -2.4001e+01],\n",
      "         [-1.6001e+01, -8.0007e+00, -6.7092e-04, -8.0007e+00, -1.6001e+01]],\n",
      "\n",
      "        [[-1.6001e+01, -8.0007e+00, -6.7092e-04, -8.0007e+00, -1.6001e+01],\n",
      "         [-2.4001e+01, -1.6001e+01, -8.0007e+00, -6.7080e-04, -8.0007e+00]],\n",
      "\n",
      "        [[-2.4001e+01, -1.6001e+01, -8.0007e+00, -6.7080e-04, -8.0007e+00],\n",
      "         [-3.2000e+01, -2.4000e+01, -1.6000e+01, -8.0003e+00, -3.3552e-04]],\n",
      "\n",
      "        [[-3.2000e+01, -2.4000e+01, -1.6000e+01, -8.0003e+00, -3.3552e-04],\n",
      "         [-3.2000e+01, -2.4000e+01, -1.6000e+01, -8.0003e+00, -3.3552e-04]]])\n",
      "loss: tensor(3.2005)\n",
      "accuracy: 0.6000000238418579\n",
      "expected accuracy: 0.6\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "# This cell tests episode_pn()\n",
    "def test_episode_pn(episode_pn):\n",
    "    n_support = 2\n",
    "    n_query = 2\n",
    "    n_way = 5\n",
    "\n",
    "    sq = torch.tensor([\n",
    "        [0.,   2.,  1.,  2.],\n",
    "        [1.,   3.,  2.,  3.],\n",
    "        [2.,   4.,  3.,  4.],\n",
    "        [3.,   5.,  4.,  5.],\n",
    "        [4.,   6.,  5.,  6.],\n",
    "    ]) # (n_way, n_support+n_query)\n",
    "    support_query = sq.view(n_way, n_support+n_query, 1, 1, 1).repeat(1, 1, 1, 28, 28)\n",
    "\n",
    "    class _CNN(nn.Module):\n",
    "        def forward(self, x):\n",
    "            out = x[:, 0, 0, 0].view(-1, 1).repeat(1, 64)\n",
    "            return out\n",
    "\n",
    "    net = _CNN()\n",
    "    loss, accuracy, outputs = episode_pn(net, support_query, n_support)\n",
    "\n",
    "    print('outputs:\\n', outputs)\n",
    "    expected_d2 = torch.tensor([\n",
    "        [[    0.,   -64.,  -256.,  -576., -1024.],\n",
    "         [  -64.,     0.,   -64.,  -256.,  -576.]],\n",
    "\n",
    "        [[  -64.,     0.,   -64.,  -256.,  -576.],\n",
    "         [ -256.,   -64.,     0.,   -64.,  -256.]],\n",
    "\n",
    "        [[ -256.,   -64.,     0.,   -64.,  -256.],\n",
    "         [ -576.,  -256.,   -64.,     0.,   -64.]],\n",
    "\n",
    "        [[ -576.,  -256.,   -64.,     0.,   -64.],\n",
    "         [-1024.,  -576.,  -256.,   -64.,     0.]],\n",
    "\n",
    "        [[-1024.,  -576.,  -256.,   -64.,     0.],\n",
    "         [-1536.,  -960.,  -512.,  -192.,     0.]]\n",
    "    ])\n",
    "    expected_d = torch.tensor([\n",
    "        [[-3.3552e-04, -8.0003e+00, -1.6000e+01, -2.4000e+01, -3.2000e+01],\n",
    "         [-8.0007e+00, -6.7080e-04, -8.0007e+00, -1.6001e+01, -2.4001e+01]],\n",
    "\n",
    "        [[-8.0007e+00, -6.7080e-04, -8.0007e+00, -1.6001e+01, -2.4001e+01],\n",
    "         [-1.6001e+01, -8.0007e+00, -6.7092e-04, -8.0007e+00, -1.6001e+01]],\n",
    "\n",
    "        [[-1.6001e+01, -8.0007e+00, -6.7092e-04, -8.0007e+00, -1.6001e+01],\n",
    "         [-2.4001e+01, -1.6001e+01, -8.0007e+00, -6.7080e-04, -8.0007e+00]],\n",
    "\n",
    "        [[-2.4001e+01, -1.6001e+01, -8.0007e+00, -6.7080e-04, -8.0007e+00],\n",
    "         [-3.2000e+01, -2.4000e+01, -1.6000e+01, -8.0003e+00, -3.3552e-04]],\n",
    "\n",
    "        [[-3.2000e+01, -2.4000e+01, -1.6000e+01, -8.0003e+00, -3.3552e-04],\n",
    "         [-3.2000e+01, -2.4000e+01, -1.6000e+01, -8.0003e+00, -3.3552e-04]]\n",
    "    ])\n",
    "    #print('expected outputs:\\n', expected_d2)\n",
    "    assert torch.allclose(outputs, expected_d, rtol=1e-4, atol=10e-4) \\\n",
    "        or torch.allclose(outputs, expected_d2, rtol=1e-4, atol=10e-4), \"outputs does not match expected value\"\n",
    "\n",
    "    print('loss:', loss)\n",
    "    expected_d2 = torch.tensor(25.6000)\n",
    "    expected_d = torch.tensor(3.2005)\n",
    "    #print('expected loss:', expected_d2)\n",
    "    assert torch.allclose(loss, expected_d2, rtol=1e-4, atol=10e-4) \\\n",
    "        or torch.allclose(loss, expected_d, rtol=1e-4, atol=10e-4), \"loss does not match expected value\"\n",
    "\n",
    "    print('accuracy:', accuracy)\n",
    "    expected = .6\n",
    "    print('expected accuracy:', expected)\n",
    "    assert np.allclose(float(accuracy), expected), \"accuracy does not match expected value\"\n",
    "\n",
    "    print('Success')\n",
    "\n",
    "test_episode_pn(episode_pn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f3808714dd1651e6bbf0877803d90a26",
     "grade": false,
     "grade_id": "cell-a88e6a271b270c93",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Train Prototypical Networks\n",
    "\n",
    "In the cell below, we defing the data loaders.\n",
    "\n",
    "Note:\n",
    "* Increasing `num_workers` speeds up the training procedure. However, `num_workers > 0` does not work on some systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Prepare dataloader\n",
    "n_support = 1\n",
    "n_query = 3\n",
    "n_way = 5\n",
    "trainset = OmniglotFewShot(root=data_dir, n_support=n_support, n_query=n_query, train=True, mix=True)\n",
    "trainloader = DataLoader(dataset=trainset, batch_size=n_way, shuffle=True, pin_memory=True, num_workers=3)\n",
    "\n",
    "testset = OmniglotFewShot(root=data_dir, n_support=n_support, n_query=n_query, train=False, mix=False)\n",
    "testloader = DataLoader(dataset=testset, batch_size=n_way, shuffle=False, pin_memory=True, num_workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b8bb613890dea2d2bb8399b3165000c8",
     "grade": false,
     "grade_id": "cell-11ddbad3bea3148f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (cnn): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU()\n",
       "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (8): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): ReLU()\n",
       "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (12): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (14): ReLU()\n",
       "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fcn): Linear(in_features=64, out_features=64, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the model\n",
    "net = CNN()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0f81cd86488a4c641de7da7445a00a8e",
     "grade": false,
     "grade_id": "cell-52691cce123039f7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Training loop\n",
    "\n",
    "Implement the training loop in the cell below.\n",
    "\n",
    "Recommended hyperparameters:\n",
    "* Adam optimizer with learning rate 0.001. It helps to anneal the learning rate to 0.00001 during training (but it is not needed to pass the tests).\n",
    "\n",
    "Hints:\n",
    "* We recommended you to track training and test accuracies returned by function `episode_pn()`.\n",
    "* During training, both training and test accuracies should reach at least the level of 0.96. Note that we sample a limited number of tasks to compute the accuracies and therefore the accuracy values may fluctuate.\n",
    "* **Do not forget to set the network into the training mode during training and to evaluation mode during evaluation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6b16bb5e848bd6f932e2bb98fc139caf",
     "grade": false,
     "grade_id": "cell-09efd9860a9751d4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'list' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m     running_loss\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     17\u001b[0m     running_accuracy\u001b[38;5;241m.\u001b[39mappend(accuracy)\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining loss:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mrunning_loss\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m,running_accuracy\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(trainloader))\n\u001b[1;32m     21\u001b[0m net\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'list' and 'int'"
     ]
    }
   ],
   "source": [
    "# Implement the training loop in this cell\n",
    "if not skip_training:\n",
    "    # YOUR CODE HERE\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr = 0.001)\n",
    "    epochs = 30\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}\")\n",
    "        net.train()\n",
    "        running_loss = []\n",
    "        running_accuracy = []\n",
    "        for support_query in trainloader:\n",
    "            optimizer.zero_grad()\n",
    "            loss, accuracy, ouptuts = episode_pn(net, support_query, n_support)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            running_accuracy += accuracy\n",
    "        print(\"Training loss:\", running_loss/len(trainloader))\n",
    "        print(\"Training accuracy\",running_accuracy/len(trainloader))\n",
    "        \n",
    "        net.eval()\n",
    "        running_loss = []\n",
    "        running_accuracy = []\n",
    "        with torch.no_grad():\n",
    "            for support_query in testloader:\n",
    "                loss, accuracy, ouptuts = episode_pn(net, support_query, n_support)\n",
    "                running_loss += loss.item()\n",
    "                running_accuracy += accuracy\n",
    "        print(\"Testing loss:\", running_loss/len(testloader))\n",
    "        print(\"Testing accuracy\",running_accuracy/len(testloader))\n",
    "    #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to disk (the pth-files will be submitted automatically together with your notebook)\n",
    "# Set confirm=False if you do not want to be asked for confirmation before saving.\n",
    "if not skip_training:\n",
    "    tools.save_model(net, '1_pn.pth', confirm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "53ad9b18adbe3d98925f32d4813e6e37",
     "grade": false,
     "grade_id": "cell-88c8729b8d92cfec",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "if skip_training:\n",
    "    net = CNN()\n",
    "    tools.load_model(net, '1_pn.pth', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "928c46f9bba94d93478392896a5f7a8e",
     "grade": true,
     "grade_id": "test_accuracy",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell tests the accuracy of your model\n",
    "def test_accuracy(net, testloader):\n",
    "    n_support = 1\n",
    "    n_query = 3\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        test_accs = []\n",
    "        for i, support_query in enumerate(testloader):\n",
    "            _, acc, outputs = episode_pn(net, support_query.to(device), n_support)\n",
    "            # My estimation of accuracy\n",
    "            n_way = support_query.size(0)\n",
    "            targets = torch.arange(n_way).view(n_way, 1).repeat(1, n_query).to(device)\n",
    "            acc = (outputs.argmax(dim=2) == targets).sum().float() / targets.numel()\n",
    "            test_accs.append(float(acc))\n",
    "    accuracy = np.mean(test_accs)\n",
    "    print('accuracy:', accuracy)\n",
    "    assert accuracy >= 0.9, 'Poor accuracy of the prototypical networks.'\n",
    "    print('Success')\n",
    "\n",
    "test_accuracy(net, testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9a384d59fc718781a332904c7e24e0cc",
     "grade": false,
     "grade_id": "cell-364bddb7a74b7f4c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Test the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "580359701032650fc35891b6a5e0c4f7",
     "grade": false,
     "grade_id": "cell-a8e2f2267077b11d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Use one clasification task from the test set\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    support_query = next(iter(testloader))\n",
    "    _, acc, outputs = episode_pn(net, support_query.to(device), n_support=1)\n",
    "    print(outputs.argmax(dim=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9a596a1e47b0fbe350f430c5e447724e",
     "grade": false,
     "grade_id": "cell-d9cb299e4585a009",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Interactive demo\n",
    "\n",
    "Please take a look at the demo in [the blog post](https://openai.com/blog/reptile/) about another meta-learning algorithm called Reptile.\n",
    "\n",
    "In the cell below, you can test your prototypical network in a similar setup. In the first row of the figure below, you can draw new classes (support set) using your mouse. Then, you can create three query examples in the second row of the figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c9d41114b1e34e94b261ba3e5ca6c253",
     "grade": false,
     "grade_id": "cell-e61b5a23b1ea87d5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "canvas = tests.Canvas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "924abd5b8121530fc7716a1b4022dd3e",
     "grade": false,
     "grade_id": "cell-c7f4ebeab3a34177",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In the next cell, we classify the images of the query set to one of three classes specified by the support set.\n",
    "The colors of the frames in the bottom row represent the labels produced by the classifier for the query set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "998274ef2834ed73cd93235e0634a850",
     "grade": false,
     "grade_id": "cell-ed7f71cbf32306b2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Convert images into torch tensors\n",
    "support_query = canvas.get_images()\n",
    "print(support_query.shape)\n",
    "\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    _, _, outputs = episode_pn(net, support_query.float().to(device), n_support=1)\n",
    "    # outputs is (n_way, n_query, n_way)\n",
    "classes = outputs.argmax(dim=2).view(-1)\n",
    "\n",
    "tests.plot_classification(support_query, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e76f9c1ff7b71b38682ff2d84db8e566",
     "grade": false,
     "grade_id": "cell-29d055bc39296797",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Conclusions</b>\n",
    "</div>\n",
    "\n",
    "In this exercise, we learned how to train prototypical networks for few-shot learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
