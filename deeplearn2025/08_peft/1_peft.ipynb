{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ab406450c1bc7905ae0a23d933ca0c5b",
     "grade": false,
     "grade_id": "cell-b247a7c1a13a2809",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Deadline:</b> March 19, 2025 (Wednesday) 23:00\n",
    "</div>\n",
    "\n",
    "# Exercise 1. Parameter-efficient fine-tuning of large language models\n",
    "\n",
    "In this assignment, we will learn how to train a large language model (LLM) to memorize new facts. We will add a [LoRA adapter](https://arxiv.org/abs/2106.09685) to the `Llama-3.2-1B-Instruct` model and fine-tuned it on our custom data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7b64bad1c1e36a3e95e9686fe88eed27",
     "grade": false,
     "grade_id": "cell-7fe78c270cc1c700",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the location of the HF cache on JupyterHub\n",
    "if __import__(\"socket\").gethostname().startswith(\"jupyter\"):\n",
    "    import os\n",
    "    os.environ[\"HF_HOME\"] = \"/coursedata/huggingface/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "skip_training = False  # Set this flag to True before validation and submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "326ae69a09daca7af710387d575f1369",
     "grade": true,
     "grade_id": "cell-f357feeef0e44248",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# During evaluation, this cell sets skip_training to True\n",
    "\n",
    "import tools, warnings\n",
    "warnings.showwarning = tools.customwarn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d33341c4691e2af91dbfd65b2d2cdd9b",
     "grade": false,
     "grade_id": "cell-68d82bb6c9d904b2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 17:05:59.933136: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from peft.peft_model import PeftModel\n",
    "from functools import partial\n",
    "\n",
    "from tools import print_message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8e6714a89d9ce93d02438ffc1a6efa93",
     "grade": false,
     "grade_id": "cell-0c81dd1c6a5383b8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "39121ffe5b6f216a8d24f9f65ca21b13",
     "grade": false,
     "grade_id": "cell-c1b86a2ff932e62f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "First we load the `Llama-3.2-1B-Instruct` model by Meta from the Hugging Face (HF) repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "15080f93bd9fd67ef0ede3c8ea8fa889",
     "grade": false,
     "grade_id": "cell-9865b9cc22ef0c12",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Select the device for training (use GPU if you have one). Please, change the `torch_dtype` from `torch.bfloat16` to `torch.float32` if you have at least 8GB of CPU memory in your machine. This helps to get responses from the Llama model much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "torch_dtype = torch.bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "761317572cd6832dd73e1e3ee5e18614",
     "grade": true,
     "grade_id": "cell-54385a05bda1c9ec",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch_dtype: torch.bfloat16\n",
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-15): 16 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "print(f\"torch_dtype: {torch_dtype}\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch_dtype\n",
    ")\n",
    "print(base_model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "482ebf8194589a9703554ff682e8867e",
     "grade": false,
     "grade_id": "cell-9987083e9d286a2d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Let's try to ask the model something. First we create a dialogue (that consists of one message from the user).\n",
    "Then we convert the dialogue into a prompt using the template required by Llama 3.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Who are you?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from llm_utils import apply_chat_template_llama3\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"Who are you?\"}]\n",
    "prompt = apply_chat_template_llama3(messages, add_bot=False)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cd116ed6e81aecb1c42f0ae330c5f343",
     "grade": false,
     "grade_id": "cell-bad2f6b1cf56a417",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Note the format of the prompt that we produced. You can find more details on Llama's prompt format [on this page](https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_2/).\n",
    "\n",
    "Now let's get a response from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "98d48be850f5eefaa86d0ae4208b2530",
     "grade": false,
     "grade_id": "cell-157c9dd551bf83d6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I'm an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "prompt_length = inputs[\"input_ids\"].size(1)\n",
    "with torch.no_grad():\n",
    "    tokens = base_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=150,\n",
    "        do_sample=True,\n",
    "        temperature=0.01,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        #streamer=TextStreamer(tokenizer=tokenizer, skip_prompt=True),\n",
    "    )\n",
    "# Extract the new tokens generated (excluding the prompt)\n",
    "output_tokens = tokens[:, prompt_length:]\n",
    "\n",
    "# Decode the output tokens to a string\n",
    "output_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "print_message(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0539cea664a39250cf6e34af30790a09",
     "grade": false,
     "grade_id": "cell-baf92f39989c8370",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Let us evaluate the model on some trivial, common, questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Q: Who wrote the play Romeo and Juliet?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "GT Answer: William Shakespeare"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Network answer: The play \"Romeo and Juliet\" was written by the renowned English playwright William Shakespeare. It is one of his most famous and iconic works, and is considered a tragedy.\n",
       "Time: 51.53s, Tokens: 37, Speed: 0.72 tokens/s"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Score: True"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Q: What is the capital city of France?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "GT Answer: Paris"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Network answer: The capital city of France is Paris.\n",
       "Time: 23.05s, Tokens: 9, Speed: 0.39 tokens/s"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Score: True"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Q: Who painted the Mona Lisa?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "GT Answer: Leonardo da Vinci"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Network answer: The Mona Lisa was painted by the Italian artist Leonardo da Vinci. He created the painting in the early 16th century, specifically between 1503 and 1506. It is one of his most famous works and is widely considered to be one of the greatest paintings of all time.\n",
       "Time: 75.21s, Tokens: 59, Speed: 0.78 tokens/s"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Score: True"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Q: What is the smallest planet in our solar system?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "GT Answer: Mercury"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Network answer: The smallest planet in our solar system is Mercury. It is the innermost planet and has a diameter of approximately 4,879 kilometers (3,031 miles).\n",
       "Time: 51.36s, Tokens: 34, Speed: 0.66 tokens/s"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Score: True"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Q: How many states in USA?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "GT Answer: 50 states"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Network answer: There are 50 states in the United States of America.\n",
       "Time: 25.86s, Tokens: 13, Speed: 0.50 tokens/s"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Score: True"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Q: What is the chemical symbol for gold?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "GT Answer: Au"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Network answer: The chemical symbol for gold is Au.\n",
       "Time: 23.13s, Tokens: 9, Speed: 0.39 tokens/s"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Score: True"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Q: Who was the first President of the United States?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "GT Answer: George Washington"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Network answer: The first President of the United States was George Washington. He was inaugurated on April 30, 1789, and served two terms in office until March 4, 1797.\n",
       "Time: 57.48s, Tokens: 40, Speed: 0.70 tokens/s"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Score: True"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Q: What is the tallest mountain in the world?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "GT Answer: Mount Everest"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Network answer: The tallest mountain in the world is Mount Everest, located in the Himalayas on the border between Nepal and Tibet, China. It stands at an impressive 8,848.86 meters (29,031.7 feet) above sea level.\n",
       "Time: 67.64s, Tokens: 50, Speed: 0.74 tokens/s"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Score: True"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Q: Which scientist developed the theory of general relativity?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "GT Answer: Albert Einstein"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Network answer: The theory of general relativity was developed by Albert Einstein.\n",
       "Time: 28.72s, Tokens: 13, Speed: 0.45 tokens/s"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Score: True"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Q: What is the largest ocean on Earth?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "GT Answer: Pacific Ocean"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from grading import Evaluator, get_answer\n",
    "\n",
    "qa_trivial_json = \"grading_trivia.json\"\n",
    "\n",
    "get_answer_fn = partial(get_answer, model=base_model, tokenizer=tokenizer)\n",
    "\n",
    "evaluator = Evaluator(qa_trivial_json)\n",
    "trivia_accuracy = evaluator.evaluate_all(get_answer_fn, verbose=True)\n",
    "print(f\"Accuracy on the trivia set (original model): {trivia_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bd9947be0c8600e6742e1235131d1c73",
     "grade": false,
     "grade_id": "cell-76af61a0caea109d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Custom document\n",
    "\n",
    "We want our model to memorize facts from a tiny document `document.txt` that we artificially generated. Let's print the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5d554d254651e1e05c555c069e048a66",
     "grade": false,
     "grade_id": "cell-1d800627adbd320c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(__import__('pathlib').Path(\"document.txt\").read_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5488effc4e87b50304585c091e157f38",
     "grade": false,
     "grade_id": "cell-f28330bd6278ea12",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "We want our model to be able to answer questions related to the document **without seeing the document in the prompt**. Let's test what the base model responds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"How many seats are there in the Midnight Sun room in the Frostbite Futures HQ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "85797cd56da5d5ba01beaf7077ac4f42",
     "grade": false,
     "grade_id": "cell-9947e187d84847f5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "_ = get_answer(question, answer=[\"4 seats\", \"4\"], model=base_model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task in this assignment is to generate training data and train the model. We advice you to inspect function `get_answer` to see how the question is converted into a prompt. You should use the same conversion in your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "88c2bd23875af4013be9d6338b383e13",
     "grade": false,
     "grade_id": "cell-05404aecd8161da1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Model training\n",
    "\n",
    "**IMPORTANT:**\n",
    "The assignment does not require a training loop to be provided. However, if you choose to include one for autograding purposes, please implement it in the designated cell.\n",
    "\n",
    "In this exercise, we integrate a [LoRA adapter](https://arxiv.org/abs/2106.09685) into the base Llama model using the `peft` library.\n",
    "\n",
    "**IMPORTANT:**\n",
    "For the `transformers` and `peft` packages, we *strongly recommend* using the versions specified in the [requirements.yml](https://mycourses.aalto.fi/mod/resource/view.php?id=1241109) file (i.e., `peft=0.13.2`, `transformers=4.47.0`).\n",
    "\n",
    "**IMPORTANT:**\n",
    "The `peft` library offers multiple methods to attach an adapter to the base model. To ensure compatibility and avoid potential issues when loading the trained adapter, please create your peft model using function `get_peft_model`, as explained on [this page](https://huggingface.co/docs/peft/en/quicktour). Using alternative methods may lead to errors or inconsistencies during the loading process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3a9b280739ef9dbd3bf35d64d5824572",
     "grade": false,
     "grade_id": "cell-610a2c0f2f01855d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Training loop\n",
    "\n",
    "* A model created by `get_peft_model` is a regular pytorch model which you can train just like any other model.\n",
    "* Note that the output of the `forward` function is not a tensor but a more complex structure.\n",
    "* You can use any code for training, for example, you can use HF's `Trainer` objects. However, we stronlgy encourage you to implement the training loop by yourselves.\n",
    "* Please save the model to folder `1_adapter` using this code:\n",
    "```\n",
    "peft_model.save_pretrained(\"1_adapter\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the train and test dataset splits below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e760e9b580262f9d843030030598e60e",
     "grade": false,
     "grade_id": "cell-227beb3a18cd47e5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cc6345ac42ff677df61b3494f71b7c3f",
     "grade": false,
     "grade_id": "cell-9dc3eb28d3cd82ce",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Implement the test and train dataloaders below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "76911311bd576a1f2135488bf527020c",
     "grade": false,
     "grade_id": "cell-e87cc603cdf62654",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "523b74f92b2dcfc9667fb2165e7db5c3",
     "grade": false,
     "grade_id": "cell-98f799f7b90fc750",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Implement your model in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f790eb9f85f867c4319bdc6d88b46c77",
     "grade": false,
     "grade_id": "cell-2f6d8bbabf9d1538",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cf7d2241bc5ed6a30150cf33c6fd5c8f",
     "grade": false,
     "grade_id": "cell-9017c45c277894e9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "The training loop is defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fcbcedba1346f31b74df0934f5f0f4b8",
     "grade": false,
     "grade_id": "cell-b98014449660f485",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3b6116eb061f02bbbb035fad51adbff3",
     "grade": false,
     "grade_id": "cell-38ef5ea884fafe9a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Save the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "994b6bfc4e0a488e98bac02130c82a83",
     "grade": false,
     "grade_id": "cell-850bc59b2c17fe3e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2a6076b945d8d50d0bbb5e690248b1c3",
     "grade": false,
     "grade_id": "cell-364bddb7a74b7f4c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Test the trained model\n",
    "\n",
    "**IMPORTANT:** Once you have trained your model, ensure that the remaining cells in this notebook execute correctly. Failure to do so may result in a loss of points, as successful execution is part of the evaluation criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "101dd72116523fc177317df6a2d859ff",
     "grade": false,
     "grade_id": "cell-91a118b4cfe44a72",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "First, we load the trained model. Note that the base model should be loaded already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "13e0b7a4ff758df9064cfa4b6b876d12",
     "grade": false,
     "grade_id": "cell-a8e2f2267077b11d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\nLoading the adapter\")\n",
    "base_model.to(device)\n",
    "peft_model = PeftModel.from_pretrained(base_model, \"1_adapter\")\n",
    "peft_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "94ff3fa77901260e6762d46c16506c9b",
     "grade": false,
     "grade_id": "cell-d9cb299e4585a009",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Test common knowledge\n",
    "\n",
    "We evaluate how well the model with the adapter recalls trivia facts.\n",
    "\n",
    "**Note:** Successfully passing this test is mandatory to earn points for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_answer_peft_fn = partial(get_answer, model=peft_model, tokenizer=tokenizer)\n",
    "\n",
    "evaluator = Evaluator(qa_trivial_json)\n",
    "trivia_accuracy_trained = evaluator.evaluate_all(get_answer_peft_fn, verbose=True)\n",
    "print(f\"Accuracy on the trivia set (trained model): {trivia_accuracy_trained:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ca003aba74c49f26bd310505d755488d",
     "grade": true,
     "grade_id": "common_knowledge_set",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# [AUTOGRADING] This cell tests the model on the public common knowledge set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a4b76f9db40e39d67c78251d96ab8539",
     "grade": false,
     "grade_id": "cell-f3c4b219df0f188e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Test new knowledge\n",
    "\n",
    "Next we test the new knowledge. It is a non-trivial task to train the model to memorize all the new facts. In order to get full points, your model should answer correctly at least two test questions. Note that the grading procedure can make mistakes as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0d60d501eb7de278b11dfe40f0bf58fc",
     "grade": false,
     "grade_id": "cell-e0259706daa05d28",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### Evaluation on the validation set (open):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3356b37439b2d4e3978ed613ab12543c",
     "grade": false,
     "grade_id": "cell-f5571411603d7136",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "qa_val_json = \"grading_val.json\"\n",
    "evaluator_val = Evaluator(qa_val_json)\n",
    "val_accuracy = evaluator_val.evaluate_all(get_answer_peft_fn, verbose=True)\n",
    "print(f\"Accuracy on the validation set: {val_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "74407ce34361cf34a216d3a9be17cd2d",
     "grade": true,
     "grade_id": "cell-d4a1a441812d9ac4",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# [AUTOGRADING] This cell is reserved for auto-grading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e9986b30e0d603e4e30da93790b5a426",
     "grade": true,
     "grade_id": "cell-c5e27193f546a8ef",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# [AUTOGRADING] This cell is reserved for auto-grading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8e297875e1efad9ade8568eee0c4ba6d",
     "grade": true,
     "grade_id": "validation_set",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# [AUTOGRADING] This cell tests the model on the public validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d7be5770ff27b1ba4d8cc8c891b7ed99",
     "grade": false,
     "grade_id": "cell-841e3cd8bd52370f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### Evaluation on the test set (hidden):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "71d2ca7249dd36ab2296eb9222610098",
     "grade": true,
     "grade_id": "test_set",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# [AUTOGRADING] This cell tests the model on the hidden test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9e4f94a79cc7580d508579a2a45b459f",
     "grade": false,
     "grade_id": "cell-29d055bc39296797",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Conclusions</b>\n",
    "</div>\n",
    "\n",
    "In this exercise, we learned how to train a large language model (LLM) to memorize new facts. We added a LoRA adapter to an LLM and fine-tuned it on our custom data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
