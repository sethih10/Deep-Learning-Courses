{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8250d9247fb604fdc9bbfc4bafaaf1c0",
     "grade": false,
     "grade_id": "cell-b247a7c1a13a2809",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Deadline:</b> March 12, 2025 (Wednesday) 23:00\n",
    "</div>\n",
    "\n",
    "# Exercise 1. Parameter-efficient fine-tuning of large language models\n",
    "\n",
    "In this assignment, we will learn how to train a large language model (LLM) to memorize new facts. We will add a [LoRA adapter](https://arxiv.org/abs/2106.09685) to the `Llama-3.2-1B-Instruct` model and fine-tuned it on our custom data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7b64bad1c1e36a3e95e9686fe88eed27",
     "grade": false,
     "grade_id": "cell-7fe78c270cc1c700",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the location of the HF cache on JupyterHub\n",
    "if __import__(\"socket\").gethostname().startswith(\"jupyter\"):\n",
    "    import os\n",
    "    os.environ[\"HF_HOME\"] = \"/coursedata/huggingface/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "skip_training = False  # Set this flag to True before validation and submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "326ae69a09daca7af710387d575f1369",
     "grade": true,
     "grade_id": "cell-f357feeef0e44248",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# During evaluation, this cell sets skip_training to True\n",
    "\n",
    "import tools, warnings\n",
    "warnings.showwarning = tools.customwarn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d33341c4691e2af91dbfd65b2d2cdd9b",
     "grade": false,
     "grade_id": "cell-68d82bb6c9d904b2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from peft.peft_model import PeftModel\n",
    "from functools import partial\n",
    "\n",
    "from tools import print_message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8e6714a89d9ce93d02438ffc1a6efa93",
     "grade": false,
     "grade_id": "cell-0c81dd1c6a5383b8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "39121ffe5b6f216a8d24f9f65ca21b13",
     "grade": false,
     "grade_id": "cell-c1b86a2ff932e62f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "First we load the `Llama-3.2-1B-Instruct` model by Meta from the Hugging Face (HF) repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "15080f93bd9fd67ef0ede3c8ea8fa889",
     "grade": false,
     "grade_id": "cell-9865b9cc22ef0c12",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Select the device for training (use GPU if you have one). Please, change the `torch_dtype` from `torch.bfloat16` to `torch.float32` if you have at least 8GB of CPU memory in your machine. This helps to get responses from the Llama model much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "torch_dtype = torch.bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "761317572cd6832dd73e1e3ee5e18614",
     "grade": true,
     "grade_id": "cell-54385a05bda1c9ec",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "print(f\"torch_dtype: {torch_dtype}\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch_dtype\n",
    ")\n",
    "print(base_model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "482ebf8194589a9703554ff682e8867e",
     "grade": false,
     "grade_id": "cell-9987083e9d286a2d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Let's try to ask the model something. First we create a dialogue (that consists of one message from the user).\n",
    "Then we convert the dialogue into a prompt using the template required by Llama 3.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llm_utils import apply_chat_template_llama3\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"Who are you?\"}]\n",
    "prompt = apply_chat_template_llama3(messages, add_bot=False)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cd116ed6e81aecb1c42f0ae330c5f343",
     "grade": false,
     "grade_id": "cell-bad2f6b1cf56a417",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Note the format of the prompt that we produced. You can find more details on Llama's prompt format [on this page](https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_2/).\n",
    "\n",
    "Now let's get a response from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "98d48be850f5eefaa86d0ae4208b2530",
     "grade": false,
     "grade_id": "cell-157c9dd551bf83d6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "prompt_length = inputs[\"input_ids\"].size(1)\n",
    "with torch.no_grad():\n",
    "    tokens = base_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=150,\n",
    "        do_sample=True,\n",
    "        temperature=0.01,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        #streamer=TextStreamer(tokenizer=tokenizer, skip_prompt=True),\n",
    "    )\n",
    "# Extract the new tokens generated (excluding the prompt)\n",
    "output_tokens = tokens[:, prompt_length:]\n",
    "\n",
    "# Decode the output tokens to a string\n",
    "output_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "print_message(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us evaluate the model on some trivial, common, questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from grading import Evaluator, get_answer\n",
    "\n",
    "qa_trivial_json = \"grading_trivia.json\"\n",
    "\n",
    "get_answer_fn = partial(get_answer, model=base_model, tokenizer=tokenizer)\n",
    "\n",
    "evaluator = Evaluator(qa_trivial_json)\n",
    "trivia_accuracy = evaluator.evaluate_all(get_answer_fn, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can test the base peft_model on some trivia questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bd9947be0c8600e6742e1235131d1c73",
     "grade": false,
     "grade_id": "cell-76af61a0caea109d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Custom document\n",
    "\n",
    "We want our model to memorize facts from a tiny document `document.txt` that we artificially generated. Let's print the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5d554d254651e1e05c555c069e048a66",
     "grade": false,
     "grade_id": "cell-1d800627adbd320c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(__import__('pathlib').Path(\"document.txt\").read_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5488effc4e87b50304585c091e157f38",
     "grade": false,
     "grade_id": "cell-f28330bd6278ea12",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "We want our model to be able to answer questions related to the document **without seeing the document in the prompt**. Let's test what the base model responds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"How many seats are there in the Midnight Sun room in the Frostbite Futures HQ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "85797cd56da5d5ba01beaf7077ac4f42",
     "grade": false,
     "grade_id": "cell-9947e187d84847f5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "_ = get_answer(question, answer=[\"4 seats\", \"4\"], model=base_model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task in this assignment is to generate training data and train the model. We advice you to inspect function `get_answer` to see how the question is converted into a prompt. You should use the same conversion in your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "88c2bd23875af4013be9d6338b383e13",
     "grade": false,
     "grade_id": "cell-05404aecd8161da1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Model training\n",
    "\n",
    "**IMPORTANT:**\n",
    "The assignment does not require a training loop to be provided. However, if you choose to include one for autograding purposes, please implement it in the designated cell.\n",
    "\n",
    "In this exercise, we integrate a [LoRA adapter](https://arxiv.org/abs/2106.09685) into the base Llama model using the `peft` library.\n",
    "\n",
    "**IMPORTANT:**\n",
    "For the `transformers` and `peft` packages, we *strongly recommend* using the versions specified in the [requirements.yml](https://mycourses.aalto.fi/mod/resource/view.php?id=1241109) file (i.e., `peft=0.13.2`, `transformers=4.47.0`).\n",
    "\n",
    "**IMPORTANT:**\n",
    "The `peft` library offers multiple methods to attach an adapter to the base model. To ensure compatibility and avoid potential issues when loading the trained adapter, please create your peft model using function `get_peft_model`, as explained on [this page](https://huggingface.co/docs/peft/en/quicktour). Using alternative methods may lead to errors or inconsistencies during the loading process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3a9b280739ef9dbd3bf35d64d5824572",
     "grade": false,
     "grade_id": "cell-610a2c0f2f01855d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Training loop\n",
    "\n",
    "* A model created by `get_peft_model` is a regular pytorch model which you can train just like any other model.\n",
    "* Note that the output of the `forward` function is not a tensor but a more complex structure.\n",
    "* You can use any code for training, for example, you can use HF's `Trainer` objects. However, we stronlgy encourage you to implement the training loop by yourselves.\n",
    "* Please save the model to folder `1_adapter` using this code:\n",
    "```\n",
    "peft_model.save_pretrained(\"1_adapter\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the train and test dataset splits below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e760e9b580262f9d843030030598e60e",
     "grade": false,
     "grade_id": "cell-227beb3a18cd47e5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the test and train dataloaders below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "76911311bd576a1f2135488bf527020c",
     "grade": false,
     "grade_id": "cell-e87cc603cdf62654",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement your model in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f790eb9f85f867c4319bdc6d88b46c77",
     "grade": false,
     "grade_id": "cell-2f6d8bbabf9d1538",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop is defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fcbcedba1346f31b74df0934f5f0f4b8",
     "grade": false,
     "grade_id": "cell-b98014449660f485",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "994b6bfc4e0a488e98bac02130c82a83",
     "grade": false,
     "grade_id": "cell-850bc59b2c17fe3e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2a6076b945d8d50d0bbb5e690248b1c3",
     "grade": false,
     "grade_id": "cell-364bddb7a74b7f4c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Test the trained model\n",
    "\n",
    "**IMPORTANT:** Once you have trained your model, ensure that the remaining cells in this notebook execute correctly. Failure to do so may result in a loss of points, as successful execution is part of the evaluation criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "101dd72116523fc177317df6a2d859ff",
     "grade": false,
     "grade_id": "cell-91a118b4cfe44a72",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "First, we load the trained model. Note that the base model should be loaded already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "13e0b7a4ff758df9064cfa4b6b876d12",
     "grade": false,
     "grade_id": "cell-a8e2f2267077b11d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\nLoading the adapter\")\n",
    "base_model.to(device)\n",
    "peft_model = PeftModel.from_pretrained(base_model, \"1_adapter\")\n",
    "peft_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "94ff3fa77901260e6762d46c16506c9b",
     "grade": false,
     "grade_id": "cell-d9cb299e4585a009",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Test common knowledge\n",
    "\n",
    "We evaluate how well the model with the adapter recalls trivia facts.\n",
    "\n",
    "**Note:** Successfully passing this test is mandatory to earn points for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d99b401951ae46757272c124411e1f70",
     "grade": true,
     "grade_id": "cell-e61b5a23b1ea87d5",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_answer_peft_fn = partial(get_answer, model=peft_model, tokenizer=tokenizer)\n",
    "\n",
    "# %%\n",
    "evaluator = Evaluator(qa_trivial_json)\n",
    "trivia_accuracy = evaluator.evaluate_all(get_answer_peft_fn, verbose=True)\n",
    "\n",
    "print(f\"Accuracy on the trivia set: {trivia_accuracy:.2f}\")\n",
    "assert trivia_accuracy >= 0.9, \"The model does not perform well on the trivia set.\"\n",
    "print(\"Success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a4b76f9db40e39d67c78251d96ab8539",
     "grade": false,
     "grade_id": "cell-f3c4b219df0f188e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Test new knowledge\n",
    "\n",
    "Next we test the new knowledge. It is a non-trivial task to train the model to memorize all the new facts. In order to get full points, your model should answer correctly at least two test questions. Note that the grading procedure can make mistakes as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on the validation set (open):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_accuracy = 0\n",
    "qa_val_json = \"grading_val.json\"\n",
    "evaluator_val = Evaluator(qa_val_json)\n",
    "val_accuracy = evaluator_val.evaluate_all(get_answer_peft_fn, verbose=True)\n",
    "assert val_accuracy > 0.1, \"The model does not perform well on the validation set.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Evaluation on the test set (hidden):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4830962072368e618fcaf637b2b173b2",
     "grade": true,
     "grade_id": "cell-7eb4e8cfef36ac5a",
     "locked": true,
     "points": 7,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_accuracy = 0.\n",
    "print(f\"Accuracy on the test set: {test_accuracy:.2f}\")\n",
    "assert test_accuracy > 0.1, \"The model does not perform well on the test set.\"\n",
    "assert trivia_accuracy >= 0.9, \"The model does not perform well on the trivia set.\"\n",
    "print(\"Success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9e4f94a79cc7580d508579a2a45b459f",
     "grade": false,
     "grade_id": "cell-29d055bc39296797",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Conclusions</b>\n",
    "</div>\n",
    "\n",
    "In this exercise, we learned how to train a large language model (LLM) to memorize new facts. We added a LoRA adapter to an LLM and fine-tuned it on our custom data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
