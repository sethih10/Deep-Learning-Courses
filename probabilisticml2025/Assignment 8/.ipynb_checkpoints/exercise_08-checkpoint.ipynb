{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4eb7fdd9fcaa6edc5c6958bf3412ea1a",
     "grade": false,
     "grade_id": "cell-7e3f1e2b3d7409fd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## CS-E4825 - Probabilistic Machine Learning D (spring 2025)\n",
    "\n",
    "Pekka Marttinen, Negar Safinianaini, Mihriban Kocak, Bo Zheng, Batuhan Avci.\n",
    "\n",
    "## Exercise 8, due on Friday 14th March at 10:15.\n",
    "\n",
    "### Contents\n",
    "1. Problem 1: Minimize KL divergence using PyTorch\n",
    "2. Problem 2: VB for a factor analysis model (1/2)\n",
    "3. Problem 3: VB for a factor analysis model (2/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "48aefe82a5afe18f9185bfb71bbfb637",
     "grade": false,
     "grade_id": "cell-8f76ca03c69cb4d1",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "# Problem 1: Minimize KL divergence using PyTorch\n",
    "PyTorch is a powerful auto-differentiation framework that allows us to do any optimization, as long as we can define the objective function and corresponding optimization variables. It has been widely used for Bayesian deep learning. In this exercise, we will study how to use PyTorch to fit a Gaussian distribution to a known Mixture of Gaussian by minimizing their KL divergence, and compare the difference between the forward and reverse form of the KL.\n",
    "\n",
    "Recall that the KL divergence between two distributions $q(x)$ and $p(x)$ is defined as:\n",
    "\n",
    "$$\\text{KL}[q(x)|p(x)]=\\int q(x)\\log\\frac{q(x)}{p(x)}dx.$$\n",
    "\n",
    "This is typically called the **Reverse KL** which we have used before in the course (like in Variational Bayes). If the probability density functions of $q(x)$ and $p(x)$ are known, and we can get samples from $q(x)$, an unbiased estimator of KL divergence is:\n",
    "$$\\text{KL}[q(x)|p(x)]\\approx\\log\\frac{q(x_i)}{p(x_i)}=\\log q(x_i)-\\log p(x_i),$$\n",
    "where $x_i\\sim q(x)$. We will use above estimator for this exercise.\n",
    "\n",
    "There is also a **Forward KL**: $\\text{KL}[p(x)|q(x)]$ defined as:\n",
    "\n",
    "$$\\text{KL}[p(x)|q(x)]=\\int p(x)\\log\\frac{p(x)}{q(x)}dx,$$\n",
    "\n",
    "which is used in other inference algorithms such as Expectration Propogation which is not within the scope of this course.\n",
    "\n",
    "Let $p(x \\mid \\pi) = \\pi \\mathcal{N}(0,1)+(1-\\pi)\\mathcal{N}(8,1)$ where $\\pi\\sim\\text{Bernoulli}(0.4)$ be the true mixture distribution which we want to fit using a Gaussian $q(x; \\mu, \\sigma)$. We want to estimate $\\mu$ and $\\sigma$ using both the forwared and reverse KL.\n",
    "\n",
    "Complete the template below with the relevant code.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d052f67014581203e59c5bf9d58823cc",
     "grade": false,
     "grade_id": "cell-33040ac0aa3bb106",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing reverse KL\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "log(): argument 'input' (position 1) must be Tensor, not float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 177\u001b[0m\n\u001b[1;32m    175\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    176\u001b[0m kl_reverse \u001b[38;5;241m=\u001b[39m KL_divergence()\n\u001b[0;32m--> 177\u001b[0m \u001b[43moptimization\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkl_reverse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m Gaussian_reverse\u001b[38;5;241m=\u001b[39m kl_reverse\u001b[38;5;241m.\u001b[39mgaussian\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOptimizing forward KL\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 166\u001b[0m, in \u001b[0;36moptimization\u001b[0;34m(kl, forward, learning_rate, num_epoch)\u001b[0m\n\u001b[1;32m    164\u001b[0m     loss \u001b[38;5;241m=\u001b[39m kl\u001b[38;5;241m.\u001b[39mcompute_forwardKL()\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mkl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_reverseKL\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    169\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[6], line 152\u001b[0m, in \u001b[0;36mKL_divergence.compute_reverseKL\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m# compute the reverse KL divergence between p and q with num_samples data points\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# Return the estimated reverse KL divergence\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    149\u001b[0m \n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# YOUR CODE HERE\u001b[39;00m\n\u001b[1;32m    151\u001b[0m samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgaussian\u001b[38;5;241m.\u001b[39msample(num_samples)\n\u001b[0;32m--> 152\u001b[0m rkl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgaussian\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogprob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmog\u001b[38;5;241m.\u001b[39mlogprob(samples)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m#raise NotImplementedError()\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rkl\n",
      "Cell \u001b[0;32mIn[6], line 48\u001b[0m, in \u001b[0;36mGaussian.logprob\u001b[0;34m(self, samples)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlogprob\u001b[39m(\u001b[38;5;28mself\u001b[39m, samples):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# Compute the log probability of each sample under Gaussian distribution\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m# Return a tensor containing the log probability of all samples        \u001b[39;00m\n\u001b[1;32m     45\u001b[0m     \n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m# logp = ? # EXERCISE\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;66;03m# YOUR CODE HERE\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m     logp \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpi\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstd_dev) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m0.5\u001b[39m\u001b[38;5;241m*\u001b[39m((samples \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m/\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstd_dev\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;66;03m#raise NotImplementedError()\u001b[39;00m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m logp\n",
      "\u001b[0;31mTypeError\u001b[0m: log(): argument 'input' (position 1) must be Tensor, not float"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.distributions as Dis\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Gaussian:\n",
    "    \"\"\"\n",
    "    This represents q(x) \n",
    "    Gaussian distribution is parametrized by mean (mu) and standard deviation. The standard deviation is \n",
    "    parametrized as sigma = log(1 + exp(rho)) to make it positive all the time. A sample from the distribution\n",
    "    can be obtained by first sampling from a unit Gaussian, shifting the samples by the mean and scaling by the \n",
    "    standard deviation: w = mu + log(1 + exp(rho)) * epsilon\n",
    "    \"\"\"\n",
    "    def __init__(self, mu, rho):\n",
    "        self.mean = mu\n",
    "        self.rho = rho\n",
    "\n",
    "    @property\n",
    "    def std_dev(self):\n",
    "        return torch.log1p(torch.exp(self.rho))\n",
    "\n",
    "    def sample(self, num_samples = 1):\n",
    "        # Sample num_samples data points from Gaussian distribution\n",
    "        # Return a tensor contains all the samples \n",
    "        \n",
    "        # Sample num_samples datapoints from N(0,1) \n",
    "        epsilon = Dis.Normal(0,1).sample([num_samples])\n",
    "        \n",
    "        # Scale and shift epsilon\n",
    "        # samples = ? # EXERCISE\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        samples = self.mean + self.std_dev*epsilon\n",
    "        #raise NotImplementedError()\n",
    "        \n",
    "        return samples\n",
    "    \n",
    "    def logprob(self, samples):\n",
    "        # Compute the log probability of each sample under Gaussian distribution\n",
    "        # Return a tensor containing the log probability of all samples        \n",
    "        \n",
    "        # logp = ? # EXERCISE\n",
    "        # YOUR CODE HERE\n",
    "        logp = -0.5*torch.log(2*torch.pi) - torch.log(self.std_dev) - 0.5*((samples - self.mean)**2)/(self.std_dev**2)\n",
    "        #raise NotImplementedError()\n",
    "        \n",
    "        return logp\n",
    "    \n",
    "class MoG:\n",
    "    \"\"\"\n",
    "    This represents p(x).\n",
    "    In this example, mixture of two Gaussian distribution is constructed by 2 Gaussian distributions \n",
    "    N(0,2) and N(8,1), and each datapoint is from N(0,2) with probability p = 0.4 and from N(8,1) with \n",
    "    probability 0.6.\n",
    "    \"\"\"\n",
    "    def __init__(self, mu_1=0., sigma_1=2., mu_2=8., sigma_2=1., prob = 0.4):\n",
    "        self.mean_1 = torch.tensor(mu_1)\n",
    "        self.sigma_1 = torch.tensor(sigma_1)\n",
    "        self.mean_2 = torch.tensor(mu_2)\n",
    "        self.sigma_2 = torch.tensor(sigma_2)\n",
    "        self.prob = torch.tensor(prob)\n",
    "\n",
    "    def sample(self, num_samples = 1):\n",
    "        # Sample num_samples data points from MoG distribution\n",
    "        # Return a tensor contains all the samples\n",
    "        \n",
    "        # sample from N(0, 2)\n",
    "        # sample form N(8, 1)\n",
    "        # sample from Bern(0.4)\n",
    "        # Combine the three to from a sample form mixture\n",
    "        # sample_gaussian_1 = ? # EXERCISE\n",
    "        # sample_gaussian_2 = ? # EXERCISE\n",
    "        # sample_bernoulli = ? # EXERCISE\n",
    "        # samples  = ? # EXERCISE\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        sample_gaussian_1 = Dis.Normal(self.mean_1, self.sigma_1).sample([num_samples])\n",
    "        sample_gaussian_2 = Dis.Normal(self.mean_2, self.sigma_2).sample([num_samples])\n",
    "        sample_bernoulli = Dis.Bernoulli(self.prob).sample([num_samples])\n",
    "        samples = sample_bernoulli*sample_gaussian_1 + (1 - sample_bernoulli)*sample_gaussian_2\n",
    "        #raise NotImplementedError()\n",
    "        \n",
    "        return samples\n",
    "    \n",
    "    def logprob(self, samples):\n",
    "        \n",
    "        # Compute the log probability of each sample under the MoG distribution\n",
    "        # Return a tensor containing the log probability of all samples\n",
    "        \n",
    "        # logp = ? # EXERCISE\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        logb1 = samples*torch.log(self.prob) + (1-samples)*torch.log(1 - self.prob)\n",
    "        logb2 = samples*torch.log(self.prob) + (1-samples)*torch.log(1 - self.prob)\n",
    "        \n",
    "        logg1 = -0.5*torch.log(2*torch.pi*self.sigma_1) - 0.5*((samples - self.mean_1)**2)/(self.sigma_1)\n",
    "        logg2 = -0.5*torch.log(2*torch.pi*self.sigma_2) - 0.5*((samples - self.mean_2)**2)/(self.sigma_2)\n",
    "        \n",
    "        logp = logb1*logg1 + logb2*logg2\n",
    "        #raise NotImplementedError()\n",
    "        \n",
    "        return logp    \n",
    "\n",
    "class KL_divergence(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(KL_divergence, self).__init__()\n",
    "        # define the mean and standard deviation as parameters, and initialization\n",
    "        self.mu = nn.Parameter(torch.Tensor(1).uniform_(-2., 12.))\n",
    "        self.rho = nn.Parameter(torch.Tensor(1).uniform_(1.0, 5.0))\n",
    "        \n",
    "        self.gaussian = Gaussian(self.mu, self.rho)\n",
    "        self.mog = MoG()\n",
    "    \n",
    "    def compute_forwardKL(self):\n",
    "        num_samples = torch.tensor(1000)\n",
    "        \n",
    "        # compute the forward KL divergence between p and q of num_samples data points\n",
    "        # Return the estimated forward KL divergence\n",
    "        \n",
    "        \n",
    "        # sample form MoG\n",
    "        # compute forware KL \n",
    "        \n",
    "        # samples = ? # EXERCISE\n",
    "        # fkl = ? # EXERCISE\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        samples = self.mog.sample(num_samples)\n",
    "        fkl_ = samples*(self.mog.logprob(samples) - self.gaussian.logprob(samples))\n",
    "        fkl = torch.sum(fkl_)\n",
    "        #raise NotImplementedError()\n",
    "        \n",
    "        return fkl\n",
    "    \n",
    "    def compute_reverseKL(self):\n",
    "        num_samples = torch.tensor(1000)\n",
    "        # compute the reverse KL divergence between p and q with num_samples data points\n",
    "        # Return the estimated reverse KL divergence\n",
    "        \n",
    "        # sample form Gaussian\n",
    "        # compute reverse KL \n",
    "        \n",
    "        # samples = ? # EXERCISE\n",
    "        # rkl = ? # EXERCISE\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        samples = self.gaussian.sample(num_samples)\n",
    "        rkl = torch.mean(self.gaussian.logprob(num_samples) - self.mog.logprob(samples))\n",
    "        #raise NotImplementedError()\n",
    "        return rkl\n",
    "\n",
    "# Optimize the KL by using gradient descent\n",
    "def optimization(kl, forward = False, learning_rate = 0.1, num_epoch = 1000):\n",
    "    parameters = set(kl.parameters())\n",
    "    optimizer = optim.Adam(parameters, lr = learning_rate, eps=1e-3)\n",
    "    \n",
    "    for epoch in range(num_epoch):\n",
    "        optimizer.zero_grad()\n",
    "        if forward:\n",
    "            loss = kl.compute_forwardKL()\n",
    "        else:\n",
    "            loss = kl.compute_reverseKL()\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (epoch % 100) == 0:\n",
    "            print('EPOACH %d: KL: %.4f.'% (epoch+1, loss))\n",
    "\n",
    "print('Optimizing reverse KL')\n",
    "torch.manual_seed(0)\n",
    "kl_reverse = KL_divergence()\n",
    "optimization(kl_reverse, forward = False)\n",
    "Gaussian_reverse= kl_reverse.gaussian\n",
    "\n",
    "print('Optimizing forward KL')\n",
    "kl_forward = KL_divergence()\n",
    "optimization(kl_forward, forward= True)\n",
    "Gaussian_forward = kl_forward.gaussian\n",
    "\n",
    "# Plot the pdf of Gaussian fitted from forward KL and reverse KL, and also the ground truth pdf from MoG\n",
    "x_plot = torch.linspace(-5., 15., 1000)\n",
    "density_mog = torch.exp(kl_forward.mog.logprob(x_plot)).detach().numpy()\n",
    "density_Gaussian_forward = torch.exp(Gaussian_forward.logprob(x_plot)).detach().numpy()\n",
    "density_Gaussian_reverse = torch.exp(Gaussian_reverse.logprob(x_plot)).detach().numpy()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_plot, density_mog)\n",
    "ax.plot(x_plot, density_Gaussian_forward)\n",
    "ax.plot(x_plot, density_Gaussian_reverse)\n",
    "\n",
    "ax.legend(('MoG Density','Forward KL', 'Reverse KL'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e9b26bff6d2b8ec275fbb86082cee644",
     "grade": false,
     "grade_id": "cell-c9f75665b42eb29f",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "# Problem 2: VB for a factor analysis model (1/2)\n",
    "\n",
    "The data set consists of $D$-dimensional vectors $\\mathbf{x}_{n}\\in \\mathbb{R}^{D},$ for $n=1,\\ldots,N$. We model the data using factor analysis with $K$-dimensional factors $\\mathbf{z}_{n}\\in\\mathbb{R}^{K}$. In detail, the\n",
    "model is specified as follows:\n",
    "\\begin{align*}\n",
    "\\mathbf{x}_{n} &  \\sim\\mathcal{N}_{D}(\\mathbf{Wz}_{n},\\text{diag}\n",
    "(\\mathbf{\\psi})^{-1}),\\quad n=1,\\ldots,N,\\\\\n",
    "\\psi_{d} &  \\sim\\text{Gamma}(a,b),\\quad d=1,\\ldots,D,\\\\\n",
    "\\mathbf{w}_{d} &  \\sim\\mathcal{N}_{K}(\\mathbf{0,}\\alpha\\mathbf{I}),\\quad\n",
    "d=1,\\ldots,D,\\\\\n",
    "\\mathbf{z}_{n} &  \\sim\\mathcal{N}_{K}(\\mathbf{0,I}),\\quad n=1,\\ldots,N.\n",
    "\\end{align*}\n",
    "Here, $\\mathbf{W}$ is a $D\\times K$ factor loading matrix and $\\mathbf{w}_{d}$ is the $d$th row of $\\mathbf{W}$ written as a column vector. Parameter $\\psi_{d}^{-1}$ is the variance for the $d$th dimension in the observed data\n",
    "and diag$(\\psi)$ denotes a diagonal matrix with elements $\\mathbf{\\psi} =(\\psi_{1},\\ldots,\\psi_{D})^{T}$ on the diagonal.\n",
    "\n",
    "We approximate the posterior $p(\\mathbf{\\psi},\\mathbf{Z},\\mathbf{W} |\\mathbf{X})$ using the mean-field approximation: \n",
    "\n",
    "$$ \n",
    "q(\\Theta)=\\prod_{d=1}^{D}q(\\mathbf{w}_{d})\\prod_{n=1}^{N}q(\\mathbf{z}_{n})\\prod_{d=1}^{D}q(\\psi_{d}).\n",
    "$$\n",
    "\n",
    "\n",
    "__1__ Write the logarithm of the joint distribution, $\\log p(\\mathbf{\\psi},\\mathbf{Z},\\mathbf{W},\\mathbf{X})$.\n",
    "\n",
    "__2__ Remove from the logarithm of the joint distribution all terms that do not depend on $\\mathbf{z}_{n}$.\n",
    "\n",
    "__3__ Show that the updated factor $q(\\mathbf{z}_{n})$ is equal to\n",
    "\n",
    "$$\n",
    "q(\\mathbf{z}_{n})=\\mathcal{N}_{K}(\\mathbf{\\mu}_{n},\\mathbf{K}_{n}),\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{K}_{n} &  =\\left[  I+\\sum_{d=1}^{D}\\left\\langle \\psi_{d}\\right\\rangle\n",
    "\\left\\langle \\mathbf{w}_{d}\\mathbf{w}_{d}^{T}\\right\\rangle \\right]  ^{-1}\n",
    "\\quad\\text{and}\\\\\n",
    "\\mathbf{\\mu}_{n} &  =\\mathbf{K}_{n}\\left\\langle \\mathbf{W}^{T}\\right\\rangle\n",
    "\\text{diag}(\\left\\langle \\mathbf{\\psi}\\right\\rangle )\\mathbf{x}_{n}.\n",
    "\\end{align*}\n",
    "\n",
    "Here $\\left\\langle \\mathbf{\\cdot}\\right\\rangle $ is used as a shorthand for the expectation of a variable with respect to its factor, e.g., $\\left\\langle \\mathbf{\\psi}\\right\\rangle =\\mathbb{E}_{q(\\mathbf{\\psi})}[\\mathbf{\\psi}]$ etc.\n",
    "\n",
    "__Hint 1:__ Try to write the log joint as \n",
    "\n",
    "$$\n",
    "-\\frac{1}{2}\\mathbf{z}_{n}^{T}\\mathbf{Az}_{n}+\\mathbf{b}^{T}\\mathbf{z}_{n}\n",
    "$$\n",
    "\n",
    "for some $\\mathbf{A}$ and $\\mathbf{b}$, after which you can apply the 'completing the square' technique.\n",
    "\n",
    "__Hint 2:__ Suppose $\\mathbf{A}$ is an $N\\times M$ matrix. Further suppose that $\\mathbf{D}$ is an $N\\times N$ diagonal matrix, $\\mathbf{D} =$diag$(d_{1},\\ldots,d_{N})$. Then $\\mathbf{A}^{T}\\mathbf{DA}$ can be written\n",
    "as\n",
    "\n",
    "$$\n",
    "\\mathbf{A}^{T}\\mathbf{DA=}\\sum_{n=1}^{N}d_{n}\\mathbf{a}_{n}\\mathbf{a}_{n}^{T},\n",
    "$$\n",
    "\n",
    "where $\\mathbf{a}_{n}$ is the $n$th row of $\\mathbf{A}$ written as a column vector.\n",
    "\n",
    "__Hint 3:__ Recall that expectation is a linear operator, i.e. $\\mathbb{E}(aX+bY)=a\\mathbb{E}(X)+b\\mathbb{E}(Y)$. Further, if some random variables $A$ and $B$ are independent, then $\\mathbb{E}_{q(A)q(B)} (AB)=\\mathbb{E}_{q(A)}(A)\\mathbb{E}_{q(B)}(B)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your answer to Problem 2 here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "337e6c54afe83f1330cba6cf779db1aa",
     "grade": false,
     "grade_id": "cell-fd4f81c2e2c0650f",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "# Problem 3: VB for a factor analysis model (2/2)\n",
    "For the factor analysis model considered in Problem 2, derive the update for factor $q(\\mathbf{w}_{d})$. The updated factor should be given in terms of the following expectations: $\\left\\langle \\psi_{d}\\right\\rangle ,\\left\\langle \\mathbf{z}_{n}\\right\\rangle ,\\left\\langle \\mathbf{z}_{n}\\mathbf{z}_{n}^{T}\\right\\rangle $, which have been calculated using the current values of the other factors for all $d,n$.\n",
    "\n",
    "__Hint__: A multivariate Gaussian with a diagonal covariance can be expressed as a product of independent univariate Gaussians, which allows you to simplify the formulas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your answer to Problem 3 here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
