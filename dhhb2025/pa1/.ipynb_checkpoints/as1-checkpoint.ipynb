{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center; font-size: 28px;\">CS-C4100 - Digital Health and Human Behavior (2025)</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = \"\"  # Your Student Id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "<details open>\n",
    "    <summary style=\"font-size: 22px\"><b>‚ö†Ô∏è General Tips</b></summary>\n",
    "\n",
    "* Please review [the general course info](https://mycourses.aalto.fi/course/view.php?id=46377) carefully.\n",
    "* Feel free to create new code cells. This is a good way to write extra tests you may want to run. Note that all cells are executed during evaluation. \n",
    "* **Never** create new cells by menu commands \"Edit/Copy Cells\" and \"Edit/Paste Cells ...\". These commands create cells with duplicate ids and make autograding impossible. Use menu commands \"Insert/Insert Cell ...\" or the button with a plus sign to insert new cells.\n",
    "* When you write code required to solve an assignment, we highly recommend you to insert the code where it says `# YOUR CODE HERE`.\n",
    "* If your notebook is broken (e.g. accidentally removed a hidden tests cell, etc.), you can always re-fetch the assignment. To do so, rename the existing folder and fetch the assignment again.\n",
    "* Do not forget to \"Submit\" your solution in the \"Nbgrader‚ÜíAssignment List\" page after finishing your work. \n",
    "* To better understand how doing assignments in JupyterLab works, please refer to the [documentation](https://scicomp.aalto.fi/aalto/jupyterhub/nbgrader-jupyterlab/).\n",
    "    \n",
    "**Note:** Exercise sessions take place on Mondays 14:15 - 15:45 via [Zoom](https://aalto.zoom.us/j/61175427911) and in-person (Tietotekniikka, C111).\n",
    "\n",
    "</details>\n",
    "</div>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Assignment 1.1: Prerequisites Assessment\n",
    "\n",
    "We will use **Python** as the programming language for all the coding assignments in this course. \n",
    "You can also recap your skills here [Python tutorial](https://docs.python.org/3/tutorial/).\n",
    "> Please note that if you do not have any previous Python experience, then the course may require more than 5 ECTs of workload.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### üìö Learning outcomes\n",
    "Upon completing this assignment, you will be able to: \n",
    "\n",
    "1. Understand the characteristics of digital health data and their applications.\n",
    "2. Apply common analysis tasks for digital health data.\n",
    "3. Become acquainted with the basic usage of all the techniques you will need in future assignments and project work.\n",
    "\n",
    "</div>\n",
    "\n",
    "For those of you already familiar with Python coding in the data science and machine learning field, this assessment will also give you a quick recap. Any detailed concepts and non-trivial techniques will be introduced under each assignment respectively.\n",
    "\n",
    "To complete the assignments, you will need some general knowledge of data science, data visualization and a tiny bit of machine learning.\n",
    "\n",
    "> Don't panic even if you have **absolute zero** or **very limited** experience with the topics mentioned above since the course primarily aims at **undergraduate students**, and it would not be too difficult to solve the tasks.\n",
    "\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "* [Chapter 1: Concept of digital health data](#chapter_1)\n",
    "* [Chapter 2: Data analysis workflow](#chapter_2)\n",
    "* [Chapter 3: Practice: Data analysis workflow ](#chapter_3)\n",
    "    * [Chapter 3.1: Importing basic packages](#chapter_3_1)\n",
    "    * [Chapter 3.2: Concept of data types](#chapter_3_2)\n",
    "    * [Chapter 3.3: Data handling and preprocesing](#chapter_3_3)\n",
    "    * [Chapter 3.4: Plotting techniques (visualization)](#chapter_3_4)\n",
    "* [Chapter 4: Basic Concepts of Machine Learning](#chapter_4)\n",
    "    * [Chapter 4.1: Supervised learning](#chapter_4_1)\n",
    "    * [Chapter 4.2: Unsupervised learning](#chapter_4_2)\n",
    "    * [Chapter 4.3: Natural languages processing (NLP)](#chapter_4_3)\n",
    "    \n",
    "* [References](#references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<hr />\n",
    "\n",
    "<a class=\"anchor\" id=\"chapter_1\"></a>\n",
    "## 1. Concept of digital health data\n",
    "\n",
    "Owing to the rapid adoption of electronic health systems and the ubiquity of mobile and wearable devices, researchers have been blessed with access to a prosperous and non-intrusive personal data source called *digital health data*. According to the World Health Organization (WHO), digital health is an umbrella term that covers multiple aspects, such as mobile health (mHealth), digital health records, personalized medicine, etc [[1]](#references). Through collecting and manipulating personal data from different sources, researchers can extract meaningful behaviour digital features such as the number of step counts per day, sleep duration each night, or the number of outgoing/incoming calls.\n",
    "\n",
    "So, what can we do with those features? Health and behavioural digital features are commonly used in predictive modeling or association discovery. For instance, by monitoring measures related to sleep (total sleep time, time in bed), researchers have found significant positive associations between total sleep time and depression [[2]](#references). In the same manner, the duration of incoming and outgoing calls/day and the number and duration of incoming calls/day have been utilized to predict bipolar disorder episodes [[3]](#references), thus providing an opportunity for relapse intervention. Moreover, natural language processing techniques have been used to infer pieces of evidence of suicide risk from social media posts [[4]](#references). Digital health applications have been continuously gaining traction in the healthcare landscape and playing an increasingly important role in policy-making.\n",
    "In this course, we will mainly deal with behavioural data. Due to its multi-sensor and longitudinal nature, behavioural data possesses the following characteristics:\n",
    "\n",
    "1. **Noise and missingness**. Noise and missingness occur for various reasons: participants drop out of surveys, data entry errors, or even the sudden death of individuals. Dealing with noise and missingness is one of the most important parts of all digital health data projects.\n",
    "\n",
    "2. **Unstructured and fragmented**. As mentioned above, digital health data can be gathered from various sources and thus are highly fragmented. The data often come in unstandardized format (physician notes, blog posts), which makes it difficult to process and digest. Curating and maintaining steady data streams is a challenge in itself.\n",
    "\n",
    "3. **Privacy and security**. Digital health data is extremely personal and sensitive and, thus, must be handled with utmost care. Problems surrounding privacy policies and proper handling of digital health data will be discussed later in the course.\n",
    "\n",
    "Inevitably, proper cleaning and preprocessing of the data already make up much of the allocated time for a digital health project. In this assignment, we will go through all the steps for a typical digital health data analysis workflow, focusing extra on the preprocessing phase. We will also have a chance to practice on a real-life longitudinal dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<hr />\n",
    "\n",
    "<a class=\"anchor\" id=\"chapter_2\"></a>\n",
    "## 2. Data analysis workflow\n",
    "A common data analysis workflow often consists of five stages. We will provide a description for each stage and some corresponding packages that you can use.\n",
    "\n",
    "**1. Loading**\n",
    "\n",
    "In this stage, we simply read/load data from different sources: APIs, CSV files, databases, etc. One of the most popular frameworks for reading and manipulating data is [pandas](https://pandas.pydata.org/). Although there are many ways to gather the data, we will only use pandas to read from CSV files in this course.\n",
    "\n",
    "**2. Preprocessing**\n",
    "\n",
    "Undoubtedly, it is the most time-consuming step in the workflow. There are several things we could do here. We can start by investigating the data types and checking various statistics (min, max, mean, and standard deviation values). Then, we can examine data quality (visualizing missingness, detecting outliers), dropping duplicates, or scaling and standardizing values. The amount of steps can vary significantly based on the data. Once again, [pandas](https://pandas.pydata.org/) provides many helpful functions for those tasks. On top of that, we often use [numpy](https://numpy.org/) for data manipulation. For specific computational problems, such as calculating distances or sampling from different distributions, we can use [scikit-learn](https://scikit-learn.org/stable/) or [scipy](https://www.scipy.org/).\n",
    "\n",
    "**3. Exploratory data analysis (EDA)**\n",
    "\n",
    "An image speaks a thousand words. Data visualization helps us gain insights from our data, discover underlying structures, and reveal relationships between variables. The most common visualization library is [matplotlib.pyplot](https://matplotlib.org/). For more beautiful and advanced plots, refer to [seaborn](https://seaborn.pydata.org/). Finally, if you need interactive plots, use [plotly](https://plotly.com/).\n",
    "\n",
    "**4. Modeling**\n",
    "\n",
    "Modeling refers to applying algorithms to our dataset to achieve a desired output. The output varies based on our needs. We could predict some metrics or find some associations between the variables. We could also discover clusters within our data or interpret the structure of some texts. Regardless of the tasks, you can utilize [scikit-learn](https://scikit-learn.org/stable/) to build the model that suits your needs. To use traditional statistical models, refer to [statsmodel](https://www.statsmodels.org/stable/index.html). For the Natural Language Processing framework, we will discuss it at the end of this assignment. More advanced methods like Deep Learning will not be covered in this course.\n",
    "\n",
    "**5. Evaluation**\n",
    "\n",
    "The final step is interpreting and visualizing the results we just extracted from our model. This step includes choosing the best model (through cross-validation and AICs comparison) or examining the quality of our model (checking goodness-of-fit and reliability of estimates). After we are satisfied with the results, we can make some nice plots to demonstrate them and put them somewhere, on a poster or a paper. We will, however, not go too deep into the details of this step in this assignment.\n",
    "\n",
    "For a nice overview of the workflow, refer to this [link](https://aaltoscicomp.github.io/data-analysis-workflows-course/chapter-1-understanding/).\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "<details open>\n",
    "    <summary style=\"font-size: 20px\"><b>References for Extra Reading</b></summary>\n",
    "    \n",
    "- [15 Python libraries for data science](https://www.dataquest.io/blog/15-python-libraries-for-data-science/)\n",
    "- [Numpy tutorial](https://cs231n.github.io/python-numpy-tutorial/#numpy)\n",
    "- [Pandas userguide](https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html)\n",
    "- [An introduction to machine learning with scikit-learn](https://scikit-learn.org/1.4/tutorial/basic/tutorial.html)\n",
    "\n",
    "</details>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7e76c3af8572509a0eec5c801c8b095d",
     "grade": false,
     "grade_id": "cell-a3083450e35d0e11",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<hr />\n",
    "\n",
    "<a class=\"anchor\" id=\"chapter_3\"></a>\n",
    "## 3. Practice: Data analysis workflow\n",
    "Enough with the theory! In this chapter, we will apply what we have just learnt into practice. We will perform a basic analysis workflow on a real dataset ([Pima Indian diabetes](https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<a class=\"anchor\" id=\"chapter_3_1\"></a>\n",
    "### 3.1. Importing basic packages\n",
    "\n",
    "Run the next cells to install and import baseline packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1e873bfdd03d2fc91d158b08dcdfd4f4",
     "grade": false,
     "grade_id": "cell-1e339d065cef4c95",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np  # Scientific computing\n",
    "import pandas as pd  # Data loading and processing\n",
    "import os  # OS operations\n",
    "import matplotlib.pyplot as plt  # For generating figures\n",
    "import seaborn as sns  # For generating visualizations, better support with pandas than matplotlib\n",
    "import json  # Json package\n",
    "from datetime import datetime  # To handle objects with certian date format\n",
    "from tqdm.notebook import tqdm  # Progress bar\n",
    "\n",
    "if 'AALTO_JUPYTERHUB' in os.environ:\n",
    "    # using jupyter sharedata directory\n",
    "    DATA = '/coursedata/pa1/'\n",
    "else:\n",
    "    DATA = '../../data/pa1/'\n",
    "\n",
    "np.random.seed(123)  # To set a random seed\n",
    "plt.rcParams['font.size'] = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "27e8cdcb3b54e90c3e1feaeafa7e1470",
     "grade": false,
     "grade_id": "cell-7906dd48e98ebdd9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<a class=\"anchor\" id=\"chapter_3_2\"></a>\n",
    "### 3.2. Concepts of Data types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0518295f2730ff155502d19ba105423d",
     "grade": false,
     "grade_id": "cell-f7f0912f23e10a9d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<!-- https://www.linkedin.com/posts/spotlit-ai_introducing-data-science-activity-6811549253794619394-m1eS/ -->\n",
    "To understand the data, knowing the data types you are dealing with is essential. It helps to select the right tools for visualization and further analysis.\n",
    "\n",
    "\n",
    "**Numerical data:**\n",
    "- Discrete data - the data that consists of numerical values that are distinct and separate. In other words, this data type cannot be measured but can be counted. Examples include the number of students in the class, the number of languages an individual speaks, and show sizes.\n",
    "- Continuous data - the data that consists of numerical values that cannot be counted but can be measured. Examples: height, weight. This type of data can be divided into intervals (represent ordered units that have the same difference but do not have an absolute zero, for example, temperature) and ratio data (the same as interval values, with the difference that they do have an absolute zero, for example, height and weight).\n",
    "\n",
    "\n",
    "**Categorical data:**\n",
    "\n",
    "\n",
    "- Nominal data - the data that represents distinct categories with no order. Examples include marital status (married, single, divorced, widowed, and so on) and the languages an individual speaks (English, Finnish, Swedish).\n",
    "- Ordinal data - the data that represents ordered categories. Examples include education (elementary, high school, undergraduate, graduate) and customer satisfaction (satisfied, somewhat satisfied, unsatisfied). The main limitation of this data type is that the distances (differences) between the values are not known.\n",
    "\n",
    "\n",
    "> Extra Material for data types: [link 1](http://www.intellspot.com/data-types/), and [link 2](https://towardsdatascience.com/data-types-in-statistics-347e152e8bee).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "87652dc78f7eb0aa225ba1cd0b82a5aa",
     "grade": false,
     "grade_id": "cell-f4dd90f8fcab3818",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Identifying the data type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e8c673cec9cab24574df5822f11e7e20",
     "grade": false,
     "grade_id": "cell-b0ddde7618f9f4c5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "<p style=\"font-size: 20px\">&#x1F4DD; <b>Question</b></p>\n",
    "\n",
    "\n",
    "Take a look at the list of variables below and try to identify their data types.\n",
    "\n",
    "<ol>\n",
    "    <li>Dogs' breeds</li>\n",
    "    <li>Lengths of the athletes' jumps</li>\n",
    "    <li>Songs' positions in the charts</li>\n",
    "    <li>Number of the times a student missed a class</li>\n",
    "    <li>Nationalities</li>\n",
    "</ol>\n",
    "\n",
    "<p>Answer options:</p>\n",
    "<ol>\n",
    "    <li>Numerical discrete data</li>\n",
    "    <li>Numerical continuous data</li>\n",
    "    <li>Categorical nominal data</li>\n",
    "    <li>Categorical ordinal data</li>\n",
    "</ol>\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "<details>\n",
    "    <summary style=\"font-size: 20px\"><b>Answers</b></summary>\n",
    "    <ul>\n",
    "        <li>Dogs' breeds: 3</li>\n",
    "        <li>Lengths of the athletes' jumps: 2</li>\n",
    "        <li>Songs' positions in the charts: 4</li>\n",
    "        <li>Number of the times a student missed a class: 1</li>\n",
    "        <li>Nationalities: 3</li>\n",
    "    </ul>\n",
    "</details>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "182ca8016b2f223e3bb825f78839042e",
     "grade": false,
     "grade_id": "cell-a5e20a2de992fb07",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<a class=\"anchor\" id=\"chapter_3_3\"></a>\n",
    "### 3.3. Data handling and processing\n",
    "\n",
    "This chapter will teach us to process and manipulate our data with the handy package `pandas`.\n",
    "\n",
    "\n",
    "> Furthermore, there are various data manipulation procedures other than those introduced in this chapter. Check the complete [tutorial](https://scikit-learn.org/stable/modules/preprocessing.html) if you are interested.\n",
    "\n",
    "\n",
    "First, let's look at the description of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "data = pd.read_csv(os.path.join(DATA, 'diabetes_data.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<table>\n",
    "  <tr>\n",
    "    <th>Column</th>\n",
    "    <th>Type</th>\n",
    "    <th>Description</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Pregnancies</td>\n",
    "    <td>Numerical</td>\n",
    "    <td>Number of times pregnant</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Glucose</td>\n",
    "    <td>Numerical</td>\n",
    "    <td>Plasma glucose concentration a 2 hours in an oral glucose tolerance test</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>BloodPressure</td>\n",
    "    <td>Numerical</td>\n",
    "    <td>Diastolic blood pressure (mm Hg)</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>SkinThickness</td>\n",
    "    <td>Numerical</td>\n",
    "    <td>Triceps skin fold thickness (mm)</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>Insulin</td>\n",
    "    <td>Numerical</td>\n",
    "    <td>2-Hour serum insulin (mu U/ml)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>BMI</td>\n",
    "    <td>Numerical</td>\n",
    "    <td>Body mass index (weight in kg/(height in m)^2)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>DiabetesPedigreeFunction</td>\n",
    "    <td>Numerical</td>\n",
    "    <td>A function that determines the risk of type 2 diabetes based on family history. The larger the function, the higher the risk of type 2 diabetes</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Age</td>\n",
    "    <td>Numerical</td>\n",
    "    <td>Age (years)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Outcome</td>\n",
    "    <td>Boolean</td>\n",
    "    <td>Class 1 indicates person having diabetes and 0 indicates other</td>\n",
    "  </tr>  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "In any case, it is always worthwhile to look at the data's statistics before doing any actual plotting. Now, let's look at our data's description by calling `pandas` built-in function `describe()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Notice that there are zero values in some features such as `BMI`, `BloodPressure`, which makes no sense in a medical setting. Looking at data's description helps us detect possible outliers earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b7505e3f4a2244f67a324c90ced46480",
     "grade": false,
     "grade_id": "cell-637f9204ce697518",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### 3.3.1. Detecting and handling duplicate values\n",
    "\n",
    "A real dataset often consists of multiple duplicate values, due to various reasons (technical errors, entry errors). We should always check for duplications from our data and get rid of them.\n",
    "\n",
    "First, we can check for duplications using the `duplicated` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "duplicates = data[data.duplicated()]\n",
    "print(\"Number of duplications:\", len(duplicates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "There are quite a lot of duplications. We can get rid of them by calling the `drop_duplicates` function from `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a16a0f4d3b4bed00e5a18e2079e961a2",
     "grade": false,
     "grade_id": "cell-59e11d105690e3fd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop the duplicate values\n",
    "data = data.drop_duplicates()\n",
    "\n",
    "# Check for the changes\n",
    "data[data.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eeb99b3df96ed3cefa9e93f33d07d1a3",
     "grade": false,
     "grade_id": "cell-4b64094e75646570",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### 3.3.2. Detecting and handling missing values\n",
    "\n",
    "\n",
    "Missing values is one of the most common problems for digital health data. The mechanism for missing data can vary. They could arise from technical issues with the sensors or participants opting out of the study. In general, there are three different Missingness Mechanism:\n",
    "\n",
    "\n",
    "1. **Missing Completely at Random (MCAR)**: The probability that a value is missing does not depend on either the observed or missing values of the response. For example, a participant in the study suddenly moves to a new area and no longer takes part in the survey. Frequently, we *want* the missing data to be MCAR.\n",
    "\n",
    "\n",
    "2. **Missing at Random (MAR)**: When missingness depends only on observed values and not on values that are missing. For example, a participant does not answer some of the survey questions because the question is not written in their first language.\n",
    "\n",
    "\n",
    "3. **Missing Not at Random (MNAR)**: This happens when neither MCAR nor MAR holds. The probability of missingness depends on reasons that have yet to be discovered. Example: Wealthy people often withdraw their income when surveyed. We can only make educated guesses on that phenomenon but cannot confirm it. Therefore, the nature of the missingness in income data is unknown. This is the worst-case scenario because we cannot make an assumption about the missing values.\n",
    "\n",
    "\n",
    "Let us see how many missing values there are in our data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fde04abdf8c6d0064c4484855d7ac86f",
     "grade": false,
     "grade_id": "cell-5a10479837332d89",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check for the missing values\n",
    "data.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "It appears that our data is complete. However, as we have seen from the visualizations in the previous chapter, some of the entries consist of suspicious values, for example, zeroes in BloodPressure, Insulin, and BMI. This is probably due to the encoding process on the original data, where missing values were imputed with zeroes. We can reverse this process and observe the changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "048f7d7d6f74db415016a5367d69631f",
     "grade": false,
     "grade_id": "cell-38b7eb06ffadc994",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assign NaN to the zero values in BP, Insulin and BMI\n",
    "cols = [\"BloodPressure\", \"Insulin\", \"BMI\"]\n",
    "data[cols] = data[cols].replace({'0': np.nan, 0: np.nan})\n",
    "\n",
    "# Check the number of missing values in each column\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Now, it seems like there are way more missing values than we thought. One of the most common strategy in this situation is simply dropping all the records with NaNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e7a3e27c3b6db22a15c1ccc23bf3c0c7",
     "grade": false,
     "grade_id": "cell-18dee2bf9ffa5232",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop records which contain missing values\n",
    "data_dropped = data.dropna()\n",
    "print(\"Number of rows after dropping missing records:\", len(data_dropped))\n",
    "\n",
    "# Assert if there is any Nan in the filtered dataset\n",
    "assert data_dropped.isnull().any().all() == False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Even though we have got rid of all the missing values, the resulting dataset shrinks significantly in size because we have removed 374 records with no `Insulin` information. In this scenario, it is better if we just simply drop the `Insulin` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = data.drop(columns=\"Insulin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "<p style=\"font-size: 20px\"><b>üîç Note</b></p>\n",
    "A common rule of thumb is that you can remove the missing rows if they account for less than 5% the size of your dataset.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "75e3eb32dd32fa6b3631b35488058f4a",
     "grade": false,
     "grade_id": "cell-203edfa97324fe9b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### 3.3.3. Handling missing values for numerical values\n",
    "\n",
    "Although dropping missing values is the most convenient method, it is usually not the most optimal. As we have seen from the above example, the resulting dataset could be extremely small and will result in biased statistical measurements. To this end, we can utilize the Imputer to preserve the sample size. [SimpleImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html) can predict and fill in missing values with some statistics (e.g. mean, median, ...) retrieved from existing data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "47d109e10c0b81bb5c7cc8ce44c39e6b",
     "grade": false,
     "grade_id": "cell-40273edd3e7c9b16",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer  # Load Simple Imputer\n",
    "\n",
    "# SimpleImputer is used to predicting missing values by their context (neighbours)\n",
    "\n",
    "# Define the imputer. We use 'mean' as the imputing strategy.\n",
    "imp_mean = SimpleImputer(missing_values=np.nan, strategy=\"mean\")\n",
    "\n",
    "# Predict column age\n",
    "data[\"BloodPressure\"] = imp_mean.fit_transform(data[[\"BloodPressure\"]])\n",
    "# Predict column salary\n",
    "data[\"BMI\"] = imp_mean.fit_transform(data[[\"BMI\"]])\n",
    "# See the result after imputation\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1bf784206c8a54f541326d43446ac6d0",
     "grade": false,
     "grade_id": "cell-18430332d51aa757",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check the number of missing data\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "272bd13c5abf64cf7e9210e8e3b4351e",
     "grade": false,
     "grade_id": "cell-8ef5c9f98ecd0b03",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### 3.3.4. Feature Scaling (Normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a5f96dfc5fef17ab81a2e8fd0a210f58",
     "grade": false,
     "grade_id": "cell-f5611eef22baadef",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Sometimes, our data might consist of multiple features (also known as *dimensions*), where each feature has its variance. Feature scaling is [necessary](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html) in many situations. For example, specific algorithms such as clustering methods require the data to be normalized before feeding into the model.\n",
    "\n",
    "Among all functions, [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) is the most commonly used one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "33c0160e18e13791995ece8f5b02c0c7",
     "grade": false,
     "grade_id": "cell-0c05e4eb5f7506d6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "data_scaled = data.copy().astype(np.float64)\n",
    "data_scaled.iloc[:, :-2] = sc.fit_transform(data_scaled.iloc[:, :-2])  # Exclude the last 2 columns\n",
    "data_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Wondering how the data looks like after scaling? Let's make a plot for each variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_scaled.iloc[:, :-2].hist(figsize=(12, 12))\n",
    "plt.suptitle(\"Histogram of features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scaled features are now in the same range with mean = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6d05bc14db6750668acfc15336416ad6",
     "grade": false,
     "grade_id": "cell-239bfaba04d9e760",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### 3.3.5. Exercise: Preprocessing data\n",
    "Now, you should write your own data processing scripts based on what you have learned from the chapter so far.\n",
    "\n",
    "The `hepatitis_data.csv` dataset describes basic information of patients and it requires some pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f8b22afd7aefd7232cafbc0bcc192011",
     "grade": false,
     "grade_id": "cell-aac53cf0edde5a13",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "df = pd.read_csv(os.path.join(DATA, \"hepatitis.csv\"))\n",
    "\n",
    "# Inspect the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5c7d768e223f4a61af3f0361f7d51948",
     "grade": false,
     "grade_id": "cell-d74294a117474a2f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Exploring basic details\n",
    "\n",
    "def basic_details(df):\n",
    "    details = pd.DataFrame()\n",
    "    details['Missing values'] = df.isnull().sum()\n",
    "    details['N unique values'] = df.nunique()\n",
    "    details['dtype'] = df.dtypes\n",
    "    return details\n",
    "\n",
    "print(basic_details(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6ccaf9bab89604ed5bc3201829f0ff43",
     "grade": false,
     "grade_id": "cell-048808d42846fab6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Here, in the \"Missing values\" column, you can see that `alk_phosphate`, `albumin` and `protime` columns consist of plenty of missing values. Try to improve the quality of the data using the methods you have learned above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "996748f03639a26b92e2b51431e132a0",
     "grade": false,
     "grade_id": "cell-ec97d506f872634e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "#### üìù Task 1: Handle missing values using `dropna()`\n",
    "\n",
    "You should remove all the missing values from the dataset.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4d0b5ad6dc900e3a71ad798ad3e10aef",
     "grade": false,
     "grade_id": "cell-a55029c1c2d942ba",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_dropped = df.copy()  # Treat df_dropped as original df\n",
    "                        # and perform all the manipulations on df_dropped\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7664ad31b87f982b0ff33d4e3f54a4f8",
     "grade": true,
     "grade_id": "task1-dropna",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hidden tests\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e3a8d3f38821b1d760ba703aff300d34",
     "grade": false,
     "grade_id": "cell-d6fd16d8ddcdce0d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "#### üìù Task 2: Handle missing values using `SimpleImputer`\n",
    "Instead of removing the missing values, impute the following columns with their mean values: `alk_phosphate`, `albumin`, `protime`, `bilirubin`.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "78a3dc216a763ee3d0b7e8f8ad91877f",
     "grade": false,
     "grade_id": "cell-957e6c6bfe3f5059",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "df_imputed = df.copy()  # Treat df_imputed as the original df and perform all the manipulations on df_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9fd7acf68bb2c8a8729b77af694b7d2d",
     "grade": false,
     "grade_id": "cell-4a0d2dbd231dcc0d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Impute the following columns with their mean values: alk_phosphate, albumin, protime, bilirubin\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bcf4349973cfcb11e6a33932ea8d12d5",
     "grade": true,
     "grade_id": "task2-imputer",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hidden tests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "178f1803e94cb4766617c525762561ac",
     "grade": false,
     "grade_id": "cell-2367133ecd50f046",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "#### üìù Task 3: Scale data using `StandardScaler`\n",
    "\n",
    "Scale the following columns with Standard scaler: `alk_phosphate` and `albumin`.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c48733956b7adf8f3f2a8bd9432091d2",
     "grade": false,
     "grade_id": "cell-538854d5127bd070",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the scaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "df_scale = df.copy()  # Treat df_scale as the original df and perform scaling on df_scale\n",
    "df_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "242bdfe35da7024e4a1c0214ecbbedd2",
     "grade": false,
     "grade_id": "cell-4827bd8bad6265ce",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dd9497ede2b4d329215ebeea5f991531",
     "grade": true,
     "grade_id": "task3-scaler",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hidden tests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "94ab1dfa4ef0412305d7f3b751d9e413",
     "grade": false,
     "grade_id": "cell-e30dd7c9b075c9c1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<a class=\"anchor\" id=\"chapter_3_4\"></a>\n",
    "### 3.4. Plottings Techniques (Visualization)\n",
    "In this chapter, you can view snippets and practice various plotting functions with seaborn and matplotlib."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7564a3cfa4673a246d2224e7ddf053b3",
     "grade": false,
     "grade_id": "cell-db17afdc5b3b7203",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### 3.4.1. Univariate plot\n",
    "\n",
    "Univariate plot is used to visualize the distribution of one variable. Probably the most commonly used univariate plot is histogram. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize a figure\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "title = fig.suptitle(\"Histogram of Blood Pressure\")\n",
    "fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "\n",
    "# Make a scatter plot with matplotlib\n",
    "ax1.hist(data['BloodPressure'])\n",
    "ax1.set_xlabel(\"value\")\n",
    "ax1.set_ylabel(\"frequency\")\n",
    "ax1.set_title(\"matplotlib\")\n",
    "\n",
    "# Make a scatter plot with seaborn\n",
    "sns.histplot(data[\"BloodPressure\"], ax=ax2)\n",
    "ax2.set(xlabel=\"value\", ylabel=\"frequency\", title=\"seaborn\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "You can easily see that there are quite a few outliers values in the data (Blood pressure = 0). Consequently, we will need to get rid of these data in the preprocessing step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b628549113ff96af9a48b90846281975",
     "grade": false,
     "grade_id": "cell-2b4d8a858972b53e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### 3.4.2. Multivariate plot\n",
    "\n",
    "A multivariate plot shows the relationship between several variables. Most of the times, we are interested in plots that show the relationship between two variables. We call them bivariate plots.\n",
    "\n",
    "In the following plots, we showcase some insightful plots, such as lineplot, correlation heatmap, and scatter plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "##### Scatter plot\n",
    "First, let's look at scatter plot. We often use this kind of plot to reveal the relationship between 2 variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize a figure\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "title = fig.suptitle(\"Blood pressure versus Age\")\n",
    "fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "\n",
    "# Make a scatter plot with matplotlib\n",
    "ax1.scatter(data[\"BloodPressure\"], data[\"Age\"])\n",
    "ax1.set_xlabel(\"Blood Pressure (mm Hg)\")\n",
    "ax1.set_ylabel(\"Age\")\n",
    "ax1.set_title(\"matplotlib\")\n",
    "\n",
    "# Make a scatter plot with seaborn\n",
    "sns.scatterplot(x=data[\"BloodPressure\"], y=data[\"Age\"], ax=ax2)\n",
    "ax2.set_title(\"seaborn\")\n",
    "ax2.set_xlabel(\"Blood Pressure (mm Hg)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also combine the histogram and scatter plots into one, by calling `seaborn`'s `pairplot` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.pairplot(data, diag_kind=\"hist\", hue=\"Outcome\")\n",
    "plt.suptitle(\"Pairwise relationships plot\", y=1.03)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8abaa19d5588d850c5b790f964fa0226",
     "grade": false,
     "grade_id": "cell-a0cd6ac8108d673f",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "#### üìù Task 4\n",
    "\n",
    "Do you notice any patterns or abnormality from the above plot?\n",
    "\n",
    "Write your answer in the provided cell below (And please don't say no!):\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8e6eb52e06f06b4850b96b37818a4c71",
     "grade": true,
     "grade_id": "task4-patterns",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "##### Boxplot/Violin plot\n",
    "When there are grouping factors within our data (like gender and origin), boxplot often helps reveal the trend within each group.\n",
    "\n",
    "Now, since there is no grouping factor in our current data, we will come up with an arbitrary one. Let's divide people into different age groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Young: age < 32\n",
    "# Middle age: age >= 32 and age < 50\n",
    "# Old: age >= 50\n",
    "data['AgeGroup'] = data['Age'].apply(\n",
    "    lambda x: 'Young' if x < 32 else ('Middle age' if 32 <= x < 50 else 'Old')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Alright, let's make a boxplot. This time we will use `pandas` built-in function `boxplot` for a change. In fact, `pandas` provides many useful built-in plotting functions that you can explore [here](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize a figure\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "\n",
    "# Make a scatter plot with pandas\n",
    "data.boxplot(column=['BloodPressure'], by=[\"AgeGroup\"], ax=ax1)\n",
    "ax1.set_ylabel(\"Blood Pressure (mm Hg)\")\n",
    "ax1.set_xlabel(\"Age group\")\n",
    "ax1.set_title(\"pandas\")\n",
    "\n",
    "# Make a box plot with seaborn\n",
    "sns.boxplot(x=data['AgeGroup'], y=data[\"BloodPressure\"], ax=ax2)\n",
    "ax2.set_xlabel(\"Age group\")\n",
    "ax2.set_ylabel(\"Blood Pressure (mm Hg)\")\n",
    "ax2.set_title(\"seaborn\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "<p style=\"font-size: 20px\"><b>üîç Note</b></p>\n",
    "Notice the patterns, quantiles, and outliers among different groups. Are they <i>significantly</i> different?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**Violin plot** is another way to visualize group difference. Violin plot is more informative than a plain boxplot, since it shows the full distribution of the data as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make a violin plot with seaborn\n",
    "fig, ax = plt.subplots(1, 1, figsize = (12,6))\n",
    "fig.suptitle(\"Blood pressure distribution among age groups\")\n",
    "sns.violinplot(x=data['AgeGroup'], y=data['BloodPressure'], ax=ax)\n",
    "ax.set_xlabel(\"Age group\")\n",
    "ax.set_ylabel(\"Blood Pressure (mm Hg)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "##### Correlation heatmap\n",
    "\n",
    "Correlation is a statistical measure to describe the extent in which two variables are linearly related. Correlation can range from -1 to 1.  We use correlation heatmap to show the strength of correlation between different variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "numerical_cols = data.columns[:-2]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 14))  # Sample figsize in inches\n",
    "sns.heatmap(data[numerical_cols].corr(), annot=True, linewidths=.5, ax=ax)\n",
    "ax.set_title(\"Correlation heatmap of features\", y=1.05, fontsize=32)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4229e03901f84438d14f77cee5f55b38",
     "grade": false,
     "grade_id": "cell-8bbdc603c7abc4a3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### 3.4.3. Practice: Make some plots by yourself\n",
    "\n",
    "Now, it's time for you to make some plots for a real dataset. You should use the `seaborn` package for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d61d1f72422fbc3bfccfc11f98fa217c",
     "grade": false,
     "grade_id": "cell-afbabd2f3ef024bf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load sample dataset\n",
    "df = pd.read_csv(os.path.join(DATA, '1503960366_dailyCaloriesAndSteps.csv'), sep=\";\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a62da90f2eb27f7857bcbfd5a5f5fedd",
     "grade": false,
     "grade_id": "cell-ccd5bb9128f231c2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "#### üìù Task 5: Make a plot for `ActivityDay` and `Calories`\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "<details>\n",
    "    <summary style=\"font-size: 20px\"><b>üí° Hint</b></summary>\n",
    "\n",
    "For timeseries data, [line plot](https://seaborn.pydata.org/generated/seaborn.lineplot.html) is the most informative method for visualization.\n",
    "\n",
    "\n",
    "</details>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7858a4bd595682bbbbc60039117c28c4",
     "grade": true,
     "grade_id": "task5-lineplot1",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calories line plot\n",
    "\n",
    "plt.figure(figsize=(16, 4))\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "plt.title(\"Daily Calories Burned\", pad=20, fontsize=24)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "#### üìù Task 6: Similarly, make a plot for `ActivityDay` and `TotalSteps`\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8aa28d0ebf865f435a367f357134bdb3",
     "grade": true,
     "grade_id": "task6-lineplot2",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TotalSteps line plot\n",
    "\n",
    "plt.figure(figsize=(16, 4))\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "plt.title(\"Daily Activity Intensity\", pad=20, fontsize=24)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "<p style=\"font-size: 20px\"><b>üîç Note</b></p>\n",
    "Can you spot any recurring patterns in this plot?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "536b4c024caf10dbf0f9a030429da1c0",
     "grade": false,
     "grade_id": "cell-67ec6c7ccddc1846",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "#### üìù Task 7: Make histograms of `Calories` and `TotalSteps`\n",
    "Comply with the following criteria:\n",
    "- set parameter `kde=True` to see the kernel density estimate\n",
    "- set parameters `height=6` and `aspect=2` to fix the size of the plot\n",
    "\n",
    "\n",
    "The [sns.displot](https://seaborn.pydata.org/generated/seaborn.displot.html) is a figure-level interface for drawing distribution plots onto a FacetGrid. It provides you with more control over the plot and does not fully comply with the metadata defined by `matplotlib`\n",
    "\n",
    "\n",
    "For instance, if you want to adjust the size of the `FacetGrid`, you should define `height` and `aspect` in the `sns.displot` function rather than using the `figsize=` definition in the `plt.figure()` function.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d99494a9fb28d8cd74d95f5fa3996a9e",
     "grade": true,
     "grade_id": "task7a-histogram1",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Part 1 - Calories histogram\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "plt.title(\"Histogram of Calories burned\")\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b48e10848df99f225861b214037c90c8",
     "grade": true,
     "grade_id": "task7b-histogram2",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Part 2 - TotalSteps histogram\n",
    "\n",
    "# Set the same parameters as those in the above section, but using TotalSteps column\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "plt.title(\"Histogram of Total Steps\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "25422ad9f51c3fc67da93e17e24550bb",
     "grade": false,
     "grade_id": "cell-6dfdfbe5f4181067",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "#### üìù Task 8: Make a scatter plot of `Calories` AND `TotalSteps`\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "145958d06bb3dedb5caf14bd01172169",
     "grade": true,
     "grade_id": "task8-scatterplot",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Scatter plot\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Saving the title text for later\n",
    "title_text = plt.gca().title.get_text()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<p style=\"font-size: 20px\"><b>üîç Note</b></p>\n",
    "The following cell serves as a reminder to <b>always</b> include a title in your plots.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Sanity Checks. Do not modify.\n",
    "\n",
    "assert len(title_text) > 0, \"You shouldn't leave the title empty\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b0e1f1d89654f747c9a36f9877e87d66",
     "grade": false,
     "grade_id": "cell-438e0a27d31efb1f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<hr>\n",
    "\n",
    "<a class=\"anchor\" id=\"chapter_4\"></a>\n",
    "## 4. Basic Concepts of Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c22715a5315c2ee852220fdcbc77ef36",
     "grade": false,
     "grade_id": "cell-1dcad090a889177b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "This chapter introduces various techniques like clustering, linear regression and classification with several hands-on coding sections.\n",
    "\n",
    "\n",
    "> Again, do not panic if you do not have any experience with machine learning! In future assignments, we will only cover a **tiny bit** of it with good explanations.\n",
    "\n",
    "\n",
    "Before going into the concepts, we define a few technical terms that will be used repeatedly below.\n",
    "\n",
    "\n",
    "1. **Dependent variable (DV)**: the effect that we want to measure. Some kinds of literature refer to them as the *outcome* or the *response variable*.\n",
    "\n",
    "\n",
    "2. **Independent variable(s) (IVs)**: the factors we believe to affect the dependent variable. They are often called *explanatory variables* or *predictor variables*.\n",
    "\n",
    "\n",
    "3. **Statistical model**: a mathematical method to draw inferences between variables. In other words, a model is a simpler and more convenient way to simulate a real-world phenomenon.\n",
    "\n",
    "\n",
    "We often define a model as follows:\n",
    "\n",
    "\n",
    "$$DV \\sim \\alpha + IV_1 + IV_2 + ... + IV_n$$\n",
    "\n",
    "\n",
    "where $\\alpha$ is an independent term (we call it the *intercept*).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b80232e1b9912f7c1b278806cc49b5c7",
     "grade": false,
     "grade_id": "cell-5f6152fd968bd54e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<a class=\"anchor\" id=\"chapter_4_1\"></a>\n",
    "### 4.1. Supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "48888b0d5ce4f75ef9a8eafafe9b61be",
     "grade": false,
     "grade_id": "cell-f4136f94d1b3efbb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "[Supervised learning](https://en.wikipedia.org/wiki/Supervised_learning) is a group of learning algorithms that are trained using labeled data. The supervised learning algorithm learns from this labeled data through an iterative process, allowing it to predict new unseen data. Supervised learning is further divided into Regression and Classification algorithms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "525fc49c2eaca8051e5970421cd3958b",
     "grade": false,
     "grade_id": "cell-d379ddd8305c6c4d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### 4.1.1. Linear Regression\n",
    "\n",
    "\n",
    "Regression analysis is a statistical methodology that estimates a relationship between a dependent variable and a set of independent variables. Linear regression is the most basic form of regression analysis and is used extensively in digital health data analysis [[5, 6]](#references). The relationship between the variables is assumed to be linear in linear regression. Typically, we can achieve two tasks by using linear regression:\n",
    "\n",
    "\n",
    "1. Predicting the value of a variable based on the value of other variables.\n",
    "\n",
    "\n",
    "2. Yielding the estimates between a dependent variable and a set of independent variables\n",
    "\n",
    "\n",
    "There are a few articles ([here](https://towardsdatascience.com/linear-regression-explained-1b36f97b7572) and [here](https://medium.com/analytics-vidhya/understanding-the-linear-regression-808c1f6941c0)) that explain the concept of linear regression in details.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9658a562387a86e361a8326c4a62e139",
     "grade": false,
     "grade_id": "cell-fd3f89603095dff8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**Splitting the dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "73a439020f8bb40186d7d6132484392f",
     "grade": false,
     "grade_id": "cell-21a21b96ea9a7ed0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Typically, we would like to split the dataset into two subsets: one for training the model and one for testing. We can easily achieve this by using the [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function from the [scikit-learn](https://scikit-learn.org/stable/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Splitting the dataset into train and test sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Independent variables: we will use a few variables to predict the outcome\n",
    "iv_cols = [\"Pregnancies\", \"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Age\", \"DiabetesPedigreeFunction\"]\n",
    "X = data_scaled[iv_cols]\n",
    "y = data_scaled[\"Outcome\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
    "\n",
    "print(\"Train sample size:\", len(X_train))\n",
    "print(\"Test sample size:\", len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "We will use the [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) model from scikit-learn for a simple prediction task. Now, we formally define our model as follows:\n",
    "\n",
    "$$Outcome \\sim Pregnancies + Glucose + BloodPressure + SkinThickness + Age + DiabetesPedigreeFunction$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d287547053e0a074fefa1e5efc8e70b2",
     "grade": false,
     "grade_id": "cell-2d14e72e73edcc93",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing LinearRegression model from the sklearn library\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression().fit(X_train, y_train)  # Training Linear Regression model\n",
    "\n",
    "print(\"[1] R-squared score: \", lin_reg.score(X_train, y_train))  # Return the mean accuracy on the given test data and labels\n",
    "print(\"[2] Calculated coefficients: \", lin_reg.coef_)  # Calculated coefficients for the linear regression problem\n",
    "print(\"[3] Independent term: \", lin_reg.intercept_)  # Independent term in the linear model\n",
    "\n",
    "print(\"[4] Predicting the first 10 datapoints from test set: \", lin_reg.predict(X_test.head(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Let us iterate through our results:\n",
    "\n",
    "\n",
    "[1] The R-squared score shows how well our model captures the variance of the dependent variable (the higher, the better, max value is 1). A value of 0.28 is relatively low, so our model needs to explain the variance better.\n",
    "\n",
    "\n",
    "[2] Coefficients show the strength and direction of the relationship between the dependent and independent variables. You can see that `Glucose` has the highest coefficient values (0.209).\n",
    "\n",
    "\n",
    "[3] Independent term is the value of the intercept. Combining this with the coefficients above, we can yield the following estimate:\n",
    "\n",
    "\n",
    "$$Outcome = 0.355 + 0.088*Pregnancies + 0.201*Glucose + 0.012*BloodPressure  \\\\\n",
    "+ 0.026*SkinThickness + 0.005*Age + 0.055*DiabetesPedigreeFunction$$\n",
    "\n",
    "\n",
    "[4] Finally, we can predict the test set. There are various ways to validate this prediction result, but we will only delve into a few details in this assignment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**What if we did not normalize the data?**\n",
    "\n",
    "Now you will see why standardization/normalization is important. We will fit the same model again, but with the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_orig = data[iv_cols]\n",
    "y_orig = data[\"Outcome\"]\n",
    "X_orig_train, X_orig_test, y_orig_train, y_orig_test = train_test_split(X_orig, y_orig, test_size=0.2, random_state=2)\n",
    "\n",
    "lin_reg = LinearRegression().fit(X_orig_train, y_orig_train)  # Train the Linear Regression model\n",
    "\n",
    "print(\"[1] R-squared score:\", lin_reg.score(X_orig_train, y_orig_train))  # Return the mean accuracy on the given test data and labels\n",
    "print(\"[2] Calculated coefficients:\", lin_reg.coef_)  # Calculate coefficients for the linear regression problem\n",
    "print(\"[3] Independent term:\", lin_reg.intercept_)  # Independent term in the linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "See how the estimates have changed? The effect of `Glucose` is *underestimated* while `DiabetesPedigreeFunction` is *overestimated* because they are not in the same value range. The estimates are biased in this scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "49a448fda5b2a7511a00c88e43a9a9fc",
     "grade": false,
     "grade_id": "cell-e8531ede4c01bf3b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### 4.1.2. Logistic regression\n",
    "\n",
    "[Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) is another regression analysis method. Logistic model can predict the probability that an outcome belongs to a class.\n",
    "\n",
    "It's highly recommended to read [this article](https://www.analyticsvidhya.com/blog/2021/04/beginners-guide-to-logistic-regression-using-python/) to learn more about regression.\n",
    "\n",
    "> You **don't need to** understand any mathematical parts, and having a general idea about what regression does is adequate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "756a9d54b756b8015799636646294674",
     "grade": false,
     "grade_id": "cell-4b4c777e5684f669",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import LogisticRegression model from the sklearn library\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define the logistic regression model\n",
    "log_reg = LogisticRegression(random_state=0)\n",
    "\n",
    "# Train Logistic Regression model\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Validate model accuracy with test samples\n",
    "log_reg.score(X_test, y_test)\n",
    "\n",
    "print(\"Probability estimates:\\n\", log_reg.predict_proba(X_test[:10]))  # Probability estimates\n",
    "print(\"Accuracy:\", log_reg.score(X_test, y_test))  # Return the mean accuracy on the given test data and labels\n",
    "print(\"Predicting the label for the unseen datapoint:\", log_reg.predict(X_test[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "As you can see from above, Logistic regression yields the probability that a sample belongs to a group. Logistic regression is instrumental when the dependent variable has more than two classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "82e864a25c310cc6fc6f0601dc0a1ca1",
     "grade": false,
     "grade_id": "cell-e284e90d9e4518e1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<a class=\"anchor\" id=\"chapter_4_2\"></a>\n",
    "### 4.2. Unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "be384275a3dafb4ab46e4c0bcb72ada1",
     "grade": false,
     "grade_id": "cell-24a509f983bd5183",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<p><b><a href=\"https://en.wikipedia.org/wiki/Unsupervised_learning\">Unsupervised learning </a></b> - group of learning algorithms that are fed with unlabeled data. It is up to the algorithm to find hidden structures, patterns, or relationships within the data.\n",
    "\n",
    "Clustering algorithms and Dimensionality reduction are two commonly used methods for unsupervised machine learning tasks.</p>\n",
    "<h4>Examples</h4>\n",
    "<b><a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\"> K-means clustering algorithm:</a></b>\n",
    "\n",
    "> Read more about clustering <a href=\"https://www.geeksforgeeks.org/clustering-in-machine-learning/\">here</a> and <a href=\"https://www.analyticsvidhya.com/blog/2019/08/comprehensive-guide-k-means-clustering/\">here</a>, and about K-means clustering <a href=\"https://towardsdatascience.com/understanding-k-means-clustering-in-machine-learning-6a6e67336aa1\">here</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "abfb706c2885ecb07208e9ad2c41e2da",
     "grade": false,
     "grade_id": "cell-573bf4e613f29941",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans  # Import KMeans model from the sklearn library\n",
    "\n",
    "df_0 = X[[\"BloodPressure\", \"Age\"]].copy()\n",
    "df_0[\"pred_label\"] = KMeans(n_clusters=2, random_state=0, n_init=\"auto\") \\\n",
    "    .fit_predict(df_0) \n",
    "\n",
    "sns.scatterplot(x=\"Age\", y=\"BloodPressure\", hue=\"pred_label\", data=df_0)\n",
    "plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The K-means algorithm attempts to separate the data points into clusters. As you can observe from the above plot, a cut-off point between the two clusters indicates that the `BloodPressure` level starts to change once you age. It is hard to conclude anything from the plot alone, but you have a general idea of how clustering works. Feel free to tinker with the `n_clusters` variable if you need more clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2ed5f448056e0b3afc44ccad30adfc7d",
     "grade": false,
     "grade_id": "cell-3c17e7fbccbace1d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<a class=\"anchor\" id=\"chapter_4_3\"></a>\n",
    "### 4.3. Natural Language Processing (NLP) Techniques\n",
    "We introduce some NLP techniques here, which will be further used in assignment 5. Don't worry; it's not hard if you are willing to spend an hour reading the concepts, and we will **NOT** use extra techniques other than those covered here.\n",
    "\n",
    "Some terms you need to understand [[7]](#references):\n",
    "- Corpus: Body of text, singular. Corpora is its plural.\n",
    "- Lexicon: Words and their meanings.\n",
    "- Token: Each ‚Äúentity‚Äù that is a part of whatever was split up based on rules. For example, each word is a token when a sentence is \"tokenized\" into words. Each sentence can also be a token if you tokenize the sentences from a paragraph. So basically, tokenizing involves splitting sentences and words from the body of the text.\n",
    "\n",
    "Below, we present a simple and commonly used template. We use the Python `NLP` module. The NLTK module is a massive tool kit that aims to help you with the entire NLP methodology.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2bb67a5be9dd12f6dca805f9ec13249c",
     "grade": false,
     "grade_id": "cell-c871d97a21ba9b03",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize  # Import String tokenizer\n",
    "\n",
    "import ssl\n",
    "\n",
    "# Bypass ssl to enable downloading punkt package\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download(\"punkt\", quiet=True)  # Download the punkt package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = \"How do we improve health and well-being using digital data? Is it a kind of magic?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokens of each complete sentence\n",
    "sent_token = sent_tokenize(text)\n",
    "\n",
    "sent_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2d2c364c862c89f417af773561163810",
     "grade": false,
     "grade_id": "cell-5a03bac02c16372d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokens of each word repectively in a list\n",
    "word_token = word_tokenize(text)\n",
    "\n",
    "word_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "43941da413b63a88f4ed07f9588be670",
     "grade": false,
     "grade_id": "cell-baff02c1012cd163",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer  # Word stem extractor\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# Extract the stems of each word token\n",
    "word_stem = [ps.stem(w) for w in word_tokenize(text)]\n",
    "\n",
    "print(word_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1cfd2f051564f31769d095f3576cbb38",
     "grade": false,
     "grade_id": "cell-68ced67f16a49d53",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compare the word stem and the word token\n",
    "np.column_stack([word_stem, word_token])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 4.4. Extra reading: Statistical test\n",
    "\n",
    "\n",
    "Statistical tests are often conducted in behavioural studies to measure the difference in some variables between multiple groups. Before going into the details of statistical testing, we first need to understand the concepts of the *null hypothesis* and *alternate hypothesis*.\n",
    "\n",
    "\n",
    "A *null hypothesis* ($H_0$) proposes that no significant difference exists in a set of observations. The *alternate hypothesis* ($H_a$), on the other hand, is the direct opposite of the *null hypothesis*.\n",
    "\n",
    "\n",
    "**Why does this matter?**\n",
    "\n",
    "\n",
    "In almost every research problem, we often develop multiple hypotheses and work to disprove/approve them. For example, to test the effect of a new drug, the gold standard in clinical practice is randomized control trials (RCT). In RCT, the participants are divided into treatment and control groups. At the end of the experiment, the researchers compare an outcome between the two groups, e.g., the cholesterol level. The hypotheses in this case are then defined as:\n",
    "\n",
    "\n",
    "$H_0$: There is no significant difference in the cholesterol level between the two groups\n",
    "\n",
    "\n",
    "$H_a$: There is a significant difference in the cholesterol level\n",
    "\n",
    "\n",
    "Of course, we would want to *reject* the null hypothesis and confidently say there is a significant difference between the two treatments. This is where the statistical tests come into play. A statistical test describes how much the relationship between variables differs from the null hypothesis of no relationship. The test yields a statistic that we can use to compare against a critical value and decide to reject/accept the null hypothesis.\n",
    "\n",
    "\n",
    "Choosing a suitable statistical test is often a confusing topic that needs to be clarified. Statistical tests are divided into two categories: parametric and non-parametric. In parametric tests, certain assumptions must hold, e.g., Independence of observations, Homogeneity of variance, and Normality of data. On the other hand, nonparametric tests do not require the above assumptions, but they often yield 'weaker' inference power than parametric tests.\n",
    "\n",
    "\n",
    "Here, we will briefly explain two statistical tests: the t-test (parametric) and the permutation test (non-parametric). You can follow [this link](https://www.scribbr.com/statistics/statistical-tests/) for a more in-depth explanation of statistical testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**Independent t-test**\n",
    "\n",
    "We use an independent t-test to compare the means of a continuous outcome between two separate groups of individuals (male/female, treatment/control)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "stats.ttest_ind(data_scaled[data_scaled.Outcome == 0][\"Glucose\"], data_scaled[data_scaled.Outcome == 1][\"Glucose\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Above, we just compare the difference in Glucose levels between those who have diabetes and those who do not. You can see that the p-value is extremely low, so we can reject the null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**Permutation test**\n",
    "\n",
    "\n",
    "Permutation tests follow a three-step procedure:\n",
    "\n",
    "\n",
    "1. Compute a test statistic for the original data\n",
    "\n",
    "\n",
    "2. Resample the data *n* times and yield *n* test statistics to create a sampling distribution of the test statistics.\n",
    "\n",
    "\n",
    "3. Compute the p-value as the percentage of test statistics as extreme or more extreme than initially observed.\n",
    "\n",
    "\n",
    "The permutation test is practical when the data generation mechanism is unknown, i.e., we assume we do not know the underlying distribution. A nice visual explanation can be found [here](https://www.jwilber.me/permutationtest/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<hr>\n",
    "\n",
    "<a class=\"anchor\" id=\"references\"></a>\n",
    "## References\n",
    "\n",
    "[[1]](https://www.who.int/europe/health-topics/digital-health) Digital health | World Health Organization. Retrieved 10 Oct 2024.\n",
    "\n",
    "[[2]](https://www.frontiersin.org/articles/10.3389/fpsyt.2021.625247/full) Moshe, I., Terhorst, Y., Opoku Asare, K., Sander, L., Ferreira, D., & Baumeister, H. et al. (2021). Predicting Symptoms of Depression and Anxiety Using Smartphone and Wearable Data. Frontiers In Psychiatry, 12. doi: 10.3389/fpsyt.2021.625247\n",
    "\n",
    "[[3]](https://doi.org/10.1111/bdi.12332) Faurholt-Jepsen, M., Vinberg, M., Frost, M., Christensen, E., Bardram, J., & Kessing, L. (2015). Smartphone data as an electronic biomarker of illness activity in bipolar disorder. Bipolar Disorders, 17(7), 715-728. doi: 10.1111/bdi.12332\n",
    "\n",
    "[[4]](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5659860/) De Choudhury, M., Kiciman, E., Dredze, M., Coppersmith, G., & Kumar, M. (2016). Discovering Shifts to Suicidal Ideation from Mental Health Content in Social Media. Proceedings Of The 2016 CHI Conference On Human Factors In Computing Systems. doi: 10.1145/2858036.2858207\n",
    "\n",
    "[[5]](https://mhealth.jmir.org/2019/2/e11638/) Do, D., Garfein, R., Cuevas-Mota, J., Collins, K., & Liu, L. (2019). Change in Patient Comfort Using Mobile Phones Following the Use of an App to Monitor Tuberculosis Treatment Adherence: Longitudinal Study. JMIR Mhealth And Uhealth, 7(2), e11638. doi: 10.2196/11638\n",
    "\n",
    "[[6]](https://link.springer.com/chapter/10.1007/978-3-030-61527-7_43) Unnikrishnan, V., Shah, Y., Schleicher, M., Strandzheva, M., Dimitrov, P., & Velikova, D. et al. (2020). Predicting the Health Condition of mHealth App Users with Large Differences in the Number of Recorded Observations - Where to Learn from?. Discovery Science, 659-673. doi: 10.1007/978-3-030-61527-7_43\n",
    "\n",
    "[[7]](https://www.geeksforgeeks.org/tokenize-text-using-nltk-python/) Tokenize text using NLTK in python | GeeksforGeeks. Retrieved 10 Oct 2024."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ea359033228c69afee6d1dfe15a26f431abadaa8efaae4b6f1ea73cb12fd6c3b"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
