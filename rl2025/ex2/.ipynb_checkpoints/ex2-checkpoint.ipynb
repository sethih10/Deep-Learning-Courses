{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53274fb5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "573807b56759d45a7a58b41f2a2ddf8a",
     "grade": false,
     "grade_id": "cell-18e2f9f50943506e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "   <h2 align=\"center\"> <center><b> Reinforcement Learning Assignment 2 - Value Iteration </b></center></h2>\n",
    "\n",
    "<center><font size=\"3\">This notebook is a part of teaching material for ELEC-E8125</font></center>\n",
    "<center><font size=\"3\">Aalto University</font></center>\n",
    "</div>\n",
    "\n",
    "\n",
    "<a id='TOC'></a>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "# Table of contents\n",
    "* <a href='#1.'> 1. Introduction </a>\n",
    "* <a href='#1.1'> 1.1 Learning Objectives </a>\n",
    "* <a href='#1.2'> 1.2 Code Structure & Files </a>\n",
    "* <a href='#2.'> 2. Sailor Gridworld </a>\n",
    "* <a href='#3.'> 3. Value Iteration </a>\n",
    "* <a href='#4.'> 4. Submitting </a>\n",
    "* <a href='#4.1'> 4.1 Feedback </a>\n",
    "* <a href='#5.'> References</a>\n",
    "\n",
    "\n",
    "<a href='#T1'><b>Student Task 1.</b> Implementing Value Iteration (30 points) </a>\\\n",
    "<a href='#Q1'><b>Student Question 1.1</b> Reinforcement Learning Components (5 points) </a>\\\n",
    "<a href='#Q2'><b>Student Question 1.2</b> Value Analysis (5 points) </a>\\\n",
    "<a href='#Q3'><b>Student Question 1.3</b> Investigating Optimal Path (10 points) </a>\\\n",
    "<a href='#Q4'><b>Student Question 1.4</b> Investigating Convergence Properties (10 points) </a>\\\n",
    "<a href='#T2'><b>Student Task 2.</b> Number of Iterations Until Convergence (10 points) </a>\\\n",
    "<a href='#T3'><b>Student Task 3.</b> Evaluating the Policy (10 points) </a>\\\n",
    "<a href='#Q5'><b>Student Question 3.1</b> Relationship Between Discounted Return and the Value Function (10 points)</a>\\\n",
    "<a href='#Q6'><b>Student Question 3.2</b> Considering Unknown Environments (10 points) </a>\n",
    "\n",
    "    \n",
    "**Total Points:** 100\n",
    "\n",
    "**Estimated runtime of all the cells:** 20 minutes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4f0b31",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a349b2c40fd5cb89280a1360caad5446",
     "grade": false,
     "grade_id": "cell-7c2df844d5c3f2e1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "# 1. Introduction <a id='1.'></a>\n",
    "\n",
    "In this exercise we look at how a simple method like value iteration can be used to find an optimal behaviour for a sailing gridworld environment. We further investigate the properties of value iteration and how environment definition can affect the behaviour learned by the agent.\n",
    "\n",
    "\n",
    "## 1.1 Learning Objectives: <a id='1.1'></a>\n",
    "- To understand the value iteration method and how it works\n",
    "- To understand the importance of environment definition on behaviour\n",
    "\n",
    "\n",
    "## 1.2 Code Structure & Files <a id='1.2'></a>\n",
    "\n",
    "You don’t have to edit any other file other than ```ex2.ipynb``` to complete this exercise.\n",
    "\n",
    "```\n",
    "├───imgs                 # Images used in notebook\n",
    "│   ex2.ipynb            # Main assignment file containing tasks <---------\n",
    "│   sailing.py           # Defines the sailing gridworld environment\n",
    "│   utils.py             # Contains save object function\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df707adf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6c35f556881515934846ea6c342ad493",
     "grade": false,
     "grade_id": "cell-1aa91773dde5d955",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# 2. Sailor Gridworld <a id='2.'></a>\n",
    "\n",
    "Consider a sailor who managed to escape from a sinking ship, and now has to find the way to the nearest harbour. The sea is divided into a grid, with each grid cell corresponding to a state. Therefore, the state can be thought of as a two dimensional vector:\n",
    "\n",
    "$$\n",
    "s=\\left(\\begin{array}{c}\n",
    "x \\\\\n",
    "y \\\\\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "There are four actions available: moving left, right, up, and down. When the sailor reaches the harbour, the episode terminates and a reward of 10 is given. If the sailor hits the rocks, the episode terminates and a reward of −2 is given. On all other steps, the reward is 0. The environment is shown in Figure 1. \n",
    "\n",
    "<figure>\n",
    "<img src=\"imgs/sailor1.png\" width=\"400px\">\n",
    "<figcaption style=\"text-align: center\"> Figure 1: The Sailor gridworld environment. Light blue squares represent the calm part of the sea, gray squares – the rocks, dark blue – the windy passage between the rocks. The green square in upper right corner is the target harbour. The current (in this picture also the initial) position of the sailor is denoted with a brown \"boat\"  </figcaption>\n",
    "</figure>\n",
    "\n",
    "The shortest way to the harbour goes through a narrow passage between rocks, which is known to have unpredictable heavy wind conditions. When moving in that area, the sailor can be carried an extra \"square\" in a random direction — that is, land in any of the squares adjacent to the desired target square. This is shown in Figure 2. \n",
    "\n",
    "<figure>\n",
    "<img src=\"imgs/sailor2.png\" width=\"700px\">\n",
    "<figcaption style=\"text-align: center\"> Figure 2: Possible state transitions in windy passage when the issued action was to go left (a) or to go up (b). The sailor may end up in the square to the left (a) or up (b), as indicated by the green arrow. There is also a small $p_{wind}$ that the sailor will move for an additional unit in a random direction, as indicated by one of the yellow arrows. Therefore, in addition to moving one square in the target direction, it can (1) move two squares in the desired direction, (2) stay in place, or (3) be carried sideways to one of the squares on the diagonal. </figcaption>\n",
    "</figure>\n",
    "\n",
    "The sea around the passage is generally calm, but there is a low probability that the sailor will be carried in the direction perpendicular to where he was heading, as shown in Figure 3. \n",
    "\n",
    "<figure>\n",
    "<img src=\"imgs/sailor3.png\" width=\"300px\">\n",
    "<figcaption style=\"text-align: center\"> Figure 3: Possible state transitions in calm water when the issued action was to go right. The sailor may end up in the square to the right, as indicated by the green arrow. There is a small chance $p_{calm}$ that the sailor will move in the perpendicular direction, as indicated by the yellow arrows.  </figcaption>\n",
    "</figure>\n",
    "\n",
    "All of these probabilities ($p_{calm}$ and $p_{wind}$) as well as the effects of the wind and the exact location of the harbour are perfectly known to the sailor. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0897c2e6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e6d1b5b2ccaf4a6d7791ba816be510fc",
     "grade": false,
     "grade_id": "cell-73b683749cd09792",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# 3. Value iteration <a id='3.'></a>\n",
    "\n",
    "Value iteration is a method for computing an optimal MDP (Markov Decision Process) policy. We start with arbitrary initial state values and iteratively update our estimate of every state’s value by using Bellman equation as an update rule. A more detailed description, along with the exact equations, can be found in [1] Section 4.4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf7740e",
   "metadata": {},
   "source": [
    "## Warnings:\n",
    "\n",
    "- Don’t copy and paste cells within a notebook. This will mess up the tracking metadata and prevent autograding from working.\n",
    "- Only add new cells using the '+' button in the upper toolbar and do not split cells.\n",
    "- Be cautious about things such as copying the whole notebook to Colab to work on it. This has sometimes resulted in removing all notebook metadata, making autograding impossible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c855ac0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dbeb34663926ee703648eb6ee3cb0dd9",
     "grade": false,
     "grade_id": "cell-e88e88348ff60963",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='T1'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Task 1.</b> Implementing Value Iteration (30 points) </h3> \n",
    "\n",
    "Implement value iteration for the sailor using the code below as a template, assuming the discount factor value $\\gamma = 0.9$. In addition to the state values, compute the policy – path to the harbour, using computed state values. Run your implementation for 100 iterations. Render the values and policy after every iteration and observe how the values and policy are updated. Also, **run the program a few times and check if the sailor is able to reach the goal every time**. Ensure the values and policy is ared saved as a ```.pkl``` file using the ```u.save_object()``` cell below . This ```.pkl``` file is saved within the same directory as this file. **Attach an image of the estimated state values and policy into your submission.**\n",
    "\n",
    "**Hint:** The environment contains a 3-D array (```env.transitions```) of shape $[n_x, n_y, n_a]$, which contains all possible state transitions. The transitions for state $(x, y)$ and action a can be accessed by ```env.transitions [x, y, a]``` . This will return a list of Python ***named-tuples*** Transition=$(s′, reward, done, p)$. The components of the named-tuple can be accessed as ```transition.state```, ```transition.reward```, ```transition.done```, ```transition.prob```. For example, ```env.transitions [3, 3, env.UP]``` would return a list of three possible state transitions:\n",
    "\n",
    "```\n",
    "(state=(3, 2), reward=0.0, done=0.0, prob=0.05),\n",
    "(state=(3, 4), reward=0.0, done=0.0, prob=0.05),\n",
    "(state=(4, 3), reward=0.0, done=0.0, prob=0.9)\n",
    "```\n",
    "\n",
    "which corresponds to moving to state (4, 3) with probability 0.9, or moving to states (3, 4) and (3, 2) with probability 0.05 for each. None of these transitions results in a reward or in terminating the episode (the second and third elements are zero). When the episode has already terminated, the next state will be set to ```None```.\n",
    "\n",
    "**Caveat:** Pay extra attention to indices in the Bellman equation – specifically, where $V_k$ and where $V_{k−1}$ must be used.\n",
    "    \n",
    "🔝\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f064efd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "skip_training = False  # Set this flag to True before validation and submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dd6620-e160-43cf-a3bb-66fcfacad671",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fe9c7267b06b0df5ef620aa472ada384",
     "grade": true,
     "grade_id": "cell-597582ab28a00fd5",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8420398",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "be9ac39b2dcd75ed1ea14253cac90f4c",
     "grade": false,
     "grade_id": "cell-6e4787a762662a16",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(\"..\"))\n",
    "import numpy as np\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "from sailing import SailingGridworld\n",
    "import utils as u\n",
    "\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c47baa0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up the environment\n",
    "env = SailingGridworld(rock_penalty=-2, value_update_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69b0e27",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_values_policy(num_iterations=100, gamma=0.9, eps=1e-4, plot_values=False, filename='values.gif'):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        num_iterations: num of iterations to update the value function and policy\n",
    "        gamma: discount factor used in value iteration\n",
    "        eps: a tiny constant used to check the convergence of the value function and policy\n",
    "        plot_values: if True, a GIF will be saved to plot state value functions and policy\n",
    "        filename: filename of the saved value plot, if plot_value is True.\n",
    "    Returns:\n",
    "        updated v_est and policy, both are np.ndarray with shape (env.w, env.h)\n",
    "    \"\"\"\n",
    "    \n",
    "    # value table V_t, it stores the value at a position (x, y).\n",
    "    v_est = np.zeros((env.w, env.h)) # env.w, env.h: width and height of the environment shown in Fig 1.\n",
    "    # policy table pi_t, it stores the current action to take at a position (x, y)\n",
    "    policy = np.zeros((env.w, env.h))\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        '''\n",
    "        # TODO: Task 1, implement value iteration and retrive the resulting policy.\n",
    "        #       In each iteration, update the state values v_est and policy with new values/policy.\n",
    "        #       The v_est is updated according to the value iteration formula.\n",
    "        '''\n",
    "        ########## Your code starts here ##########\n",
    "\n",
    "        ########## Your code ends here ##########\n",
    "\n",
    "        '''\n",
    "        # TODO: Task 2, check the convergence of the value function and policy.\n",
    "        #       In each iteration, check the maximal absolute difference in successive v_est/policy,\n",
    "        #       and compare it with eps, e.g., check the difference between v_est_i and v_est_i-1\n",
    "        '''\n",
    "        ########## Your code starts here ##########\n",
    "\n",
    "        ########## Your code ends here ##########\n",
    "\n",
    "    \n",
    "    if plot_values:\n",
    "        # Call env.record_values_policy function with your estimated state values and policy\n",
    "        # to produce a GIF of the changing values. In our case, we only record the final state values and policy to save running time.\n",
    "        # Call env.save_values_policy function to save the recorded GIF to filename.gif\n",
    "        env.record_values_policy(v_est, policy)\n",
    "        env.save_values_policy(filename=filename)\n",
    "\n",
    "    return v_est, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40d65d6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# value iteration -- update value estimation and policy\n",
    "if not skip_training:\n",
    "    value_est, policy = get_values_policy(num_iterations=100, gamma=0.9, eps=1e-4, plot_values=True, filename='T1_values.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e989d418",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def eval_policy(policy, N=1, gamma=0.9, render=False, filename='env.gif'):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        policy: the policy to evaluate.\n",
    "        N: number of evaluation episode. \n",
    "            e.g., N=10 means the policy is evaluated 10 times.\n",
    "        gamma: discount factor to calculate the discounted_return.\n",
    "        render: if True, a GIF of the policy's behavior will be saved.\n",
    "        filename: filename to save the GIF file.\n",
    "    Returns:\n",
    "        mean and stddev of discounted_returns.\n",
    "    \"\"\"\n",
    "    discounted_returns = np.empty((N,))\n",
    "    # Eval policy\n",
    "    for ep in range(N):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        steps = 0\n",
    "        discounted_return = 0\n",
    "        \n",
    "        while not done:\n",
    "\n",
    "            ########## Your code starts here ##########\n",
    "            '''\n",
    "            # TODO: Use the policy to take the optimal action (Task 1)\n",
    "            '''\n",
    "            action = ...\n",
    "\n",
    "            # Take a step in the environment\n",
    "            state, reward, done, _ = env.step(action)\n",
    "\n",
    "            '''\n",
    "            # TODO: Accumulate discounted return for this episode\n",
    "            '''\n",
    "            discounted_return += ...\n",
    "\n",
    "            steps += 1\n",
    "            \n",
    "            ########## Your code ends here ##########\n",
    "\n",
    "            if render:\n",
    "                env.render(filename=filename)\n",
    "        # Record the discounted return for         \n",
    "        discounted_returns[ep] = discounted_return\n",
    "    \n",
    "    mean_discounted_returns = np.mean(discounted_returns)\n",
    "    std_discounted_returns = np.std(discounted_returns)\n",
    "    \n",
    "    return mean_discounted_returns, std_discounted_returns\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b63b7e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "    mean_discounted_returns, std_discounted_returns = eval_policy(policy=policy, N=1, gamma=0.9, render=True, filename='T1_eval.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b136626",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5f596a076438a5ed493c485e740b217b",
     "grade": false,
     "grade_id": "cell-1309d275d6fe8d7f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "To see how the sailboat traverses the gridworld use this cell below to display the created gif:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7124a67b-840b-4f96-a76e-a68a8fa2b42c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "    display(Image(data=open('T1_values.gif','rb').read(), format='png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706400e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "    display(Image(data=open('T1_eval.gif','rb').read(), format='png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1d13be",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9656850de8695acf1050a75bb36f0453",
     "grade": false,
     "grade_id": "cell-bc1976681ad69d81",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Do not change the cells below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0028f3e6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "279c9666423ec38f892a9fc5eca82b87",
     "grade": false,
     "grade_id": "cell-d4ac907da7701256",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "    # save value and policy\n",
    "    u.save_object({'value': value_est, 'policy': policy}, './value_policy.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d70b6eb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e1a71ca1be4e6d949edbeee393c62c50",
     "grade": false,
     "grade_id": "cell-9dec9d88b018e937",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "with open(\"value_policy.pkl\", 'rb') as file:\n",
    "    student_value_policy = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0ace9e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b86d61c5b15cb9b67edeaab50f2482eb",
     "grade": true,
     "grade_id": "cell-6b2ae5e19672276c",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert student_value_policy['value'].shape == (15, 10)\n",
    "assert student_value_policy['policy'].shape == (15, 10)\n",
    "\n",
    "assert student_value_policy[\"policy\"][6][7] == 2.\n",
    "assert student_value_policy[\"policy\"][14][8] == 3.\n",
    "\n",
    "np.testing.assert_allclose(student_value_policy['value'][0][0], 0.80533352, atol=1e-03)\n",
    "np.testing.assert_allclose(student_value_policy['value'][14][8], 9.8369366, atol=1e-03)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520e3926",
   "metadata": {},
   "source": [
    "The following cell is used for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433cd331",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "19bb9b32940c9f43cde37b74d0f6b216",
     "grade": true,
     "grade_id": "cell-d82f937544909b8f",
     "locked": true,
     "points": 30,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef32ef45",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c1067e7031986937809bbfb186241a81",
     "grade": false,
     "grade_id": "cell-f42e1f1e91da955b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "<a id='Q1'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Question 1.1</b> Reinforcement Learning Components (5 points) </h3> \n",
    "\n",
    "What is the agent and the environment in this sailor gridworld?\n",
    "            \n",
    "🔝\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0bfbc8-2dbb-418d-b774-8c5fe08474ea",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "72666ec03ca2293e1f30f4a529b61e19",
     "grade": false,
     "grade_id": "cell-4cd3641fc65531cc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Choose **only one**:\n",
    "\n",
    "1. Agent is the sailor and the environment is the sea.\n",
    "2. Agent is the sea and the environment is the sailor.\n",
    "3. There is no environment, the agent consists of the sailor and the sea.\n",
    "4. There is no agent, the environment consists of the sailor and the sea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22eb858f-0bc6-4edf-bf40-6d35b70b29db",
   "metadata": {},
   "outputs": [],
   "source": [
    "sq1_1 = None # Answer question 1.1 with appropriate option number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a037351-e1d0-4e2f-a9ed-df0d904b9de8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "72c96558388bb637fc5b04a3b9c079e4",
     "grade": true,
     "grade_id": "cell-f832291a4588a1a9",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert sq1_1 in range(1, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5802aabd-a38c-441a-8550-e86703833903",
   "metadata": {},
   "source": [
    "The following cells are used for grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9e60f1-4728-43b4-ae7b-773819980593",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ea7a367731f7caaa75414ae9d1af8fc4",
     "grade": true,
     "grade_id": "cell-3f9bcc7ca9eedce8",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b45a8080",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "29ea3f15b6173c9a4735cbc9e0a92740",
     "grade": false,
     "grade_id": "cell-9d1bd67bcac85be3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='Q2'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Question 1.2</b> Value Analysis (5 points) </h3> \n",
    "\n",
    "What is the state value of the harbour and rock states? Why?\n",
    "            \n",
    "🔝\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a06ea0d-3aaa-48c9-8baa-c43dd88fb40c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "20554c72adf376d4df71d116e69b2637",
     "grade": false,
     "grade_id": "cell-c6308551754343b4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Choose **only one**:\n",
    "1. The state-value of harbour is 10 and state-value of the rock states is -2 because it is the reward received at these states and both are terminal states.\n",
    "2. The state-value of harbour is 0 and state-value of the rock states is -2 because harbour is a terminal state and the received reward at a rock is -2.\n",
    "3. The state-value of harbour is 10 and state-value of the rock states is 0 because the received reward at the harbour is 10, and the rock states are terminal states.\n",
    "4. The state-value of harbour is 0 and state-value of the rock states is 0 because both states are terminal states.\n",
    "5. The state-value of harbour is 9 and state-value of rock states is 0 because the optimal policy receives reward 10 at the harbour with discount factor 0.9, and the rock-state is a terminal state.   \n",
    "6. The state-value of harbour is 9 and state-value of rock states is -1.8 because the optimal policy receives reward 10 at the harbour and reward -2 at the rock-states, with discount factor 0.9.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38526dbf-a8a0-4e35-9e0f-a2df50868171",
   "metadata": {},
   "outputs": [],
   "source": [
    "sq1_2 = None # Answer question 1.2 with appropriate option number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94a468e-83ce-4f2e-aa42-b003cd7d5aae",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dcfd6f67a0f3b2fd512ad57e80095048",
     "grade": true,
     "grade_id": "cell-bff826e2eb549613",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert sq1_2 in range(1, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ebb5e1-f2ef-4871-b359-4dfd5c8b5083",
   "metadata": {},
   "source": [
    "The following cells are used for grading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc886682-19d6-4f18-82d8-6b5eb5d637d3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "205f082c832ab5aba676e49e7d47d851",
     "grade": true,
     "grade_id": "cell-6ffde1f6f098d6d6",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "071f9f21",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8538c6e6d3def8045f859ac115db0887",
     "grade": false,
     "grade_id": "cell-6a0cfd12ea795bb5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='Q3'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Question 1.3</b> Investigating Optimal Path (10 points) </h3> \n",
    "\n",
    "Which path did the sailor choose, the safe path below the rocks, or the dangerous path between the rocks? If you change the reward for hitting the rocks to -10 (that is, make the sailor value life more), does the sailor still choose the same path?\n",
    "            \n",
    "🔝\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75c666b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "    env = SailingGridworld(rock_penalty=-10, value_update_iter=100) # Set up env\n",
    "    value_est, policy = get_values_policy(num_iterations=100, gamma=0.9, eps=1e-4, plot_values=False) # Run value iteration\n",
    "    mean_discounted_returns, std_discounted_returns = eval_policy(policy=policy, N=1, gamma=0.9, render=True, filename='Q1_3.gif') # Evaluate policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9693ffee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "    display(Image(data=open('Q1_3.gif','rb').read(), format='png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e38645-10f6-4ea8-81e6-0b1343ec3828",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3fda226bd94dbfb29acc387315c11fbf",
     "grade": false,
     "grade_id": "cell-f750fe95835fb004",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Choose **only one**:\n",
    "1. Initially, the sailor chose the safe path  (below rocks) and after changing the reward to -10 it chose the same path.\n",
    "2. Initially, the sailor chose the safe path (below rocks) and after changing the reward to -10 it chose the dangerous path (between rocks).\n",
    "3. Initially, the sailor chose the dangerous path (between rocks) and after changing the reward to -10 it chose the same path.\n",
    "4. Initially, the sailor chose the dangerous path (between rocks) and after changing the reward to -10 it chose the safe path (below rocks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e0ddcf-503b-4831-b2cf-e82f8e1c8042",
   "metadata": {},
   "outputs": [],
   "source": [
    "sq1_3 = None # Answer question 1.3 with appropriate option number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7acb94e-656c-45b4-bd73-e615cca59f25",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2afea156a253a16136c657f6ba255b20",
     "grade": true,
     "grade_id": "cell-701b901fbd8bc71c",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert sq1_3 in range(1, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6663358f-4149-4bf9-89ad-3cdbd5078c36",
   "metadata": {},
   "source": [
    "The following cells are used for grading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ce5515-08e0-4e9c-81b7-3cf675ba7408",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7a190de3d19582f00f4ff98cfedcddf8",
     "grade": true,
     "grade_id": "cell-e165f1a97d94343a",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ecca922",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "57b157b3edc3a5564631ab6726938adb",
     "grade": false,
     "grade_id": "cell-b7582700d7b23f55",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='Q4'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Question 1.4</b> Investigating Convergence Properties (10 points) </h3> \n",
    "\n",
    "What happens if you run the algorithm for 30 iterations? Do the value function and policy still converge? For the value function, you can assume they have converged\n",
    "if the maximum change in value is lower than a certain threshold $\\epsilon = 10^{-4}$:\n",
    "$$\n",
    "\\max _s\\left|V_k(s)-V_{k-1}(s)\\right|<\\epsilon\n",
    "$$\n",
    "\n",
    "where $V_k(s)$ is the estimated value of state $s$ in $k$-th iteration of the algorithm.\n",
    "Generally, **which of them** - the policy or value function - needs less iterations to converge?\n",
    "    \n",
    "🔝\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2311be",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "    env = SailingGridworld(rock_penalty=-10, value_update_iter=30) # Set up env\n",
    "    value_est, policy = get_values_policy(num_iterations=30, gamma=0.9, eps=1e-4,  plot_values=True, filename='T2_values.gif') # Run value iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158f430b-4e2d-4e5c-9569-2f009062abca",
   "metadata": {},
   "source": [
    "Choose **only one**:\n",
    "\n",
    "1. The policy generally converges faster since the policy considers only the relative ranking of the states (i.e. which states have larger values) instead of using the absolute values.\n",
    "2. The value function generally converges faster since the policy directly depends on the value function and therefore cannot converge before it.\n",
    "3. The value function generally converges faster since if the value function changes, the policy must also change to reflect the new expected rewards for each state.\n",
    "4. The policy function generally converges faster because the threshold (epsilon) is often small enough for the value iterations to be such small adjustments as to not affect the policy.\n",
    "5. Neither generally converges faster. Instead, the convergence speed is dependent on the chosen threshold (epsilon)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbfb3ac-c9aa-4a8a-b3db-a7f9c92e5a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "sq1_4 = None # Answer question 1.4 with appropriate option number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d044c77f-10a6-412c-9d6d-2a3013a679a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sq1_3 in range(1, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f25d3b-6496-4c56-803d-d107c426610b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2f4c3538050a4cbc12969421dbe0da95",
     "grade": true,
     "grade_id": "cell-783960665e249b2c",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb88aa6c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b06f52326cc7b1263c9d840b5cd25ed4",
     "grade": false,
     "grade_id": "cell-4951321af874b0c1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='T2'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Task 2.</b> Number of Iterations Until Convergence (10 points) </h3> \n",
    "\n",
    "Set the reward for crashing into the rocks back to -2. Change the termination condition of your algorithm to make it run until convergence. **Report the number of iterations required for the value function to converge.**\n",
    "    \n",
    "🔝\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e1c21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "    env = SailingGridworld(rock_penalty=-2, value_update_iter=100) # Set up env\n",
    "    value_est, policy = get_values_policy(num_iterations=100, gamma=0.9, eps=1e-4,  plot_values=True, filename='T3_values.gif') # Run value iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0cb75f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1aa6a2e8a748cd3e1765e4da3ad341f7",
     "grade": false,
     "grade_id": "cell-4001dc07a870157b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "**Select the most appropriate range**:\n",
    "1. 9-12\n",
    "2. 30-35\n",
    "3. 70-75\n",
    "4. 100-105"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0712e288",
   "metadata": {},
   "outputs": [],
   "source": [
    "st2 = None #Aswer task 3 with the appropriate answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ed4aa8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "65fbd3126a589dc8c324abb77093a70d",
     "grade": true,
     "grade_id": "cell-de9f06d677680e9e",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert st2 in range(1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2413744",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "266637496db6a5dfad3d5a5a10ee0658",
     "grade": true,
     "grade_id": "cell-b639d0c5fcd51f9a",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d484fdd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "063a6bdb7a2816d893d16d2b14c862c1",
     "grade": false,
     "grade_id": "cell-ebd4b3420a74adbf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='T3'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Task 3.</b> Evaluating the Policy (10 points) </h3> \n",
    "\n",
    "Evaluate your learned policy for N = 800 episodes, and compute the discounted return of the initial state, see [1] Eq. (3.8), for each episode. The reward for crashing into rocks must be kept at -2 for this exercise. **Report the average and standard deviation of the initial state’s discounted return over the N=800 episodes.**\n",
    "    \n",
    "🔝\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4f47f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "    env = SailingGridworld(rock_penalty=-2, value_update_iter=100) # Set up env\n",
    "    value_est, policy = get_values_policy(num_iterations=100, gamma=0.9, eps=1e-4,  plot_values=True, filename='T4_values.gif') # Run value iteration\n",
    "    mean_discounted_returns, std_discounted_returns = eval_policy(policy=policy,N=800, gamma=0.9)\n",
    "    print(f'Average: {mean_discounted_returns:.3f}, Standard Deviation: {std_discounted_returns:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de766b89",
   "metadata": {},
   "source": [
    "DOUBLE CLICK HERE TO EDIT, CLEAR THIS TEXT AND REPORT HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516d6294",
   "metadata": {},
   "source": [
    "The cell below is used for testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a451c4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2d721bdfe8be42faea2246b1c9e051f4",
     "grade": true,
     "grade_id": "cell-1bce7b411270be45",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%capture --no-stdout\n",
    "\"TEST CELL\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f22f560",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2fb271292bc6b86c994c3be132aaa4b4",
     "grade": false,
     "grade_id": "cell-a8694c9bce81b1a7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='Q5'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Question 3.1</b> Relationship Between Discounted Return and the Value Function (10 points) </h3> \n",
    "\n",
    "What is the relationship between the discounted return and the value function?\n",
    "\n",
    "            \n",
    "🔝\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef58082",
   "metadata": {
    "tags": []
   },
   "source": [
    "Choose **only one**:\n",
    "\n",
    "1. The discounted return is the average of the value function over all states.\n",
    "2. The value function is the maximum discounted return achievable from a state.\n",
    "3. The value function is the expected discounted return from a state under a given policy.\n",
    "4. The discounted return is proportional to the gradient of the value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1844db38-990b-4fd3-9c9d-ac97690ee545",
   "metadata": {},
   "outputs": [],
   "source": [
    "sq3_1 = None # Answer question 3.1 with appropriate option number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87818bf1-2994-4c95-94de-3e0e744b3a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sq3_1 in range(1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28aa6030-1296-4196-9367-6f6341ad0be7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "704aa67a86181dbea5f5ffc67d18bf76",
     "grade": true,
     "grade_id": "cell-2378a56ca54c8c06",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02e967f1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dfcb10137feeb0338d173f699bbd9c14",
     "grade": false,
     "grade_id": "cell-787a0eb359e9e5f4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='Q6'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Question 3.2</b> Considering Unknown Environments (10 points) </h3> \n",
    "\n",
    "Imagine a reinforcement learning problem involving a robot exploring an ***unknown*** environment. Could the ***value iteration*** **approach used in this exercise** be applied **directly** to that problem? Select the most appropriate answer.\n",
    "            \n",
    "🔝\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9173695",
   "metadata": {
    "tags": []
   },
   "source": [
    "Choose **only one**:\n",
    "\n",
    "1. No, because the real-world state space can be discrete.\n",
    "2. Yes, as long as the robot can sense the environment (immediate nearby states).\n",
    "3. No, because the environment is completely different.\n",
    "4. No, because the transition probabilities are unknown.\n",
    "5. Yes, as long as the real-world environment has limited dimensionality making it possible to represent the Q function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa69e3f-a77a-419d-9b96-209f759055e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sq3_2 = None # Answer question 3.2 with appropriate option number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9ec917-1c5a-4993-914b-d26ed36b7d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sq3_2 in range(1, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6527e3d6-c576-4387-bfc5-5a0c4dd5ce9d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b2410bc67d5cacf43477d2d03affd930",
     "grade": true,
     "grade_id": "cell-b40cd36628988a3d",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0dac5f5b",
   "metadata": {},
   "source": [
    "\n",
    "# 4. Submitting <a id='4.'></a>\n",
    "Ensure all tasks and questions (in ```ex2.ipynb```) are answered and the relevant plots are recorded in the relevant places. \n",
    "\n",
    "Ensure the correct model files are saved:\n",
    "- Task1, ensure ```value_policy.pkl``` file for the state values and the policy in Task 1 are included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bc818e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that skip training is set to True before submission\n",
    "assert skip_training == True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad098b5f",
   "metadata": {},
   "source": [
    "## 4.1 Feedback <a id='4.1'></a>\n",
    "In order to help the staff of the course as well as the forthcoming students, it would be great if you could answer to the following questions in your submission:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d482741-8dd4-4294-a5c8-d380d3780ef4",
   "metadata": {},
   "source": [
    "1) How much time did you spend solving this exercise? (change the ```hrs``` variable below to a floating point number representing the number of hours taken e.g. 5.43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306e0536-3cb9-4e4e-9acb-52aa752087f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hrs = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac10609-bc78-49d1-8446-c6d0549dcf4e",
   "metadata": {},
   "source": [
    "2. Difficulty of each task/question from 1 to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03264b69-b78d-49d9-ace0-9a051af46166",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "T1 = None # Student task 1. Implementing value iteration\n",
    "Q1_1 = None # Student question 1.1 Reinforcement learning components\n",
    "Q1_2 = None # Student question 1.2 Value analysis\n",
    "Q1_3 = None # Student question 1.3 Investigating optimal path\n",
    "Q1_4 = None # Student question 1.4 Investigating convergence properties.\n",
    "T2 = None # Student task 2. Number of iterations until convergence.\n",
    "T3 = None # Student task 3. Evaluating the policy.\n",
    "Q3_1 = None # Student question 3.1. Relationship between discounted return and the value function\n",
    "Q3_2 = None # Student question 3.2 Considering unknown environments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02276f97-8763-43fe-bb92-2d551154b567",
   "metadata": {},
   "source": [
    "4. General feedback. Consider questions like:\n",
    "     - Did the content of the lecture relate well with the assignment?\n",
    "     - To what extend did you find the material to be potentially useful for your research and studies?\n",
    "    \n",
    "Add other feedback you think is worth including. Type in the box below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aefce25-1a6b-4d5c-9add-925a80feccab",
   "metadata": {
    "tags": []
   },
   "source": [
    "CLEAR THIS BOX AND ADD YOUR GENERAL FEEDBACK HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f5f8d4",
   "metadata": {},
   "source": [
    "# References <a id='5.'></a>\n",
    "\n",
    "[1] Sutton R S, Barto A G. Reinforcement learning: An introduction[M]. MIT press, 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1118a6ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
