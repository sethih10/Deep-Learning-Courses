{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba46938f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4ad95861e29278a27b9195cfd545d64c",
     "grade": false,
     "grade_id": "cell-4a4cca829fb02ea2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "   <h2 align=\"center\"> <center><b> Reinforcement Learning Assignment 6 - Actor Critic part 2 </b></center></h2>\n",
    "\n",
    "<br>\n",
    "<center><font size=\"3\">This notebook is part of the teaching material for ELEC-E8125</font></center>\n",
    "<center><font size=\"3\">Aalto University</font></center>\n",
    "</div>\n",
    "\n",
    "\n",
    "<a id='TOC'></a>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "# Table of contents\n",
    "* <a href='#1.'> 1. Introduction </a>\n",
    "* <a href='#1.1'> 1.1 Learning Objectives </a>\n",
    "* <a href='#1.2'> 1.2 Code Structure & Files </a>\n",
    "* <a href='#2.'> 2. Deep Deterministic Policy Gradient</a>\n",
    "* <a href='#3.'> 3. Submitting </a>\n",
    "* <a href='#3.1'> 3.1 Feedback </a>\n",
    "* <a href='#8.'> References</a>\n",
    "\n",
    "<a href='#T1'><b>Student Task 1.</b> Implement DDPG algorithm (25 points)</a>\\\n",
    "<a href='#Q1'><b>Student Question 1.1</b> Reasons for using off-policy data (10 points) </a>\\\n",
    "<a href='#Q2'><b>Student Question 2</b> Disadvantages of DDPG (10 points) </a>\n",
    "\n",
    "    \n",
    "**Total Points:** 45\n",
    "\n",
    "**Estimated runtime of all the cells:** 1 hour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de82751-fa7f-43fd-be23-890898fb184c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8a51f3eecbe7fd635de808001e4ad078",
     "grade": false,
     "grade_id": "cell-93adec8b983ab5ee",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# 1. Introduction <a id='1.'></a>\n",
    "In this assignment, we will implement the Deep Deterministic Policy Gradient (DDPG) [1] algorithm for the **HalfCheetah-v4** environment. DDPG is a reinforcement learning algorithm that uses a critic, approximated as a neural network, to estimate values. DDPG uses policy gradient methods to update a deterministic policy (actor) to follow the critic. DDPG can be applied to problems with a continuous action space.\n",
    "\n",
    "## 1.1 Task environments: <a id='1.1'></a>\n",
    "\n",
    "In this exercise, we will focus on HalfCheetah-v4 tasks:\n",
    "- HalfCheetah-v4(https://gymnasium.farama.org/environments/mujoco/half_cheetah/): The HalfCheetah is a 2-dimensional robot consisting of 9 body parts and 8 joints connecting them (including two paws). The goal is to apply torques on the joints to make the cheetah run forward (right) as fast as possible, with a positive reward allocated based on the distance moved forward and a negative reward allocated for moving backward. The torso and head of the cheetah are fixed, and the torque can only be applied on the other 6 joints over the front and back thighs (connecting to the torso), shins (connecting to the thighs) and feet (connecting to the shins).\n",
    "<figure style=\"text-align: center\">\n",
    "    <img src=\"imgs/HalfCheetah-v4.png\" width=\"300\"/>\n",
    "    <figcaption style=\"text-align: center\">  Figure 1: The HalfCheetah-v4 environment. </figcaption>\n",
    "</figure>\n",
    "\n",
    "## 1.2 Learning Objectives: <a id='1.2'></a>\n",
    "\n",
    "- Understand the DDPG algorithm\n",
    "- Understand the difference between on-policy methods and off-policy methods\n",
    "\n",
    "## 1.3 Code Structure & Files <a id='1.3'></a>\n",
    "\n",
    "```ex6_DDPG.ipynb``` is the file needed to be modified for this part of the assignment.  \n",
    "\n",
    "<span style=\"color:red\"> **# IMPORTANT: DO NOT FORGET ANOTHER PART IN ```ex6_PG_AC.ipynb```** </span>\n",
    "\n",
    "```\n",
    "‚îú‚îÄ‚îÄ‚îÄcfg                            # Config files for environments\n",
    "‚îú‚îÄ‚îÄ‚îÄimgs                           # Images used in notebook\n",
    "‚îú‚îÄ‚îÄ‚îÄresults\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ‚îÄHalfCheetah-v4\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ‚îÄlogging                \n",
    "‚îÇ   ‚îÇ   ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄlogging.pkl        # Contains logged data\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ‚îÄmodel              \n",
    "‚îÇ   ‚îÇ   ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ*HalfCheetah-v4_params.pt    # Contains trained model\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄvideo                   # Videos saved\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ*ddpg.png               # Contains training performance plot\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ‚îÄInvertedPendulum-v4\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ‚îÄlogging                \n",
    "‚îÇ   ‚îÇ   ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄlogging.pkl        # Contains logged data\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ‚îÄmodel              \n",
    "‚îÇ   ‚îÇ   ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ*InvertedPendulum-v4_params.pt      # Contains trained model\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄvideo                   # Videos saved\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ*pg_ac.png              # Contains training performance plot\n",
    "‚îÇ   ex6_DDPG.ipynb                  # 2nd assignment file containing tasks <---------This task\n",
    "‚îÇ   ex6_PG_AC.ipynb                 # 1st assignment file containing tasks <---------\n",
    "‚îÇ   train.py                        # Contains train and test functions \n",
    "‚îÇ   utils.py                        # Contains useful functions \n",
    "‚îî‚îÄ‚îÄ‚îÄbuffer.py                       # Contains buffer functions\n",
    "```\n",
    "\n",
    "## 1.4 Execution time <a id='1.4'></a>\n",
    "\n",
    "The training of DDPG might take more than 30 min depending on server load. If you have problems with the training time, you can train DDPG locally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2396913a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "341fec6bcd954016c6860be81cc43916",
     "grade": false,
     "grade_id": "cell-efb187db4234b21a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Warnings:\n",
    "\n",
    "- Don‚Äôt copy and paste cells within a notebook. This will mess up the tracking metadata and prevent autograding from working.\n",
    "- Only add new cells using the '+' button in the upper toolbar and do not split cells.\n",
    "- Be cautious about things such as copying the whole notebook to Colab to work on it. This has sometimes resulted in removing all notebook metadata, making autograding impossible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a27be2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4cf362fbaa7d0d358be854de78b66e3b",
     "grade": false,
     "grade_id": "cell-5bd4644de054d484",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# 2. Deep Deterministic Policy Gradient <a id='2.'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6b0c3f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d8e4ce6a1453b96980c2d1290a2378f3",
     "grade": false,
     "grade_id": "cell-213dada1ac585536",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "<a id='T1'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Task 1.</b> Implementing DDPG (25 points) </h3> \n",
    "\n",
    "Implement the deep deterministic policy gradient (DDPG) algorithm for the HalfCheetah environment. You can refer to the code from the \"ex4_dqn.ipynb\" notebook for guidance. Additionally, if necessary, consult the paper [1] for a deeper understanding of the algorithm. You can check the training performance plot in the result folder after running the plot cell.Take Figure 2 as a reference training plot. \n",
    "\n",
    "<figure style=\"text-align: center\">\n",
    "<img src=\"imgs/ddpg.png\" width=\"400px\">\n",
    "<figcaption style=\"text-align: center\"> Figure 2:  Training plot of the deterministic policy gradient on the HalfCheetah environment.\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "**Complete the unfinished implementation in the `DDPG` class (marked with ```TODOs```)**. \n",
    "    \n",
    "1. **Agent Update Function**: Finish the `_update(self, )` function within the `DDPG` class\n",
    "2. **Get Action Method**: Finish the `get_action(self, observation, evaluation=False)` function within the `DDPG` class.\n",
    "\n",
    "**Hint:** Make always sure variables have correct dimensions!\n",
    "    \n",
    "    \n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df43ce60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "skip_training = False  # Set this flag to True before validation and submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6805eae",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1a892f0c80589be8a8d619ca8c81ce66",
     "grade": true,
     "grade_id": "cell-87f9737c001a2bc7",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61638d97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(\"..\"))\n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from IPython.display import Video\n",
    "\n",
    "import copy, torch, yaml, time\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from buffer import ReplayBuffer\n",
    "\n",
    "import train as t\n",
    "import utils as u\n",
    "\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce70bace",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Actor-critic agent\n",
    "\n",
    "# Policy class. The policy is represented by a neural network. \n",
    "# Reminder: in DDPG the policy is deterministic.\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super().__init__()\n",
    "        self.max_action = max_action\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, 32), nn.ReLU(),\n",
    "            nn.Linear(32, 32), nn.ReLU(),\n",
    "            nn.Linear(32, action_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.max_action * torch.tanh(self.actor(state))\n",
    "\n",
    "\n",
    "# Critic class. The critic is represented by a neural network.\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(state_dim+action_dim, 32), nn.ReLU(),\n",
    "            nn.Linear(32, 32), nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], 1)\n",
    "        return self.value(x) # output shape [batch, 1]\n",
    "\n",
    "\n",
    "class DDPG(object):\n",
    "    def __init__(self, state_shape, action_dim, max_action, lr, gamma, tau, batch_size, buffer_size=1e6):\n",
    "        self.name = 'ddpg'\n",
    "        state_dim = state_shape[0]\n",
    "        self.action_dim = action_dim\n",
    "        self.max_action = max_action\n",
    "        self.pi = Policy(state_dim, action_dim, max_action).to(device)\n",
    "        self.pi_target = copy.deepcopy(self.pi)\n",
    "        self.pi_optim = torch.optim.Adam(self.pi.parameters(), lr=float(lr))\n",
    "\n",
    "        self.q = Critic(state_dim, action_dim).to(device)\n",
    "        self.q_target = copy.deepcopy(self.q)\n",
    "        self.q_optim = torch.optim.Adam(self.q.parameters(), lr=float(lr))\n",
    "        \n",
    "        self.buffer = ReplayBuffer(state_shape, action_dim, max_size=int(float(buffer_size)))\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        \n",
    "        # used to count number of transitions in a trajectory\n",
    "        self.buffer_ptr = 0\n",
    "        self.buffer_head = 0 \n",
    "        self.random_transition = 5000 # collect 5k random data for better exploration\n",
    "    \n",
    "    def update(self,):\n",
    "        \"\"\" After collecting one trajectory, update the pi and q for #transition times: \"\"\"\n",
    "        info = {}\n",
    "        update_iter = self.buffer_ptr - self.buffer_head # update the network once per transition\n",
    "\n",
    "        if self.buffer_ptr > self.random_transition: # update once we have enough data\n",
    "            for _ in range(update_iter):\n",
    "                info = self._update()\n",
    "        \n",
    "        # update the buffer_head:\n",
    "        self.buffer_head = self.buffer_ptr\n",
    "        return info\n",
    "\n",
    "    '''\n",
    "    # TODO: Task 2\n",
    "    # Complete the following 3 functions\n",
    "    # Hints: 1. Compute the target Q value using the q_target and pi_target networks.\n",
    "    #        2. compute the critic loss and update the q's parameters\n",
    "    #        3. compute actor loss and update the pi's parameters\n",
    "    #        4. Use mean to calculate the actor (policy) loss ;)\n",
    "    #           - https://discuss.pytorch.org/t/loss-reduction-sum-vs-mean-when-to-use-each/115641/2\n",
    "    '''\n",
    "    \n",
    "    # 1. compute target Q, you should not modify the gradient of the variables\n",
    "    def calculate_target(self, batch):\n",
    "        ########## Your code starts here. ##########\n",
    "        with torch.no_grad():\n",
    "            #next_action = ...\n",
    "            #q_tar = ...\n",
    "            #target_Q = ...\n",
    "            next_action = self.pi_target(batch.next_state)\n",
    "            q_tar = self.q_target(batch, next_action)\n",
    "            target_Q = batch.reward + self.gamma*batch.not_done*q_tar\n",
    "        ########## Your code ends here. ##########\n",
    "        return target_Q\n",
    "        \n",
    "    # 2. compute critic loss\n",
    "    def calculate_critic_loss(self, current_Q, target_Q):\n",
    "        ########## Your code starts here. ##########\n",
    "        #critic_loss = ...\n",
    "        critic_loss = F.mse_loss(current_Q, target_Q)\n",
    "        ########## Your code ends here. ##########\n",
    "        return critic_loss\n",
    "\n",
    "    # 3. compute actor loss\n",
    "    def calculate_actor_loss(self, batch):\n",
    "        ########## Your code starts here. ##########\n",
    "        #actor_loss = ...\n",
    "        actions = self.pi(batch.state)\n",
    "        actor_loss = -self.q(batch.state, actions).mean()\n",
    "        ########## Your code ends here. ##########\n",
    "        return actor_loss\n",
    "\n",
    "    def _update(self,):\n",
    "        # get batch data\n",
    "        batch = self.buffer.sample(self.batch_size, device=device)\n",
    "        #    batch contains:\n",
    "        #    state = batch.state, shape [batch, state_dim]\n",
    "        #    action = batch.action, shape [batch, action_dim]\n",
    "        #    next_state = batch.next_state, shape [batch, state_dim]\n",
    "        #    reward = batch.reward, shape [batch, 1]\n",
    "        #    not_done = batch.not_done, shape [batch, 1]\n",
    "\n",
    "        \"\"\"\n",
    "        # TODO: Get the current Q estimate\n",
    "        \"\"\"\n",
    "        #current_Q = ...\n",
    "        current_Q = self.q(batch.state, batch.action)\n",
    "\n",
    "        target_Q = self.calculate_target(batch)\n",
    "        critic_loss = self.calculate_critic_loss(current_Q, target_Q)\n",
    "\n",
    "        # optimize the critic\n",
    "        self.q_optim.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.q_optim.step()\n",
    "\n",
    "        actor_loss = self.calculate_actor_loss(batch)\n",
    "        \n",
    "        # optimize the actor\n",
    "        self.pi_optim.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.pi_optim.step()\n",
    "\n",
    "        \"\"\"\n",
    "        # TODO: update the target q and pi using u.soft_update_params() (See the DQN code)\n",
    "        \"\"\"\n",
    "        u.soft_update_params(..., ..., self.tau)\n",
    "        u.soft_update_params(..., ..., self.tau)\n",
    "\n",
    "        return {}\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def get_action(self, observation, evaluation=False):\n",
    "        if observation.ndim == 1: observation = observation[None] # add the batch dimension\n",
    "        x = torch.from_numpy(observation).float().to(device)\n",
    "\n",
    "        if self.buffer_ptr < self.random_transition: # collect random trajectories for better exploration.\n",
    "            action = torch.rand(self.action_dim)\n",
    "        else:\n",
    "            expl_noise = 0.1 * self.max_action # the stddev of the expl_noise if not evaluation\n",
    "            \n",
    "            '''\n",
    "            # TODO: Use the policy to calculate the action to execute\n",
    "            #       if evaluation equals False, add normal noise to the action, where the std of the noise is expl_noise\n",
    "            # Hint: Make sure the returned action's shape is correct.\n",
    "            '''\n",
    "            ########## Your code starts here. ##########\n",
    "            action = ...\n",
    "            \n",
    "            if not evaluation:\n",
    "                action = ...\n",
    "            ########## Your code ends here. ##########\n",
    "\n",
    "        return action, {} # just return a positional value\n",
    "\n",
    "    def record(self, state, action, next_state, reward, done):\n",
    "        \"\"\" Save transitions to the buffer. \"\"\"\n",
    "        self.buffer_ptr += 1\n",
    "        self.buffer.add(state, action, next_state, reward, done)\n",
    "        \n",
    "    def load(self, filepath):\n",
    "        d = torch.load(filepath)\n",
    "        self.q.load_state_dict(d['q'])\n",
    "        self.q_target.load_state_dict(d['q_target'])\n",
    "        self.pi.load_state_dict(d['pi'])\n",
    "        self.pi_target.load_state_dict(d['pi_target'])\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        torch.save({\n",
    "            'q': self.q.state_dict(),\n",
    "            'q_target': self.q_target.state_dict(),\n",
    "            'pi': self.pi.state_dict(),\n",
    "            'pi_target': self.pi_target.state_dict()\n",
    "        }, filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464e266d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a5ef778c1225f730567b146f09d6a136",
     "grade": true,
     "grade_id": "cell-9b5703720a28f32e",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%capture --no-stdout\n",
    "#Test get_action method\n",
    "def test_get_action_1():\n",
    "    cfg_path=Path().cwd()/'cfg'/'ddpg.yaml'\n",
    "    cfg_args=dict(save_video=False,testing=False,seed=43)\n",
    "    env, cfg = t.setup(cfg_path, cfg_args=cfg_args)\n",
    "    agent = DDPG(cfg.state_shape, cfg.action_dim, cfg.max_action,\n",
    "        cfg.lr, cfg.gamma, cfg.tau, cfg.batch_size, cfg.buffer_size)\n",
    "    agent.random_transition = 0\n",
    "    obs = np.array([-0.02888771, -0.05278197, -0.03048919,  0.00728878,  0.02529376,\n",
    "        -0.08290133,  0.09657571,  0.09825779, -0.08613934,  0.00585323,\n",
    "         0.04022427,  0.05581473, -0.13554548,  0.05622923,  0.00300506,\n",
    "        -0.01851144, -0.18261562])\n",
    "    action = torch.Tensor([ 0.0654,  0.0368,  0.0689, -0.0534,  0.0056, -0.0300])\n",
    "    assert torch.allclose(agent.get_action(obs,True)[0],action,atol=1e-03)\n",
    "\n",
    "test_get_action_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d2c873",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1231482af84079879f530c3b6cfcfcce",
     "grade": true,
     "grade_id": "cell-7c73c8953a0e68f7",
     "locked": true,
     "points": 7,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%capture --no-stdout\n",
    "\"TEST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2313677",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b7ffb6b2cc6a146b9974c7c6eca50e60",
     "grade": true,
     "grade_id": "cell-4edd8fb579960344",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from buffer import Batch\n",
    "\n",
    "def test_update_function_1():\n",
    "    cfg_path=Path().cwd()/'cfg'/'ddpg.yaml'\n",
    "    cfg_args=dict(save_video=False,testing=False,seed=43)\n",
    "    env, cfg = t.setup(cfg_path, cfg_args=cfg_args)\n",
    "    agent = DDPG(cfg.state_shape, cfg.action_dim, cfg.max_action,\n",
    "        cfg.lr, cfg.gamma, cfg.tau, 16, cfg.buffer_size)\n",
    "    \n",
    "    batch = Batch(\n",
    "        state = torch.rand((16,17))*2-1,\n",
    "        action = torch.rand((16,6))*2-1, \n",
    "        next_state = torch.rand((16,17))*2-1, \n",
    "        reward = torch.rand((16,1))*2-1,\n",
    "        not_done = torch.randint(0, 2, (16,1)),\n",
    "        extra = {}\n",
    "    )\n",
    "    current_Q = torch.tensor([[-0.0723],[-0.0669],[-0.0874],[-0.0960],[-0.0206],[-0.0449],[-0.0654],[-0.0624],[-0.0489],\n",
    "                          [-0.0547],[-0.0580],[-0.0534],[-0.0843],[-0.0551],[-0.0456],[-0.0443]])\n",
    "    \n",
    "    target_Q = agent.calculate_target(batch)\n",
    "    critic_loss = agent.calculate_critic_loss(current_Q, target_Q)\n",
    "    actor_loss = agent.calculate_actor_loss(batch)\n",
    "    \n",
    "    assert torch.allclose(target_Q, torch.tensor([[-0.9068],[-0.1060],[ 0.5872],[ 0.4086],[ 0.1976],[-0.4565],[-0.2704],[-0.0492],[-0.4271],\n",
    "                [ 0.2148],[-0.5635],[-0.6509],[-0.4589],[ 0.3514],[-0.2476],[-0.0811]]), atol=0.001)\n",
    "    assert torch.allclose(critic_loss, torch.tensor(0.1777), atol=0.001)\n",
    "    assert torch.allclose(actor_loss, torch.tensor(0.0562), atol=0.001)\n",
    "\n",
    "test_update_function_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b509da28",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d5027eab67a95d1799753b6f1f040a82",
     "grade": true,
     "grade_id": "cell-7c1ab77b16161ff5",
     "locked": true,
     "points": 7,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%capture --no-stdout\n",
    "\"TEST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383c6f18-51e9-47e0-afcd-74f07b688c66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialize the agent\n",
    "with open(Path().cwd()/'cfg'/'ddpg.yaml', 'r') as f:\n",
    "    cfg = u.Struct(**yaml.safe_load(f))\n",
    "\n",
    "agent = DDPG(cfg.state_shape, cfg.action_dim, cfg.max_action,\n",
    "             cfg.lr, cfg.gamma, cfg.tau, cfg.batch_size, cfg.buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cbf81a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train the agent, training should take roughly 35min\n",
    "if not skip_training:\n",
    "    t.train(agent, cfg_path=Path().cwd()/'cfg'/'ddpg.yaml', cfg_args={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4d3d60-4711-444e-9c3f-27a8025cc7e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot training performance\n",
    "if not skip_training:\n",
    "    t.plot(cfg_path=Path().cwd()/'cfg'/'ddpg.yaml',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2750d9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test the trained policy and save a video of the test\n",
    "if not skip_training:\n",
    "    t.test(agent, cfg_path=Path().cwd()/'cfg'/'ddpg.yaml', cfg_args=dict(save_video=True,testing=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b10fd24-a4d3-41f7-9685-45a6064a7e68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "    video = Video(Path().cwd()/'results'/'HalfCheetah-v4'/'video'/'test'/'ex6-episode-0.mp4',\n",
    "    embed=True, html_attributes=\"loop autoplay\") # Set html_attributes=\"controls\" for video control\n",
    "    display(video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922abcaf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5f9814bd31fe241b041323a51740aeae",
     "grade": true,
     "grade_id": "cell-5ccbe463affb19ee",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%capture --no-stdout\n",
    "\"TEST\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712dff78-5132-448e-9513-73f7deb6a464",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dfaf2f1cdd0d7413a218ba17212eb307",
     "grade": false,
     "grade_id": "cell-68d6afdbf240eaf4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "<a id=\"Q1\"></a>\n",
    "<div class= \"alert alert-warning\">\n",
    "    <h3><b>Student Question 1.1</b> On-policy vs off-policy part 1 (10 points) </h3> \n",
    "\n",
    "Why can the Deep Deterministic Policy Gradient (DDPG) algorithm use off-policy data, while policy gradient methods(seen in exercise 5) require on-policy data?\n",
    "    \n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476389a5-6346-47a1-972d-ee9d8d03419d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fc1ad5d93545531f02b0c3a982c8d69b",
     "grade": false,
     "grade_id": "cell-28b7023b6741c120",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    " <h5><b>Choices</b></h5>\n",
    " \n",
    " 1. In DDPG, the critic can generalize from off-policy data, whereas policy gradient methods require data specific to the current policy for accurate gradient estimates.\n",
    " 2. Policy gradient methods are more efficient than DDPG, so they do not need off-policy data.\n",
    " 3. DDPG learns from both on-policy and off-policy data simultaneously, unlike policy gradient methods. \n",
    " 4. The actor in DDPG uses a deterministic policy which is computed only over the state distribution, making it possible to use off-policy data.\n",
    " 5. The critic in DDPG uses Q-learning, which allows it to learn from off-policy data without requiring importance sampling.\n",
    " 6. The actor and critic both rely on importance sampling to effectively handle off-policy data.\n",
    " 7. The actor in DDPG is stochastic, so off-policy data is important for training.\n",
    " 8. In DDPG, the experiences are stored in a replay buffer and sampled to improve the sample efficiency. \n",
    " 9. The actor in DDPG uses a stochastic policy which is computed only over the state distribution, making it possible to use off-policy data.\n",
    " \n",
    "Choose **three** most appropriate choices. Choosing more than three options will remove result in 0 points for this task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f9c6bc-42d2-47e9-a78a-667a95d71574",
   "metadata": {},
   "outputs": [],
   "source": [
    "sq1_1 = [] # Select the appropriate answer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af63cd73-8d2d-4174-a567-1922c50e9b6f",
   "metadata": {},
   "source": [
    "The below cells are used for grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1176570-aae6-4ca7-b1ff-ffee172df282",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8608f3cd00d63588a35a9a539f49f4bd",
     "grade": true,
     "grade_id": "cell-c139ec8eec2b7e05",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert set(sq1_1) < set(range(1, 10))\n",
    "assert 1 <= len(sq1_1) <= 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b544586-9cc4-4478-90bb-ea81134acc63",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8237f597668bd2b3abfce7b7d5f85265",
     "grade": true,
     "grade_id": "cell-fa8bb3d1231f0632",
     "locked": true,
     "points": 3.33,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f8f277-de60-4903-a51a-f7b7693e38af",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "09e1d8272b2197c6745959a3cb3fd0c0",
     "grade": true,
     "grade_id": "cell-fd25785e2be4429b",
     "locked": true,
     "points": 3.33,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451c9f3a-3e85-4b46-a053-39dbd54d36eb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2a332c7f1c53864d46e5fc585fc7721e",
     "grade": true,
     "grade_id": "cell-8c24fa5c552a04d5",
     "locked": true,
     "points": 3.34,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c5754f7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "030004102a0563629c9a917cee5d8ab3",
     "grade": false,
     "grade_id": "cell-96f9f3682903a6eb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='Q2'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Question 1.2</b> On-policy vs off-policy part 2 (10 points) </h3> \n",
    "\n",
    "A key advantage of DDPG over policy gradient methods is its ability to utilize off-policy data. However, what are the disadvantes of the deterministic policy gradient compared to the policy gradient method implemented in ex6_PG_AC.ipynb? Give two of them.\n",
    "\n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac77d2d",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8121188f726821d200595fe858b6d5a9",
     "grade": true,
     "grade_id": "cell-09508e3e337cd302",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fb02ea-e912-46fa-9d35-e4869b07c28b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f74312e3d25e5fd9d699e564855a3943",
     "grade": false,
     "grade_id": "cell-e10ba997078f2b49",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# 3. Submitting <a id='3.'></a>\n",
    "Ensure all tasks and questions (in ```ex6_DDPG.ipynb``` and ```ex6_PG_AC.ipynb```) are answered and the necessary plots are saved in the appropriate locations. The relevant plots and files that need to be submitted for this assignment are:\n",
    "\n",
    "\n",
    "- Training performance plots:\n",
    "  - `ddpg.png`: Training performance plots in terms of episode and episodic reward for DDPG\n",
    "<br>\n",
    "\n",
    "  \n",
    "- Model files:\n",
    "  - `ddpg_params.pt`: Trained model\n",
    "\n",
    "\n",
    "Ensure the model files and plots are saved in correct paths:\n",
    "- ```results/HalfCheetah-v4/ddpg.png``` Training result\n",
    "- ```results/HalfCheetah-v4/model/ddpg_params.pt``` Training Model\n",
    "\n",
    "\n",
    "<span style=\"color:red\"> **# IMPORTANT: DO NOT FORGET ANOTHER TASK IN ```ex6_PG_AC.ipynb```** </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940c3f0a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "79586fc539e0b09b5ae41406379a936d",
     "grade": true,
     "grade_id": "cell-d2219325e83b78bf",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Make sure that skip training is set to True before submission\n",
    "assert skip_training == True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a07d0e-2b83-4e02-8e9b-c457fbb35485",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d52ce543293add85308d941087d69dbe",
     "grade": false,
     "grade_id": "cell-8b79a5c980651c49",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## 3.1 Feedback <a id='3.1'></a>\n",
    "\n",
    "In order to help the staff of the course as well as the forthcoming students, it would be great if you could answer the following questions in your submission:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa58b944-9121-4827-a042-72b01ed21146",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "34a4527eb60a5f49b8f8bbef73f66c5f",
     "grade": false,
     "grade_id": "cell-8f0edb5f0b141cc4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "1) How much time did you spend solving this exercise? (change the ```hrs``` variable below to a floating point number representing the number of hours taken e.g. 5.43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5f3e51-b088-4c7d-8ad4-4eac14e6691c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hrs = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f72584-ec47-492f-8d91-beabc1bbf10d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f2951147edeac0863f6dfcf8408b744b",
     "grade": false,
     "grade_id": "cell-4e98111eb86792ae",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "2) Difficulty of each task/question from 1-5 (int or float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a68775-ff72-40c5-b29b-51de6e6df98b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "T1 = None   # Implementing DDPG (20 points)\n",
    "Q1_1 = None   # Question 1.1 Reasons of using off-policy data (10 points)\n",
    "Q1_2 = None   # Question 1.2 Disadvantages of DDPG (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19fb542-b18b-453a-8ce2-b3bd72b43b4a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4169a6bc56142853bcd827fa90a9099d",
     "grade": false,
     "grade_id": "cell-bdca9673d83a3277",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "3) How well did you understand the content of the task/question from 1-5? (int or float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668114c2-b143-4971-9f3f-dc18583aef8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "T1 = None   # Implementing DDPG (20 points)\n",
    "Q1_1 = None   # Question 1.1 Reasons of using off-policy data (10 points)\n",
    "Q1_2 = None   # Question 1.2 Disadvantages of DDPG (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f07196-619f-47d7-85be-6d2c2d5334d8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1667d4abac473e1ffc974572e6338295",
     "grade": false,
     "grade_id": "cell-ebbc9bbf78336256",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "4) General feedback. Consider questions like:\n",
    "\n",
    "    - Did the content of the lecture relate well with the assignment?\n",
    "    - To what extent did you find the material to be potentially useful for your research and studies?\n",
    "    \n",
    "Please share any additional feedback, suggestions, or comments you have about the lecture, assignment, or course content. Your input is valuable in helping us improve the learning experience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4994fbd2-12d1-41ef-aa8f-16e4cc266fe3",
   "metadata": {
    "tags": []
   },
   "source": [
    "DOUBLE CLICK HERE TO EDIT, CLEAR THIS TEXT AND ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f5f8d4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b3a241fa0c40ec0f265c6a98a12db88d",
     "grade": false,
     "grade_id": "cell-38d448f68fac3e6c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# References <a id='3.'></a>\n",
    "Please use the following section to record references.\n",
    "\n",
    "[1] Timothy P. Lillicrap et al. \"Continuous control with deep reinforcement learning\" ICLR 2016 https://arxiv.org/abs/1509.02971"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
