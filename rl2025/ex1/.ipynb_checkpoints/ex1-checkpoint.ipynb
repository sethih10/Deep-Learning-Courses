{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53274fb5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9b1433e97d4756e2c9546a6a35a1562c",
     "grade": false,
     "grade_id": "cell-36af2f1e65d7b244",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "   <h2 align=\"center\"> <center><b> Reinforcement Learning Assignment 1 - The Reinforcement Learning Framework</b></center></h2>\n",
    "\n",
    "<br>\n",
    "<center><font size=\"3\">This notebook is a part of teaching material for ELEC-E8125</font></center>\n",
    "<center><font size=\"3\">Sep 1, 2025 - Dec 1, 2025</font></center>\n",
    "<center><font size=\"3\">Aalto University</font></center>\n",
    "</div>\n",
    "\n",
    "\n",
    "<a id='TOC'></a>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "# Table of contents\n",
    "* <a href='#1.'> 1. Introduction </a>\n",
    "* <a href='#1.1'> 1.1 Learning Objectives </a>\n",
    "* <a href='#1.2'> 1.2 Code Structure & Files </a>\n",
    "* <a href='#2.'> 2. Cartpole</a>\n",
    "* <a href='#3.'> 3. Reacher</a>\n",
    "* <a href='#4.'> 4. Submitting </a>\n",
    "* <a href='#4.1'> 4.1 Feedback </a>\n",
    "* <a href='#5.'> References</a>\n",
    "\n",
    "<a href='#T1'><b>Student Task 1.</b> Training a Model for Simple Cartpole Environment (10 points)</a>\\\n",
    "<a href='#Q1'><b>Student Question 1.1</b> Learning (10 points)</a>\\\n",
    "<a href='#T2'><b>Student Task 2.</b> Investigating Training Performance (12 points) </a>\\\n",
    "<a href='#Q2'><b>Student Question 2.1</b> Analysis of Training Performance (15 points) </a>\\\n",
    "<a href='#Q3'><b>Student Question 2.2</b> Stochasticity (10 points) </a>\\\n",
    "<a href='#T3'><b>Student Task 3.</b> Reward Functions (20 points) </a>\\\n",
    "<a href='#T4'><b>Student Task 4.</b> Visualizing Behavior </a>\\\n",
    "<a href='#Q4'><b>Student Question 4.1</b> Achieved Peformance (5 points)</a>\\\n",
    "<a href='#Q5'><b>Student Question 4.2</b> Analysis of Behaviour (9 points)</a>\\\n",
    "<a href='#Q6'><b>Student Question 4.3</b> Checking your knowledge about RL (9 points)</a>\n",
    "    \n",
    "**Total Points:** 100\n",
    "\n",
    "**Estimated runtime of all the cells:** 30 minutes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ab28e9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ecb4bed28908246a16850b56315b8e8e",
     "grade": false,
     "grade_id": "cell-afbabf910b8ed9fd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# 1. Introduction <a id='1.'></a>\n",
    "In this exercise we will take a first look at a reinforcement learning environment, its components and modify the reward function of a simple agent.\n",
    "\n",
    "In this notebook two environments are used: Cartpole and Reacher. The cartpole environment is taken from [OpenAI's Gym library](https://www.gymlibrary.dev/). The reacher environment is custom made (and defined in ```reacher.py```) but utilizes the Gym API.\n",
    "\n",
    "## 1.1 Learning Objectives: <a id='1.1'></a>\n",
    "- To become familiar with assignment structure and the agent-environment relationshp\n",
    "- To understand the effects of stochasticity\n",
    "- To understand and explore the effects of task definition\n",
    "\n",
    "## 1.2 Code Structure & Files <a id='1.2'></a>\n",
    "\n",
    "The ```train.py``` file instantiates the environment and the RL agent that acts in it. The ```agent.py``` file contains the implementation of a simple reinforcement learning agent; for the sake of this exercise, you can assume it to be a black box (you don‚Äôt need to understand how it works, although you are encouraged to study it in more detail). You don‚Äôt have to edit any other file other than ```ex1.ipynb``` to complete this exercise.\n",
    "```\n",
    "‚îú‚îÄ‚îÄ‚îÄcfg                  # Config files for environments e.g. define the maximum number of steps in an episode.\n",
    "‚îú‚îÄ‚îÄ‚îÄimgs                 # Images used in notebook\n",
    "‚îú‚îÄ‚îÄ‚îÄresults \n",
    "‚îÇ   ‚îú‚îÄ‚îÄ‚îÄlogging          # Contains logged data\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ‚îÄmodel            # Contains the policies learned\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ‚îÄvideo            # Contains videos for each environment\n",
    "‚îÇ       ‚îî‚îÄ‚îÄ‚îÄCartPole-v0\n",
    "‚îÇ       ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄtest      # Videos saved during testing\n",
    "‚îÇ       ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄtrain     # Videos saved during training\n",
    "‚îÇ       ‚îî‚îÄ‚îÄ‚îÄSpinningReacher-v0\n",
    "‚îÇ           ‚îî‚îÄ‚îÄ‚îÄtest\n",
    "‚îÇ           ‚îî‚îÄ‚îÄ‚îÄtrain\n",
    "‚îÇ   ex1.ipynb            # Main assignment file containing tasks <---------\n",
    "‚îÇ   feedback.ipynb       # Please give feedback in here\n",
    "‚îÇ   README.ipynb         # This file\n",
    "‚îÇ   agent.py             # Contains functions that govern the policy\n",
    "‚îÇ   reacher.py           # Defines the reacher environment\n",
    "‚îÇ   train.py             # Contains training and testing functions\n",
    "‚îÇ   utils.py             # Contains useful functions \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100c1cfb",
   "metadata": {},
   "source": [
    "Please consult ```README.md``` for more details the assignments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28556beb",
   "metadata": {},
   "source": [
    "## Warnings:\n",
    "\n",
    "- Don‚Äôt copy and paste cells within a notebook. This will mess up the tracking metadata and prevent autograding from working.\n",
    "- Only add new cells using the '+' button in the upper toolbar and do not split cells.\n",
    "- Be cautious about things such as copying the whole notebook to Colab to work on it. This has sometimes resulted in removing all notebook metadata, making autograding impossible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f097ab74",
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_training = False  # Set this flag to True before validation and submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc004e57",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ecf78d754d0b1f6eb047222f9e14a0e4",
     "grade": true,
     "grade_id": "cell-8fb587d5abdec975",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cb8a88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path # to find directory\n",
    "work_dir = Path().cwd()/'results'\n",
    "import os\n",
    "\n",
    "import train as t # for training\n",
    "import utils as u # helper functions\n",
    "\n",
    "import numpy as np # The numpy library can be used for math functions\n",
    "import torch # Used to manage policy and learning\n",
    "from IPython.display import Video, display, HTML # to display videos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e36a57",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6468fd86adbbe289e7e8853d2b1cdf8b",
     "grade": false,
     "grade_id": "cell-56c1faa5635120ab",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# 2. Cartpole <a id='2.'></a>\n",
    "\n",
    "The Cartpole environment consists of a cart and a pole mounted on top of it, as shown in Figure 1. The cart can move either to the left or to the right. The goal is to balance the pole in a vertical position in order to prevent it from falling down. The cart should also stay within limited distance from the center (trying to move outside screen boundaries is considered a failure).\n",
    "\n",
    "<figure>\n",
    "<img src=\"imgs/cartpole.png\" width=\"400px\">\n",
    "<figcaption style=\"text-align: center\"> Figure 1: The Cartpole environment  </figcaption>\n",
    "</figure>\n",
    "\n",
    "The state and the observation are four element vectors:\n",
    "\n",
    "$$\n",
    "o=s=\\left(\\begin{array}{c}\n",
    "x \\\\\n",
    "\\dot{x} \\\\\n",
    "\\theta \\\\\n",
    "\\dot{\\theta}\n",
    "\\end{array}\\right) \\text {, }\n",
    "$$\n",
    "\n",
    "where $x$ is the position of the cart, $\\dot{x}$ is its velocity, $\\theta$ is the angle of the pole w.r.t. the vertical axis, and $\\dot{\\theta}$ is the angular velocity of the pole.\n",
    "\n",
    "In the standard formulation, a reward of 1 is given for every timestep the pole remains balanced. Upon failing (the pole falls) or completing the task, an episode is finished.\n",
    "\n",
    "The training script will record videos of the agent‚Äôs learning progress during training, and the recorded videos are saved to ```results/video/CartPole-v0/train```. By default, the training information is saved to ```results/logging/CartPole-v0_{seed}.csv```. When the training is finished, the models are saved to ```results/model/Cartpole-v0_params.pt```. Videos of the agent‚Äôs behaviour during testing are saved to ```results/video/CartPole-v0/test```."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5bb7ba",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a1c02d0e9b975ce9813ef40bd06998d0",
     "grade": false,
     "grade_id": "cell-4902974772926f28",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "<a id='T1'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Task 1.</b> Training a Model for Simple Cartpole Environment (10 points) </h3> \n",
    "\n",
    "This task requires you to train a model for the cartpole environment with 100 timesteps per episode. Then test the model for 500 timesteps and report average reward. To do this, you can simply run the code in the cells below. \n",
    "\n",
    "To see a full list of options that can be passed through ```cfg_args``` consult the configuation file found in ```cfg/```.\n",
    "\n",
    "- **1st** Run training over 100 steps per episode by using ```t.train``` function. See the cell below. The training will run for 500 episodes automatically.\n",
    "- **2nd:** Export the training plot ```episodesep_reward``` from logged data (.csv format).\n",
    "- **3rd:** Run testing over 500 steps by using ```t.test``` function. See the cell below. See the cell below. Notice ```max_episode_steps``` parameter. \n",
    "- **4th:** **Manually** report the average reward after the cells have completed execution.\n",
    "    \n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0481176",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "    t.train(cfg_path=Path().cwd()/'cfg'/'cartpole_v1.yaml', cfg_args=dict(seed=1, max_episode_steps=100, model_name=\"CartPole-v1\")) # < 5 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8cd415-c26a-44cf-9f76-f80237cdd918",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "    u.plot_reward(Path().cwd()/'results'/'logging'/'CartPole-v1_1.csv', 'CartPole')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0230ba0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4b2dc935e91cccb3f0345d4c423e7190",
     "grade": false,
     "grade_id": "cell-64fbeb2b60de708a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The command below will evaluate the trained model in 10 episodes and report the average reward (and episode length) for these 10 episodes. Do not delete the cell below as it is used for grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95418d8d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8b492043c94a6382f5ff92b768d1cc89",
     "grade": true,
     "grade_id": "cell-8b1bd0d092f865b7",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture --no-stdout\n",
    "assert t.test(episodes=10, cfg_path=Path().cwd()/'cfg'/'cartpole_v1.yaml', \n",
    "        cfg_args=dict(testing=True, save_video=(not skip_training), save_model=False, seed=None, max_episode_steps=500,model_name=\"CartPole-v1\")) > 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7829f371",
   "metadata": {},
   "source": [
    "Report below the average reward after testing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18da8596",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e556373c147e8173d8451bd1c237691e",
     "grade": true,
     "grade_id": "cell-24f7f8d91599a232",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a689b7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9f2c06a70d499a71cbcb437b5400a3c1",
     "grade": false,
     "grade_id": "cell-17b4eb8d711f2770",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The agent acting in the environment can be seen using the following command. Change the ```path``` to pick the episode you want to visualize. Bear in mind by default video saving for training is taken every 50 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7bc0ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "    # Train Result\n",
    "    video_dir = work_dir/'video'/'CartPole-v1'/'train'\n",
    "    \n",
    "    # List all MP4 files in the directory\n",
    "    mp4_files = [file for file in os.listdir(video_dir) if file.endswith(\".mp4\")]\n",
    "    frame_colors = ['#FF5733', '#33FF57', '#5733FF', '#FFFF33', '#33FFFF', '#FF33FF']\n",
    "    # Display each MP4 file\n",
    "    for i, mp4_file in enumerate(mp4_files):\n",
    "        video_path = os.path.join(video_dir, mp4_file)\n",
    "        video = Video(video_path, embed=True, html_attributes=\"loop autoplay\", width=200, height=100)\n",
    "        frame_color = frame_colors[i % len(frame_colors)]\n",
    "        video_frame = HTML(f'<div style=\"width: 200px; height: 100px;; border: 1px solid #FF5733;\">{video._repr_html_()}</div>')\n",
    "        print(\"test/\",mp4_file)\n",
    "        display(video_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f0e835-79ad-47c7-9984-bfc705927d7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "    # Test Result\n",
    "    \n",
    "    video_dir = work_dir/'video'/'CartPole-v1'/'test'\n",
    "    \n",
    "    # List all MP4 files in the directory\n",
    "    mp4_files = [file for file in os.listdir(video_dir) if file.endswith(\".mp4\")]\n",
    "    frame_colors = ['#FF5733', '#33FF57', '#5733FF', '#FFFF33', '#33FFFF', '#FF33FF']\n",
    "    # Display each MP4 file\n",
    "    for i, mp4_file in enumerate(mp4_files):\n",
    "        video_path = os.path.join(video_dir, mp4_file)\n",
    "        video = Video(video_path, embed=True, html_attributes=\"loop autoplay\", width=200, height=100)\n",
    "        frame_color = frame_colors[i % len(frame_colors)]\n",
    "        video_frame = HTML(f'<div style=\"width: 200px; height: 100px;; border: 1px solid #5733FF;\">{video._repr_html_()}</div>')\n",
    "        print(\"test/\",mp4_file)\n",
    "        display(video_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9298a34",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "29f3456bbec5bacc9d93ec415d74e5d3",
     "grade": false,
     "grade_id": "cell-9b1e05f80922c2bf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "<a id='Q1'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Question 1.1</b> Learning (10 points) </h3> \n",
    "\n",
    "Test the trained model from Task 1 five times with different random seeds. Did the same model, trained to balance for 100 timesteps, learn to always balance the pole for 1000 timesteps? Why/why not?\n",
    "    \n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76718b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the trained model by setting 5 different random seeds chosen by you\n",
    "#Type 5 different  random seeds  in eval_seeds array\n",
    "eval_seeds =  [ ,  ,  ,  ,  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1549ae65",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "55507b4aacb261bb5e0c39e9fb9933f9",
     "grade": true,
     "grade_id": "cell-a885538088286a64",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert len(set(eval_seeds)) == 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea940e9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ebb50086ad7448789468612578cd825d",
     "grade": true,
     "grade_id": "cell-2f9ca7392eabd6d2",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%capture --no-stdout\n",
    "def question1_1_evaluate1(eval_seeds):\n",
    "    mean_reward = []\n",
    "\n",
    "    for seed in eval_seeds:\n",
    "        acc_rewards = t.test(episodes=1, cfg_path=Path().cwd()/'cfg'/'cartpole_v1.yaml', \n",
    "            cfg_args=dict(testing=True,save_video=False, save_logging=False, save_model=False, \n",
    "            seed=seed, max_episode_steps=1000, model_name=\"CartPole-v1\"))\n",
    "        mean_reward.append(acc_rewards)\n",
    "\n",
    "    return np.mean(mean_reward)\n",
    "\n",
    "assert question1_1_evaluate1(eval_seeds) > 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d817076",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a8a797866432c49e79a07cbe95f60a7d",
     "grade": false,
     "grade_id": "cell-34afac26642e2bc4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Question:\n",
    "Did the same model consistently balance the pole for 1000 timesteps? There are multiple correct answers. Choose the one you think is most appropriate.\n",
    "\n",
    "**Options:**\n",
    "Choose **only one**:\n",
    "\n",
    "1. No, the agent generally failed around 100-200 timesteps because it encountered unfamiliar states.\n",
    "\n",
    "2. Yes, the agent balanced the pole for 1000 timesteps in every test due to a perfectly learned policy.\n",
    "\n",
    "3. No, although the agent sometimes balanced the pole for 1000 timesteps, it typically failed due to limited generalization to new states.\n",
    "\n",
    "4. No, the agent frequently moved out of bounds or the pole tilted too much, showing poor adaptation.\n",
    "\n",
    "5. Yes, the agent demonstrated perfect control in all tests due to effective training.\n",
    "\n",
    "6. No, the agent's performance varied significantly with different random seeds, indicating inconsistency in learned behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b571667",
   "metadata": {},
   "outputs": [],
   "source": [
    "sq1_1 = None # Answer question 1.1 with the appropriate answer number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d85d22",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "11c62ac2a464b078e231c9be051ec0cc",
     "grade": true,
     "grade_id": "cell-c0b2457bfc16e0b6",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert sq1_1 in range(1,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b1a1ca",
   "metadata": {},
   "source": [
    "The following cells are used for grading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14c81c7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d5b622a3c44df9ba7ba3b01cfcc246d6",
     "grade": true,
     "grade_id": "cell-c5e675812f807564",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f3b5ed9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "404aae2a11c55e21c7625ae8a845b515",
     "grade": false,
     "grade_id": "cell-6fdccf7da0340917",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='T2'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Task 2.</b> Investigating Training Performance (12 points) </h3> \n",
    "\n",
    "Repeat the experiment in Task 1 five times, each time training the model from scratch with 100 timesteps and testing it for 1000 timesteps. Use a different seed number for each training/testing cycle. You can use the box below to write a small script to do this. Use the result textbox below to report the average test reward for each repeat. \n",
    "    \n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8b9768",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0e6bbcf84dee7d5a7e4e77f40fef39da",
     "grade": false,
     "grade_id": "cell-52c4064985fce5f1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Make sure to change the model_name for each agent to the ones given and and that the respective parameter file is saved in results/model/\n",
    "\n",
    "In order to get the points for this exercise each of the agents must have a test accuracy of higher than 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cb1f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# uncomment if you want to skip the output\n",
    "\n",
    "if not skip_training:\n",
    "    model_names = [\"CartPole-v1-model0\", \"CartPole-v1-model1\", \"CartPole-v1-model2\", \"CartPole-v1-model3\", \"CartPole-v1-model4\"]\n",
    "    model_seeds = [1, 1, 1, 1, 1]\n",
    "\n",
    "    assert len(set(model_seeds)) == 5\n",
    "    '''\n",
    "    TODO: Repeat the experiment in Task 1 five times\n",
    "    '''\n",
    "    ########## Your code starts here ##########\n",
    "\n",
    "    ########## Your code ends here ##########\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0106a2db",
   "metadata": {},
   "source": [
    "Do not delete the cell below as it is used for grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b59f4fd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "27f80c6109a365207abe48d1a8553f27",
     "grade": true,
     "grade_id": "cell-410200f55bb863ed",
     "locked": true,
     "points": 12,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture --no-stdout\n",
    "test_seeds = [700, 800, 900, 1000, 2000]\n",
    "\n",
    "for i in range(5):\n",
    "    for seed in test_seeds:\n",
    "        assert t.test(episodes=1, cfg_path=Path().cwd()/'cfg'/'cartpole_v1.yaml', \n",
    "            cfg_args=dict(testing=True, seed=seed,save_video=False, save_logging=False, \n",
    "            save_model=False, max_episode_steps=1000, model_name=str(\"CartPole-v1-model\"+str(i)))) > 100\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5efc121-7d09-4645-804c-25015f5dea2a",
   "metadata": {
    "tags": []
   },
   "source": [
    "**DOUBLE CLICK HERE TO EDIT, CLEAR THIS TEXT AND REPORT HERE**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ff4274",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4a996d7ff3ccc16e98ef6c64a247a59e",
     "grade": false,
     "grade_id": "cell-7b64e24c45653faa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='Q2'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Question 2.1</b> Analysis of Training Performance (15 points) </h3> \n",
    "\n",
    "Are the behavior and performance of the trained models the same every time? Why/why not? Analyze the causes briefly. Hint: the random seed initializes the weights and the environment settings randomly.\n",
    "    \n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee2bcdb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "397e056ea791b621a4248328ad689f75",
     "grade": false,
     "grade_id": "cell-7af2d94176449236",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Select all the correct answers. You get 3 points for each correct answer. Select at **most** 6 answers, or you will **lose all points**.\n",
    "\n",
    "1. Yes, because the model has been optimized to always reach the highest possible reward regardless of random seeds or initial conditions.\n",
    "\n",
    "2. No, because the training dynamics and the evaluating dynamics are not the same\n",
    "\n",
    "3. No, because the trained model has stochastic behaviour which may perform differently in different environments\n",
    "\n",
    "4. No, the behaviour and performance are not the same, as shown by the different average rewards in previous Task. \n",
    "\n",
    "5. No, because there‚Äôs a lot of stochasticity involved in the training process\n",
    "\n",
    "6. No, because agent explores with random actions during training\n",
    "\n",
    "7. No, because policy is initially randomly initialised with random weights, gradient updates of the policy are noisy. \n",
    "\n",
    "8. No, because the environment may be stochastic: agent is randomly initialised in beginning of each episode, (transitions from one state to next one may follow a probability distribution)\n",
    "\n",
    "9. Yes, all machine learning models perform consistently after training due to the deterministic nature of their algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c0e6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sq2_1 = [] # Answer question 2.1 with the appropriate answer numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f08826",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8984880c713f3f91b66fe44e266ed251",
     "grade": true,
     "grade_id": "cell-722a5d335b120083",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert 1 <= len(set(sq2_1)) <= 6\n",
    "assert set(sq2_1) < set(range(1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5011bb0",
   "metadata": {},
   "source": [
    "Do not remove the following blocks as they are used for grading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea13a03",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ecec9d4c828f9bed69a14039122d01f8",
     "grade": true,
     "grade_id": "cell-c78029cac9905a77",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2fbbce",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ee347f3560f406aeae33d04d56afa907",
     "grade": true,
     "grade_id": "cell-9d6e251e5ab4caaa",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfded39b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b68ba72b4beaba16e6a1a23bd6c7af06",
     "grade": true,
     "grade_id": "cell-0cc6e8153c39e73f",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5871d663",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "119a95a042c705feb85c5dc20e6f7143",
     "grade": true,
     "grade_id": "cell-0622d1b37ab64c5f",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58c5f09",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2482be09b178df1a0623964d5ca75e57",
     "grade": true,
     "grade_id": "cell-cf801ba86d95dc4f",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b248abf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dc5ef08285c80b3826ba67701c70dede",
     "grade": false,
     "grade_id": "cell-e0d965bc4eca7b76",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='Q3'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Question 2.2</b> Stochasticity (10 points) </h3> \n",
    "\n",
    "What are the implications of this stochasticity, when it comes to comparing reinforcement learning algorithms to each other?\n",
    "    \n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b41e22",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "626187977d5460c138a08a7220fa18a3",
     "grade": false,
     "grade_id": "cell-20eea1ddc122c6c4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Select **only one**:\n",
    "\n",
    "1. It is only necessary to compare the maximum rewards achieved by each algorithm to determine the best one.\n",
    "\n",
    "2. Stochasticity requires that algorithms be trained and evaluated multiple times to obtain a reliable estimate of performance, including measures of variance besides the average reward.\n",
    "\n",
    "3. Algorithms should be trained once under identical conditions to ensure a fair comparison, focusing on the consistency of the training process.\n",
    "\n",
    "4. The best approach is to evaluate algorithms based on their performance in a single, well-designed test to avoid the confounding effects of random variability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddea5e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "sq2_2 = None # Answer question 2.2 with the appropriate answer number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fb2651",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8e55384a770d91c7f192f6daeea33c89",
     "grade": true,
     "grade_id": "cell-f97b7298905094bd",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert sq2_2 in range(1,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d5d136",
   "metadata": {},
   "source": [
    "The following cells are used for grading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f38a7a5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3c9b327530544c6d2be97f448098ae51",
     "grade": true,
     "grade_id": "cell-390e4a656c3243a8",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "842afa68",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c77d78adb7ac2e8073a9ea95d4ddece4",
     "grade": false,
     "grade_id": "cell-18b846ee50e61942",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Reacher <a id='3.'></a>\n",
    "\n",
    "Now we will focus on designing a reward function for a different environment, the Reacher environment, where a two-joint manipulator needs to reach a goal (see Figure 2).\n",
    "\n",
    "<figure>\n",
    "<img src=\"imgs/reacher.png\" width=\"200px\">\n",
    "<figcaption style=\"text-align: center\"> Figure 2: The Reacher environment  </figcaption>\n",
    "</figure>\n",
    "\n",
    "The Cartesian ($x$, $y$) position of the end-effector of the manipulator can be determined following the equation:\n",
    "\n",
    "$$\n",
    " x = L_1 \\sin(\\theta_0)+L_2 \\sin(\\theta_0+\\theta_1)\\\\\n",
    " y = -L_1 \\cos(\\theta_0)-L_2 \\cos(\\theta_0+\\theta_1)\n",
    "$$\n",
    "\n",
    "where $L1 = 1$, $L2 = 1$ are the lengths, and $\\theta_0$, $\\theta_1$ the joint angles of the first and second links respectively. The state (and observation) in this environment is the two element vector:\n",
    "\n",
    "$$\n",
    "o=s=\\left(\\begin{array}{c}\n",
    "\\theta_0 \\\\\n",
    "\\theta_1 \\\\\n",
    "\\end{array}\\right) \\text {, }\n",
    "$$\n",
    "\n",
    "The action space now consists of 5 \"options\"; 4 correspond rotating the first/second joint left/right, and the final one performs no motion at all (the configuration doesn‚Äôt change). The episode terminates when the agent reaches the target position, marked in red. Now, let us design a custom reward function and use it for training the RL agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e138b75d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ff94517d52449f9ad78ed836e8ae5056",
     "grade": false,
     "grade_id": "cell-47ba07fead317fd8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='T3'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Task 3.</b> Reward Functions (20 points) </h3> \n",
    "\n",
    "Below two classes are shown that modify the reward function of the reacher function provided in ```reacher.py```. Edit the function ```get_reward``` _below_ (not in ```reacher.py```) in both classes. For each class, write a reward function to incentivise the agent to learn the following behaviors:\n",
    "\n",
    "Class 1) ```SpinningReacherEnv```: Keep the manipulator rotating clockwise continuously (w.r.t. angle Œ∏_0). You can use a lower number of training episodes for this, e.g. train(cfg_args=dict(env_name='SpinningReacher-v0', train_episodes=200), overrides=['env=reacher_v1'])\n",
    "\n",
    "Class 2) ```TargetReacherEnv```: Reach the goal point located in x = [1.0,1.0] (marked in red). Use at least 500 training episodes.\n",
    "    \n",
    "Train one model for each behavior. \n",
    "\n",
    "**Hint:** Use the observation vector to get the quantities required to compute the new reward (such as the position of the manipulator). You can get the Cartesian position of the end-effector with ```self.get_cartesian_pos(state)```.\n",
    "            \n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bf76fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from reacher import ReacherEnv\n",
    "from typing import Optional\n",
    "from gymnasium.envs.registration import register\n",
    "\n",
    "class SpinningReacherEnv(ReacherEnv):\n",
    "    def __init__(self, render_mode: Optional[str] = None, max_episode_steps=200):\n",
    "        super().__init__(render_mode=render_mode, max_episode_steps=max_episode_steps)\n",
    "        \n",
    "    def get_reward(self, prev_state, action, next_state):\n",
    "        '''\n",
    "        TODO: Task 3: Implement and test the first reward function\n",
    "        '''\n",
    "        ########## Your code starts here ##########\n",
    "        return 1\n",
    "        ########## Your codes end here ########## \n",
    "        \n",
    "    \n",
    "register(\"SpinningReacher-v0\",\n",
    "        entry_point=\"%s:SpinningReacherEnv\"%__name__,\n",
    "        max_episode_steps=200)\n",
    "\n",
    "class TargetReacherEnv(ReacherEnv):\n",
    "    def __init__(self, render_mode: Optional[str] = None, max_episode_steps=200):\n",
    "        super().__init__(render_mode=render_mode, max_episode_steps=max_episode_steps)\n",
    "        \n",
    "    def get_reward(self, prev_state, action, next_state):\n",
    "        '''\n",
    "        # TODO: Task 3: Implement and test the second reward function\n",
    "        '''\n",
    "        ########## Your code starts here ##########\n",
    "        return 1\n",
    "        ########## Your codes end here ########## \n",
    "        \n",
    "register(\"TargetReacher-v0\",\n",
    "        entry_point=\"%s:TargetReacherEnv\"%__name__,\n",
    "        max_episode_steps=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765ea45c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "      t.train(cfg_path=Path().cwd()/'cfg'/'reacher_v1.yaml', \n",
    "      cfg_args=dict(env_name='SpinningReacher-v0',model_name='SpinningReacher-v0', train_episodes=200, seed=1)) # < 5 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4cb332",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "       t.test(episodes=10, cfg_path=Path().cwd()/'cfg'/'reacher_v1.yaml', \n",
    "       cfg_args=dict(env_name='SpinningReacher-v0',model_name='SpinningReacher-v0', testing=True,seed=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383507bc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ae28a94ff0891a34fcdffc954b90cda2",
     "grade": false,
     "grade_id": "cell-06f756f42a921878",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The agent acting in the environment can be seen using the following command. Change the ```path``` to pick the episode you want to visualize. Bear in mind by default video saving for training is taken every 50 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555bc002",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "      video = Video(work_dir/'video'/'SpinningReacher-v0'/'test'/f'ex1-episode-0.mp4',\n",
    "      embed=True, html_attributes=\"loop autoplay\") # Set html_attributes=\"controls\" for video control\n",
    "      display(video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663c258a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "      t.train(cfg_path=Path().cwd()/'cfg'/'reacher_v1.yaml', \n",
    "      cfg_args=dict(env_name='TargetReacher-v0',model_name='TargetReacher-v0', train_episodes=500, seed=1)) # < 5 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4714194c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "       t.test(episodes=10, cfg_path=Path().cwd()/'cfg'/'reacher_v1.yaml', \n",
    "       cfg_args=dict(env_name='TargetReacher-v0',model_name='TargetReacher-v0', seed=None, testing=True,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67152513",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "      video = Video(work_dir/'video'/'TargetReacher-v0'/'test'/f'ex1-episode-0.mp4',\n",
    "      embed=True, html_attributes=\"loop autoplay\") # Set html_attributes=\"controls\" for video control\n",
    "      display(video)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947add89",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "24b4e1b132030de471dd3ab7d7d89a7b",
     "grade": false,
     "grade_id": "cell-8fbe6b6920b12c52",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Do not delete the following cells as they are used for grading. Make sure to test that both agents work correctly for the selected set of seeds [860, 241, 73, 543, 582, 211, 524, 484, 954, 656]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141e44fe",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8d262615dcd66ffad7534f4c0c2a5424",
     "grade": true,
     "grade_id": "cell-626f99384bd58109",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%capture --no-stdout\n",
    "\"TEST CELL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee03e99",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "80c6477f2fe781715a9c816f5434d617",
     "grade": true,
     "grade_id": "cell-233242cce6bb1187",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%capture --no-stdout\n",
    "\"TEST CELL\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c384e863",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b3bb72081ab68915f7c37a6a93a897ee",
     "grade": false,
     "grade_id": "cell-54aa2850dfdfb7d1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='T4'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Task 4.</b> Visualizing Behavior </h3> \n",
    "\n",
    "Now, let us visualize the reward function for the second behavior (reaching the goal [1,1]). Plot the values of the second reward function from Task 3 and the learned best action as a function of the state (the joint positions). Use the code below as a starting point. After plotting, answer the questions below.\n",
    "            \n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139b3387",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gymnasium as gym\n",
    "from agent import Agent, Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0035a7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"TargetReacher-v0\" \n",
    "resolution = 101  # Resolution of the policy/reward image\n",
    "\n",
    "# Load policy from default path to plot\n",
    "policy_dir = Path().cwd()/'results'/'model'/f'{env_name}_params.pt'\n",
    "\n",
    "sns.set()\n",
    "\n",
    "# Create a gym environment\n",
    "env = gym.make(env_name)\n",
    "\n",
    "action_space_dim = u.get_space_dim(env.action_space)\n",
    "observation_space_dim = u.get_space_dim(env.observation_space)\n",
    "policy = Policy(observation_space_dim, action_space_dim)\n",
    "\n",
    "if policy_dir:\n",
    "    policy.load_state_dict(torch.load(policy_dir))\n",
    "    print(\"Loading policy from\", policy_dir)\n",
    "else:\n",
    "    print(\"Plotting a random policy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170a06be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a grid and initialize arrays to store rewards and actions\n",
    "npoints = resolution\n",
    "state_range = np.linspace(-np.pi, np.pi, npoints)\n",
    "rewards = np.zeros((npoints, npoints))\n",
    "actions = np.zeros((npoints, npoints), dtype=np.int32)\n",
    "\n",
    "# Loop through state[0] and state[1]\n",
    "for i,th1 in enumerate(state_range):\n",
    "    for j,th2 in enumerate(state_range):\n",
    "        # Create the state vector from th1, th2\n",
    "        state = np.array([th1, th2])\n",
    "\n",
    "        # Query the policy and find the most probable action\n",
    "        with torch.no_grad():\n",
    "            action_dist, _ = policy(torch.from_numpy(state).float().unsqueeze(0))\n",
    "        action_probs = action_dist.probs.numpy()\n",
    "\n",
    "        '''\n",
    "        # TODO: Task 4: \n",
    "        # 1. What's the best action, according to the policy?\n",
    "        # 2. Compute the reward given state\n",
    "        '''\n",
    "        ########## Your code starts here ##########\n",
    "        # Use the action probabilities in the action_probs vector\n",
    "        # (it's a numpy array)\n",
    "        actions[i, j] = 0\n",
    "        \n",
    "        rewards[i,j] = 0 \n",
    "        ########## Your code ends here ##########\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea36aa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the reward plot\n",
    "num_ticks = 10\n",
    "tick_skip = max(1, npoints // num_ticks)\n",
    "tick_shift = 2*np.pi/npoints/2\n",
    "tick_points = np.arange(npoints)[::tick_skip] + tick_shift\n",
    "tick_labels = state_range.round(2)[::tick_skip]\n",
    "\n",
    "sns.heatmap(rewards)\n",
    "plt.xticks(tick_points, tick_labels, rotation=45)\n",
    "plt.yticks(tick_points, tick_labels, rotation=45)\n",
    "plt.xlabel(\"J2\")\n",
    "plt.ylabel(\"J1\")\n",
    "plt.title(\"Reward\")\n",
    "plt.suptitle(\"Rewards in %s\" % env_name)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f6c89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create the policy plot\n",
    "cmap = sns.color_palette(\"deep\", action_space_dim)\n",
    "sns.heatmap(actions, cmap=cmap, vmin=0, vmax=action_space_dim-1)\n",
    "plt.xticks(tick_points, tick_labels, rotation=45)\n",
    "plt.yticks(tick_points, tick_labels, rotation=45)\n",
    "colorbar = plt.gca().collections[0].colorbar\n",
    "ticks = np.array(range(action_space_dim))*((action_space_dim-1)/action_space_dim)+0.5\n",
    "colorbar.set_ticks(ticks)\n",
    "if env.spec.id == \"Reacher-v1\":\n",
    "    # In Reacher, we can replace 0..4 with more readable labels\n",
    "    labels = [\"J1+\", \"J1-\", \"J2+\", \"J2-\", \"Stop\"]\n",
    "else:\n",
    "    labels = list(map(str, range(action_space_dim)))\n",
    "colorbar.set_ticklabels(labels)\n",
    "plt.xlabel(\"J2\")\n",
    "plt.ylabel(\"J1\")\n",
    "plt.title(\"Best action\")\n",
    "plt.suptitle(\"Best action in %s\" % env_name)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2187e8-19c3-43a3-8854-7c20a9c73f9d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8a786bb3b1a68bc06adae81775588814",
     "grade": false,
     "grade_id": "cell-29064556f36a3c80",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Do not remove this cell as it is used for grading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45dc911",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "38774681f9b8ea8d0c120330f044c23a",
     "grade": false,
     "grade_id": "cell-a90e3e94423fcb0c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='Q4'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Question 4.1</b> Achieved Performance (5 points) </h3> \n",
    "\n",
    "Where should the reward be higher for the reaching task? \n",
    "            \n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7011e57b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b6af53eeeb3490d7b74f6e2ca3a59c9b",
     "grade": false,
     "grade_id": "cell-e19d78f23a6643a3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Select **only one**.\n",
    "1. Reward should be higher when the end-effector is far from the goal.\n",
    "2. Reward should be higher when the end-effector is near the goal. \n",
    "3. Reward should be equal in joint space.\n",
    "4. Reward should have noise to improve exploration and be higher when close to the goal.\n",
    "5. Reward should be calculated based on the history, if end-effector is approaching the goal, it should be higher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21dbcb90-01c8-4825-bc3e-5f28f0bcf965",
   "metadata": {},
   "outputs": [],
   "source": [
    "sq4_1 = None # Answer question 4.1 with the appropriate answer number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77452fd6-23f2-4045-9437-3107a3a50a0b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "67f2ef8d46163f9aae541c7ff63a427b",
     "grade": true,
     "grade_id": "cell-d6a8fcabc4c42c92",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert sq4_1 in range(1, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783b614d-8f10-42ed-980e-c2def5d66181",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "08f817d52414224e40facb15ed356d59",
     "grade": true,
     "grade_id": "cell-385126983a8fd92d",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f77cbd95",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9114aa69847c9c2be97907cfa2af71a4",
     "grade": false,
     "grade_id": "cell-4346eec80fe675af",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='Q5'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Question 4.2</b> Analysis of Behaviour (9 points) </h3> \n",
    "\n",
    "Did the policy learn to reach the goal from every possible state (manipulator configuration) in an optimal way (i.e. with lowest possible number of steps)? Why/why not?\n",
    "            \n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b2b7fd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "79a2ad8ef18bc08b07496b71340a0dfb",
     "grade": false,
     "grade_id": "cell-44ab862bff7d27bf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Select all the correct answers. You get 3 points for each correct answer. Select at **most** 4 answers, or you will **lose all points**.\n",
    "\n",
    "1. Yes, the policy efficiently learns to reach the goal from every state due to comprehensive state exploration.\n",
    "\n",
    "2. No, the policy fails to explore all states adequately because the initial and goal states are nearly constant, leading to limited exploration ability.\n",
    "\n",
    "3. Yes, the model is trained to always find the shortest path due to advanced algorithmic efficiency.\n",
    "\n",
    "4. No, because the policy often chooses longer paths for certain initial states , not exploiting shorter alternatives.\n",
    "\n",
    "5. Yes, continuous training ensures the policy adapts to find the best route from any state over time.\n",
    "\n",
    "6. No, optimal paths are not always found because the exploration phase is limited, causing repeated suboptimal actions.\n",
    "\n",
    "7. No, the policy sometimes achieves the goal optimally, but not consistently across all trials and configurations.\n",
    "\n",
    "8. Yes, the training regime guarantees that all possible configurations are optimally addressed by the final model.\n",
    "\n",
    "9. No, the policy cannot reach from all state because the policy has stochastic behavior.\n",
    "\n",
    "10. No, bacause the policy overfits to the online it collects.\n",
    "\n",
    "11. No, bacause the the reward function is not defined in an optimal way for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1621dc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "sq4_2 = [] #Answer question 4.2 with the appropriate answer numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc40e62",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "343ce1ee6a8e4d8c9e31750dca196477",
     "grade": true,
     "grade_id": "cell-44c69f0bc8c72e3f",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert 1 <= len(set(sq4_2)) <= 4\n",
    "assert set(sq4_2) < set(range(1, 12))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eaeb40d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b652e7d56b0faeab487f22b14f619240",
     "grade": false,
     "grade_id": "cell-c1270f97d5ec1f3d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Do not remove the following blocks as they are used for grading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05031d7b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "363e721a50466efc4a48b8eb31060bfc",
     "grade": true,
     "grade_id": "cell-5a73a8d6f1e62a4c",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c7b426",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7ac4e9f13a548e2d539bd454d10ee8b0",
     "grade": true,
     "grade_id": "cell-b18c9cd3dfeb8639",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db031f32",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "27bd6747e4d994a69c36e743a49c9e70",
     "grade": true,
     "grade_id": "cell-20401f4d0e76b565",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c934ffd-63fb-4162-9d90-5682a4d24e0b",
   "metadata": {},
   "source": [
    "<a id='Q6'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Question 4.3</b> Checking your knowledge about RL (9 points) </h3> \n",
    "            \n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d4b6d0-97c7-4627-9742-8f67563bd815",
   "metadata": {},
   "source": [
    "Select all the true statements. You get 3 points for each correct answer. Select at **most** 4 answers, or you will **lose all points**.\n",
    "\n",
    "1. Each trial in reinforcement learning is called an episode. \n",
    "\n",
    "2. Each trial in reinforcement learning should have the same length.\n",
    "\n",
    "3. Reinforcement learning always requires a model of the environment.\n",
    "\n",
    "4. Once the agent learns, it never makes mistakes again.\n",
    "\n",
    "5. Exploitation means choosing the action that seems best based on past experience.\n",
    "\n",
    "6. Optimal policies are always found in reinforcement learining. \n",
    "\n",
    "7. There is always only one optimal policy.\n",
    "\n",
    "8. The exploration‚Äìexploitation trade-off is central to reinforcement learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054d3397-37f7-4c08-8291-e285b80ec907",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sq4_3 = [] #Answer question 4.2 with the appropriate answer numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8127189-016c-4e5a-b90f-4040047c63cb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "827fe7185982bb28d0d7593ccea30093",
     "grade": true,
     "grade_id": "cell-f6aef306d8725802",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert 1 <= len(set(sq4_3)) <= 4\n",
    "assert set(sq4_3) < set(range(1, 9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7241aea4-ff50-4d85-a502-0d1df5d78627",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7fb4f785894e812af8f5a4f71dbc3a48",
     "grade": true,
     "grade_id": "cell-9b87d4e533b58f65",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3054efbc-e7c1-4583-ad3b-5b7c04ce7958",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "823ca0672ba2112c1554c928516e9318",
     "grade": true,
     "grade_id": "cell-582e8b9d6a11c222",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090291db-697d-495e-850e-d184a04c8224",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f8e2c7de06cbb3496a3346d3e652dcd8",
     "grade": true,
     "grade_id": "cell-ffc63b0be74b0da6",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c50da4d",
   "metadata": {},
   "source": [
    "# 5. Submitting <a id='4.'></a>\n",
    "Ensure all tasks and questions (in ```ex1.ipynb```) are answered and the relevant plots are recorded in the relevant places. Details about attaching images and figures can be found below. The relevant graphs to be included for this assignment are:\n",
    "- Task 1, CartPole ```episodesep_reward``` plot from logged csv file\n",
    "- x2 Task 4 reward plots\n",
    "\n",
    "Ensure the correct model files are saved:\n",
    "- results/model/CartPole-v1_params.pt\n",
    "- results/model/CartPole-v1-model{0-4}_params.pt\n",
    "- results/model/SpinningReacher-v0_params.pt\n",
    "- results/model/TargetReacher-v0_params.pt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d551ec",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ec29d723e13463965e0cd445e8d76d21",
     "grade": true,
     "grade_id": "cell-b0b285ad316b6cdf",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Make sure that skip training is set to True before submission\n",
    "assert skip_training == True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c3b7e3-49a1-4ca6-8484-e9fd422271be",
   "metadata": {},
   "source": [
    "## 5.1 Feedback <a id='5.1'></a>\n",
    "\n",
    "In order to help the staff of the course as well as the forthcoming students, it would be great if you could answer to the following questions in your submission:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29381b17-ce64-4a1c-b16c-e82ea2a0af96",
   "metadata": {},
   "source": [
    "1) How much time did you spend solving this exercise? (change the ```hrs``` variable below to a floating point number representing the number of hours taken e.g. 5.43)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc91b59-0048-475d-a679-8e976ba0c7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hrs = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbefa77-e4a4-48ec-8049-e52b40fff463",
   "metadata": {},
   "source": [
    "2) Difficulty of each task/question from 1-5 (int or float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35321f8a-a156-4953-ba84-689b6e53a0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "T1 = None # Student Task 1. Training a simple model for Cartpole environment\n",
    "Q1_1 = None # Student Question 1.1 Learning\n",
    "T2 = None # Student Task 2. Investigating training performance\n",
    "Q2_1 = None # Student Question 2.1 Analyzing the training performance\n",
    "Q2_2 = None # Stochasticity\n",
    "T3 = None # Student Task 3. Reward function\n",
    "T4 = None # Student Task 4. Visualize behavior\n",
    "Q4_1 = None # Student Question 4.1 Achieved performance\n",
    "Q4_2 = None # Student Question 4.2 Analysis of behavior\n",
    "Q4_3 = None # Student Question 4.3 Checking your knowledge about RL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4983ba-5031-460d-bdb3-e7cd708ed203",
   "metadata": {},
   "source": [
    "3) How well did you understand the content of the task/question from 1-5? (int or float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed5d535-89b1-4852-81ef-541a0e1d6dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "T1 = None # Student Task 1. Training a simple model for Cartpole environment\n",
    "Q1_1 = None # Student Question 1.1 Learning\n",
    "T2 = None # Student Task 2. Investigating training performance\n",
    "Q2_1 = None # Student Question 2.1 Analyzing the training performance\n",
    "Q2_2 = None # Stochasticity\n",
    "T3 = None # Student Task 3. Reward function\n",
    "T4 = None # Student Task 4. Visualize behavior\n",
    "Q4_1 = None # Student Question 4.1 Achieved performance\n",
    "Q4_2 = None # Student Question 4.2 Analysis of behavior\n",
    "Q4_3 = None # Student Question 4.3 Checking your knowledge about RL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7670bd2-6bff-418a-bf25-fabf1a3e5396",
   "metadata": {},
   "source": [
    "4) General feedback. Consider questions like:\n",
    "\n",
    "    - Did the content of the lecture relate well with the assignment?\n",
    "    - To what extent did you find the material to be potentially useful for your research and studies?\n",
    "    \n",
    "And other feedback you think is worth including. Type in the box below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463630b4-2c2f-4926-bd4e-f7d280744d5e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4330ed5-2f15-4580-bf05-34e9aaf0004c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# References <a id='5.'></a>\n",
    "\n",
    "[1] Sutton, Richard S., and Andrew G. Barto. \"Reinforcement Learning: An Introduction (in progress).\" London, England (2017). http://incompleteideas.net/book/RLbook2018.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d837ff-dea1-4e5a-84a1-c1771fe296a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
