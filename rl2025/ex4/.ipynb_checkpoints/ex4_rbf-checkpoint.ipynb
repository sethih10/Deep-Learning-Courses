{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53274fb5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "42aa8ce65cf2808c1df90efc58a0a546",
     "grade": false,
     "grade_id": "cell-d0c446e141ba6217",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "   <h2 align=\"center\"> <center><b> Reinforcement Learning Assignment 4 - Function Approximators Part 1: Radial Basis Functions </b></center></h2>\n",
    "\n",
    "<br>\n",
    "<center><font size=\"3\">This notebook is a part of teaching material for ELEC-E8125</font></center>\n",
    "<center><font size=\"3\">Aalto University</font></center>\n",
    "</div>\n",
    "\n",
    "\n",
    "<a id='TOC'></a>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "# Table of contents\n",
    "* <a href='#1.'> 1. Introduction </a>\n",
    "* <a href='#1.1'> 1.1 Task environments </a>\n",
    "* <a href='#1.2'> 1.2 Learning Objectives </a>\n",
    "* <a href='#1.3'> 1.3 Code Structure & Files </a>\n",
    "* <a href='#2.'> 2. Approximate with non-linear features</a>\n",
    "* <a href='#2.1'> 2.1 Radial Basis Functions</a>\n",
    "* <a href='#2.2'> 2.2 Sklearn RBF function</a>\n",
    "* <a href='#3.'> 3. Submitting </a>\n",
    "* <a href='#3.1'> 3.1 Feedback </a>\n",
    "\n",
    "<a href='#T1'><b>Student Task 1.</b> Implementing Q-learning using Function Approximation (20 points) </a>\\\n",
    "<a href='#Q1'><b>Student Question 1.1</b> Considering Linear Features (10 points) </a>\\\n",
    "<a href='#Q2'><b>Student Question 1.2</b> Experience Replay Buffer (10 points) </a>\\\n",
    "<a href='#Q3'><b>Student Question 1.3</b> Grid-based vs Function Approximators (10 points) </a>\\\n",
    "<a href='#T2'><b>Student Task 2.</b> Visualizing the Policy (10 points)</a>\n",
    "    \n",
    "**Total Points:** 60\n",
    "\n",
    "**Estimated runtime of all the cells:** 2 - 3 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30395ea",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7a2b0a701ed47cff056b5cb211ab235e",
     "grade": false,
     "grade_id": "cell-e08152dda643a700",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# 1. Introduction <a id='1.'></a>\n",
    "\n",
    "In many real world scenarios, the dimensionality of the state space may be too high to compute and store the Q-values for each possible state and action in a Q-table. Instead, the state value and action value functions can be learned by using a function approximator, such as a radial basis functions (RBFs). In this exercise, you will start with handcrafted features and then move to radial basis functions in the **Cartpole** environment.\n",
    "\n",
    "\n",
    "<figure style=\"text-align: center\">\n",
    "<img src=\"imgs/cartpole.png\" width=\"300px\">\n",
    "<figcaption style=\"text-align: center\"> Figure 1: The Cartpole environment. </figcaption>\n",
    "</figure>\n",
    "\n",
    "Useful Sources:\n",
    "\n",
    "- Sutton, Richard S., and Andrew G. Barto. \"Reinforcement Learning: An Introduction (2nd Edition).\" London, England (2017). http://incompleteideas.net/book/RLbook2018.pdf\n",
    "- Lecture 4: Function Approximation (Sutton & Barto Ch. 9-9.3, 10-10.1)\n",
    "\n",
    "## 1.1 Task environments <a id='1.1'></a>\n",
    "\n",
    "In this part, we will specifically use RBF-based Q-learning to solve cartpole task. In this environment, a pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum is placed upright on the cart and the goal is to balance the pole by applying forces in the left and right direction on the cart. \n",
    "\n",
    "You can find the detailed description of the task through this link: https://gymnasium.farama.org/environments/classic_control/cart_pole/\n",
    "\n",
    "\n",
    "## 1.2 Learning Objectives: <a id='1.2'></a>\n",
    "- Understand why and how function approximators can be used for Q-learning\n",
    "- Develop intuition behind function approximators: radial basis functions\n",
    "\n",
    "## 1.3 Code Structure & Files <a id='1.3'></a>\n",
    "\n",
    "```ex4_rbf.ipynb``` is the file needed to be modified for this part of the assignment. \n",
    "\n",
    "<span style=\"color:red\"> **# IMPORTANT: DO NOT FORGET ANOTHER TASK IN ```ex4_dqn.ipynb```** </span>\n",
    "\n",
    "```\n",
    "‚îú‚îÄ‚îÄ‚îÄcfg                            # Config files for environments\n",
    "‚îú‚îÄ‚îÄ‚îÄimgs                           # Images used in notebook\n",
    "‚îú‚îÄ‚îÄ‚îÄresults\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ‚îÄCartPole-v1\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ‚îÄlogging                \n",
    "‚îÇ   ‚îÇ   ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄlogging.pkl        # Contains logged data\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ‚îÄmodel              \n",
    "‚îÇ   ‚îÇ   ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ*dqn.pt            # Contains trained model for dqn task\n",
    "‚îÇ   ‚îÇ   ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ*rbf.pkl           # Contains trained model for rbf task\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄvideo                   # Videos saved\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ*cartpole_rbf.png       # Contains training performance plot for rbd\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ*cartpole_hd.png        # Contains training performance plot for handcrafted feature\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ‚îÄLunarLander-v2\n",
    "‚îÇ   ex4_dqn.ipynb                   # 2nd assignment file containing tasks <---------\n",
    "‚îÇ   ex4_rbf.ipynb                   # 1st assignment file containing tasks <---------This task\n",
    "‚îÇ   train.py                        # Contains train and test functions \n",
    "‚îÇ   utils.py                        # Contains useful functions \n",
    "‚îî‚îÄ‚îÄ‚îÄbuffer.py                       # Contains buffer functions\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3c5f5c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "36b40a9e7e6f787b1b792f5f3a7e224a",
     "grade": false,
     "grade_id": "cell-ab51a95c38cfeb41",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# 2. Approximate with non-linear features <a id='2.'></a>\n",
    "## 2.1 Radial Basis Functions <a id='2.1'></a>\n",
    "A radial basis function (RBF) is a real-valued function $\\phi$ that maps a vector $\\boldsymbol{x} \\in \\mathbb{R}^d$ into a real-value $\\phi : \\mathbb{R}^d \\mapsto \\mathbb{R}$. The RBF $\\phi(\\boldsymbol{x} )$ = $\\phi(||\\boldsymbol{x} ||)$ acts on a distance (radial) and a basis function $\\phi$ (e.g. Gaussian function). The RBFs are commonly used as kernels in machine learning. The RBF kernel is defined as a $K(\\boldsymbol{x} , \\boldsymbol{x} ‚Ä≤) = „Äà\\phi(\\boldsymbol{x}),\\phi(\\boldsymbol{x}‚Ä≤)„Äâ$, where $\\phi$ is the Gaussian function. Therefore, the RBF kernel is defined as\n",
    "\n",
    "$$\n",
    "K\\left(\\boldsymbol{x}, \\boldsymbol{x}^{\\prime}\\right)=\\exp \\left(-\\frac{\\left\\|\\boldsymbol{x}-\\boldsymbol{x}^{\\prime}\\right\\|^2}{2 \\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "where the parameter $\\sigma^2$ is the variance and gives shape to the Gaussian function. \n",
    "\n",
    "On the exercise you are given a ```featurizer``` which uses an ```RBFSampler```. The ```RBFSampler``` uses a Monte Carlo approximation of the RBF kernel and generates samples with the specified variance. The ```RBFSampler``` has been trained on a predefined dataset. Thus, the RBF kernel will map the state $x \\in \\mathbb{R}$ d to the distance to the trained dataset. Therefore, an input state will be transformed to as many features as samples $N$ are specified in the ```RBFSampler``` $\\phi : \\mathbb{R}^d \\mapsto \\mathbb{R}^N $. \n",
    "\n",
    "In a nutshell, the ```featurizer``` function maps low-dimensional states to a high-dimensional representation, which reveals properties that haven‚Äôt been shown in the original low-dimensional space, e.g., for the Cartpole environment, it maps the original four-dimensional states to a 230-dimensional vector.\n",
    "\n",
    "## 2.2 Sklearn RBF function<a id='2.2'></a>\n",
    "In this excercise, we use scikit-learn library to implement the RBF approximation by doing following steps:\n",
    "\n",
    "- RBFSampler: A RBF kernel will map feature into higher dimensions using random Fourier features.\n",
    "- FeatureUnion: Concatenates results of multiple transformer objects.\n",
    "- SGDRegressor: Linear model fitted by minimizing a regularized empirical loss with SGD.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8cedc4-14e0-4fb1-b3ba-f26009fa02a4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "47530e48db6901812adae6ba0239bc2d",
     "grade": false,
     "grade_id": "cell-fdd24e05653d4a99",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Warnings:\n",
    "\n",
    "- Don‚Äôt copy and paste cells within a notebook. This will mess up the tracking metadata and prevent autograding from working.\n",
    "- Only add new cells using the '+' button in the upper toolbar and do not split cells.\n",
    "- Be cautious about things such as copying the whole notebook to Colab to work on it. This has sometimes resulted in removing all notebook metadata, making autograding impossible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5511d36c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d50ae4772484f424d02f23d09e82d04f",
     "grade": false,
     "grade_id": "cell-102128ef6321fb44",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='T1'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Task 1.</b> Implementing Q-learning using Function Approximation (20 points) </h3> \n",
    "\n",
    "Implement Q-learning using function approximation. Also implement $\\epsilon$-greedy action selection. Test two different features for state representations:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(a) handcrafted feature vector $\\phi(s) = [s, |s|]^T$,(use the ```featurizer``` inside the ```RBFAgent``` class).\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(b) radial basis function representations (use the ```featurizer``` inside the ```RBFAgent``` class).\n",
    "\n",
    "For this task you need to modify functions ```featurize```, ```get_action```, and ```update``` inside the ```RBFAgent``` class. Test the implementation on the Cartpole environment by running the ```train(agent)``` script with default configs. You can test the trained model with the ```test(agent, cfg_args=dict(save_video=true))``` script. \n",
    "    \n",
    "**See Figure 2 for an example training performance plot for Task1(b). Save the training performance plots for both (a) and (b), and check if they are in the right place (the paths please refer to <a href='#3.'>Submitting<a>).**\n",
    "\n",
    "<figure style=\"text-align: center\">\n",
    "<img src=\"imgs/rbf.png\" width=\"300px\">\n",
    "<figcaption style=\"text-align: center\">Figure 2: The training performance plot for Task 1 (b) might look something like this.</figcaption>\n",
    "</figure>\n",
    "    \n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57105930",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "skip_training = False  # Set this flag to True before validation and submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6ed5fa",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8292581c94100e7cfcfa56cb34739b65",
     "grade": true,
     "grade_id": "cell-21f05aed2ce5f313",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62817342",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(\"..\"))\n",
    "\n",
    "import yaml\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "from buffer import Batch\n",
    "import utils as u\n",
    "import train as t\n",
    "\n",
    "from IPython.display import Video # to display videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ee3fb4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RBFAgent(object):\n",
    "    def __init__(self, n_actions, gamma=0.98, batch_size=32):\n",
    "        self.scaler = None\n",
    "        self.featurizer = None\n",
    "        self.q_functions = None\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.n_actions = n_actions\n",
    "        self._initialize_model()\n",
    "\n",
    "    def _initialize_model(self):\n",
    "        # Draw some samples from the observation range and initialize the scaler (used to normalize data)\n",
    "        obs_limit = np.array([4.8, 5, 0.5, 5],\n",
    "                             dtype=np.float32)\n",
    "        samples = np.random.uniform(-obs_limit, obs_limit,\n",
    "                                    (1000, obs_limit.shape[0])).astype(np.float32)\n",
    "        \n",
    "        # calculate the mean and var of samples, used later to normalize training data\n",
    "        self.scaler = StandardScaler().fit(samples) \n",
    "\n",
    "        # Initialize the RBF featurizer\n",
    "        self.featurizer = FeatureUnion([\n",
    "            (\"rbf1\", RBFSampler(gamma=5.0, n_components=100)),\n",
    "            (\"rbf2\", RBFSampler(gamma=2.0, n_components=80)),\n",
    "            (\"rbf3\", RBFSampler(gamma=1.0, n_components=50)),\n",
    "        ])\n",
    "        self.featurizer.fit(self.scaler.transform(samples))\n",
    "\n",
    "        # Create a value approximator for each action dimension\n",
    "        self.q_functions = [SGDRegressor(learning_rate=\"constant\", max_iter=500, tol=1e-3)\n",
    "                           for _ in range(self.n_actions)]\n",
    "\n",
    "        # Initialize it to whatever values; implementation detail\n",
    "        for q_a in self.q_functions:\n",
    "            q_a.partial_fit(self.featurize(samples), \n",
    "                            np.zeros((samples.shape[0],), dtype=np.float32))\n",
    "\n",
    "    def featurize(self, state):\n",
    "        \"\"\" Map state to a higher dimension for better representation.\"\"\"\n",
    "        if len(state.shape) == 1:\n",
    "            state = state.reshape(1, -1)\n",
    "        \n",
    "        # TODO: Task 1, choose which feature to use. \n",
    "        # Task 1a: implement the manual features here \n",
    "        ########## You code starts here #########\n",
    "        #manual_feature = ...\n",
    "        \n",
    "        manual_feature = np.concatenate([state, np.abs(state)], axis=1)\n",
    "        ########## You code ends here #########\n",
    "\n",
    "        # Task 1b: implement the RBF features transformation here \n",
    "        # it will map a state to a higher dimension (100+80+50)\n",
    "        ########## You code starts here #########\n",
    "        # rbf_feature = ...\n",
    "        scaled = self.scaler.transform(state)\n",
    "        rbf_feature = self.featurizer.transform(scaled)\n",
    "        \n",
    "        ########## You code ends here #########\n",
    "\n",
    "        #Task 1: return manual_feature or return rbf_feature\n",
    "        ########## You code starts here #########\n",
    "        feature = manual_feature #(task 1 a)\n",
    "        # feature = rbf_feature #(task 1 b)\n",
    "        ########## You code ends here #########\n",
    "        \n",
    "        return feature\n",
    "    \n",
    "    def get_action(self, state, epsilon=0.0):\n",
    "        # TODO: Task 1: Implement epsilon-greedy policy\n",
    "        # Hints:\n",
    "        # 1. self.q_functions is a list which defines a q function for each action dimension\n",
    "        # 2. for each q function, use predict(feature) to obtain the q value\n",
    "        ########## Your code starts here ##########\n",
    "        p = np.random.uniform(0, 1)\n",
    "        \n",
    "        #feature = ...\n",
    "        #q_values = ...\n",
    "        #\n",
    "        #if p < epsilon:\n",
    "        #    action = ...\n",
    "        #else:\n",
    "        #    action = ...\n",
    "            \n",
    "        feature = self.featurize(state)  \n",
    "        q_values = np.array([q.predict(feature)[0] for q in self.q_functions], dtype=np.float32)\n",
    "        \n",
    "        if p < epsilon:\n",
    "            action = np.random.randint(0, self.n_actions)\n",
    "        else:\n",
    "            action = int(np.argmax(q_values))\n",
    "        ########## Your code ends here #########\n",
    "        return action\n",
    "\n",
    "    def _to_squeezed_np(self, batch:Batch) -> Batch:\n",
    "        \"\"\" A simple helper function squeeze data dimension and cover data format from tensor to np.ndarray.\"\"\"\n",
    "        _ret = {}\n",
    "        for name, value in batch._asdict().items():\n",
    "            if isinstance(value, dict): # this is for extra, which is a dict\n",
    "                for k, v in value.items():\n",
    "                    value[k] = v.squeeze().numpy()\n",
    "                _ret[name] = value\n",
    "            else:\n",
    "                _ret[name] = value.squeeze().numpy()\n",
    "        return Batch(**_ret)\n",
    "        \n",
    "    def update(self, buffer):\n",
    "        # batch is a namedtuple, which has state, action, next_state, not_done, reward\n",
    "        # you can access the value be batch.<name>, e.g, batch.state\n",
    "        batch = buffer.sample(self.batch_size) \n",
    "        # Hint:\n",
    "        #    state = batch.state\n",
    "        #    action = batch.action \n",
    "        #    next_state = batch.next_state\n",
    "        #    reward = batch.reward \n",
    "        #    not_done = batch.not_done \n",
    "        \n",
    "        # the returned batch is a namedtuple, where the data is torch.Tensor\n",
    "        # we first squeeze dim and then covert it to Numpy array.\n",
    "        batch = self._to_squeezed_np(batch)\n",
    "\n",
    "        # TODO: Task 1, update q_functions\n",
    "        # Hints: \n",
    "        # 1. featurize the state and next_state\n",
    "        # 2. calculate q_target (check q-learning)\n",
    "        # 3. self.q_functions is a list which defines a q function for each action dimension\n",
    "        #    for each q function, use q.predict(featurized_state) to obtain the q value \n",
    "        # 4. remember to use not_done to mask out the q values at terminal states (treat them as 0)\n",
    "        ########## You code starts here #########\n",
    "        #f_state = ...\n",
    "        #f_next_state = ...\n",
    "        \n",
    "        f_state = self.featurize(batch.state)         \n",
    "        f_next_state = self.featurize(batch.next_state) \n",
    "\n",
    "        \n",
    "        # Check the Hints to finish the rest ;)\n",
    "        q_tar = ... \n",
    "        ########## You code ends here #########\n",
    "        \n",
    "        # Get new weights for each action separately\n",
    "        for a in range(self.n_actions):\n",
    "            # Find states where `a` was taken\n",
    "            idx = batch.action == a\n",
    "\n",
    "            # If a not present in the batch, skip and move to the next action\n",
    "            if np.any(idx):\n",
    "                act_states = f_state[idx]\n",
    "                act_targets = q_tar[idx]\n",
    "\n",
    "                # Perform a single SGD step on the Q-function params to update the q_function corresponding to action a\n",
    "                self.q_functions[a].partial_fit(act_states, act_targets)\n",
    "\n",
    "        return {}\n",
    "\n",
    "    def save(self, fp):\n",
    "        path = fp/'rbf.pkl'\n",
    "        u.save_object(\n",
    "            {'q': self.q_functions, 'featurizer': self.featurizer},\n",
    "            path\n",
    "        )\n",
    "\n",
    "    def load(self, fp):\n",
    "        path = fp/'rbf.pkl'\n",
    "        d = u.load_object(path)\n",
    "\n",
    "        self.q_functions = d['q']\n",
    "        self.featurizer = d['featurizer']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb219433-160a-4c5b-b1bf-6c654a57e703",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d198c51b373df885c3794fef8724caf0",
     "grade": false,
     "grade_id": "cell-f9705dbb63754c13",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "<a id='T1'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h4><b>Student Task 1.(a)</b> Linear model with handcraft feature </h4> \n",
    "\n",
    "First, let's test Q-learning with handcrafted features. We define the feature as :\n",
    "    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\phi(s) = [s, |s|]^T$\n",
    "\n",
    "Then we will feed this feature into a linear model to predict corresponding Q-values.\n",
    "\n",
    "For this Task, implement the code in ```class RBFAgent```:\n",
    "    \n",
    "1. First, you need to modify functions ```featurize```, to convert the state $s$ from encironment to $[s, |s|]^T$.\n",
    "2. Modify the function ```get_action```, use ```self.q_functions``` to select the action has the highest return.\n",
    "3. Modify the function ```update```, based on the Q-learning objective to update the Q-function (linear models).\n",
    "\n",
    "After the modifying, test the implementation on the Cartpole environment by running the ```train(agent)``` script with default configs. You can test the trained model with the ```test(agent, cfg_args=dict(save_video=true))``` script. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dd6467",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# After the modifying, run\n",
    "# init agent\n",
    "with open(Path().cwd()/'cfg'/'cartpole_rbf.yaml', 'r') as f:\n",
    "    cfg = t.Struct(**yaml.safe_load(f))\n",
    "        \n",
    "agent = RBFAgent(n_actions=cfg.n_actions, gamma=cfg.gamma, batch_size=cfg.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78c6566",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "    t.train(agent, cfg_path=Path().cwd()/'cfg'/'cartpole_rbf.yaml', cfg_args={}) # >= 20 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d57a812",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "    t.test(agent, cfg_path=Path().cwd()/'cfg'/'cartpole_rbf.yaml', cfg_args=dict(save_video=True), seeds=[67, 23, 89, 12, 45, 78, 34, 90, 21, 56]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9ba558-6057-4142-804b-d6bc43d0d709",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "    t.plot(save_name='cartpole_hd.png', cfg_path=Path().cwd()/'cfg'/'cartpole_rbf.yaml', cfg_args={}) # < 15 mins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fb6b18",
   "metadata": {},
   "source": [
    "The agent acting in the environment can be seen using the following command. Change the path to pick the episode you want to visualize. Bear in mind by default video saving for training is taken every 500 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b48c8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "    video = Video(Path().cwd()/'results'/'CartPole-v1'/'video'/'test'/'ex4_rbf-episode-0.mp4',\n",
    "    embed=True, html_attributes=\"loop autoplay\") # Set html_attributes=\"controls\" for video control\n",
    "    display(video)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571f48c8-9ab6-43fb-bce0-19719ce2c9e2",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='T1'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h4><b>Student Task 1.(b)</b> RBF approximation </h4> \n",
    "\n",
    "Now, let's test Q-learning with RBF approximation. In this task, we will do a feature transform using the Radial Basis Function. An input state will be transformed to as many features as samples $N$ are specified in the ```RBFSampler``` $\\phi : \\mathbb{R}^d \\mapsto \\mathbb{R}^N $. Then we will feed this $\\mathbb{R}^N $ feature into a linear model to predict corresponding Q-values.\n",
    "\n",
    "For this Task:\n",
    "    \n",
    "1. You need to modify functions ```featurize```, to **convert the state $s$ to a higher dimensional feature using** ```RBFSampler```.\n",
    "\n",
    "2. Run the cell of ```class RBFAgent``` to update the class function\n",
    "\n",
    "3. <span style=\"color:red\"> **IMPORTANT: Directly come to this cell after the modifications, DO NOT RUN the handcraft feature cells again!!!** </span>\n",
    "\n",
    "4. Run the cells below to check the performances of Q-learning with RBF approximation.\n",
    "    \n",
    "After the modifying, test the implementation on the Cartpole environment by running the ```train(agent)``` script with default configs. You can test the trained model with the ```test(agent, cfg_args=dict(save_video=true))``` script. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab43c155-a9a9-4d7c-8829-0b472d25314e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# After modifying the function 'featurize', \n",
    "# 1. run the cell of class RBFAgent(object) to update the class\n",
    "# 2. After update the class, directly run this cell (Skip the cells above of training hadncraft feature) \n",
    "# 2. now let's rerun the agent\n",
    "\n",
    "with open(Path().cwd()/'cfg'/'cartpole_rbf.yaml', 'r') as f:\n",
    "    cfg = t.Struct(**yaml.safe_load(f))        \n",
    "agent = RBFAgent(n_actions=cfg.n_actions, gamma=cfg.gamma, batch_size=cfg.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793f3d51-0bc1-469e-a889-86f217668c02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Takes more time to train :)\n",
    "if not skip_training:\n",
    "    t.train(agent, cfg_path=Path().cwd()/'cfg'/'cartpole_rbf.yaml', cfg_args={}) # >= 60 mins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b602bfa",
   "metadata": {},
   "source": [
    "In order to get full points from the task make sure most of the test episodes give a reward of 200, given the seeds below (at least 50% of episodes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88396eef-a80e-41dd-bdc7-c5cb67a2e75f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "    t.test(agent, cfg_path=Path().cwd()/'cfg'/'cartpole_rbf.yaml', cfg_args=dict(save_video=True), seeds=[67, 23, 89, 12, 45, 78, 34, 90, 21, 56]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5fd3b7-ee1d-46c7-830f-54b295ececa3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "    t.plot(save_name='cartpole_rbf.png', cfg_path=Path().cwd()/'cfg'/'cartpole_rbf.yaml', cfg_args={}) # < 5 mins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445eb823-f602-442f-b4b3-ed6806c18ffe",
   "metadata": {
    "tags": []
   },
   "source": [
    "The agent acting in the environment can be seen using the following command. Change the path to pick the episode you want to visualize. Bear in mind by default video saving for training is taken every 500 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd524780-6930-4de3-ae4b-9157e89760d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "    video = Video(Path().cwd()/'results'/'CartPole-v1'/'video'/'test'/'ex4_rbf-episode-0.mp4',\n",
    "    embed=True, html_attributes=\"loop autoplay\") # Set html_attributes=\"controls\" for video control\n",
    "    display(video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34862517",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "930235c2e6039ea417917618e7bfb84c",
     "grade": true,
     "grade_id": "cell-b8217caa685dddf5",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%capture --no-stdout\n",
    "\"TEST\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f725b2e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5e212891ed3fc555ca4d0fa87023b40a",
     "grade": false,
     "grade_id": "cell-133e2c93df20c4dc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "<a id='Q1'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Question 1.1</b> Considering Linear Features (10 points) </h3> \n",
    "    \n",
    "**Hint:** directly return 'state' in function 'featurize'\n",
    "    \n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5635de8e-3bdb-4856-a21e-20381601edbe",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "39f071752749e25e2b4545e6e33a2d28",
     "grade": false,
     "grade_id": "cell-f75a1189f158c6ae",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### 1.1.1 Question (4 points):\n",
    "Which of the following best describes why linear approximation of Q-values in the Cartpole problem is inaccurate?\n",
    "\n",
    "**Select one:**\n",
    "\n",
    "1. The environment dynamics are non-linear\n",
    "2. The Q-function is inherently non-linear\n",
    "3. The state space is too large\n",
    "4. The action space is continuous\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7903b02f-aae1-4b91-bd55-fee1f72129fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sq_1_1_1 = 0 # Replace with the appropriate answer number"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f058dca0-e5d7-4890-a4e8-a26a1f3bea00",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d127ac274ca6ea486fd1e79e155ab5c1",
     "grade": false,
     "grade_id": "cell-09a45dbf2a6f0e1d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### 1.1.2 Question (2 points):\n",
    "What does the Q-function represent?\n",
    "\n",
    "**Select one:**\n",
    "\n",
    "1. The immediate reward for taking an action\n",
    "2. The probability of the pole falling\n",
    "3. The expected cumulative reward for taking an action and then following the optimal policy\n",
    "4. The linear transformation of the current state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a93ee55-0e47-4a1a-b3d5-6d8e8dde720b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sq_1_1_2 = 0 # Replace with the appropriate answer number"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a783f6e-c95b-445e-9f7f-27ba47eab902",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "554a02b73714d9443d27251b6d1a8ab7",
     "grade": false,
     "grade_id": "cell-7e7997faef0448da",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### 1.1.3 Question (4 points):\n",
    "If the environment dynamics of Cartpole were perfectly linear, which statement would be true?\n",
    "\n",
    "**Select one:**\n",
    "\n",
    "1. The Q-function would also be linear\n",
    "2. Linear approximation of the Q-function would be accurate\n",
    "3. The Q-function could still be non-linear\n",
    "4. Reinforcement learning would not be necessary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223f8512-2231-4eb8-b8db-16cd6d62d420",
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_1_1_3 = 0 # Replace with the appropriate answer number"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5253364e-6e75-4729-8e00-cb606b48dae9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "144ffb087f8f0ec19ba52b26a9735626",
     "grade": false,
     "grade_id": "cell-22cb658f14d5cde6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "The following cells are used for grading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af865cb-688d-4e9a-b93d-44cb625bebcd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a1b5c33652bada8565e4ee5fb2c1175d",
     "grade": true,
     "grade_id": "cell-b4096a96472e5562",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert sq_1_1_1 in range(1, 5)\n",
    "assert sq_1_1_2 in range(1, 5)\n",
    "assert sq_1_1_3 in range(1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd02bb8-7933-4647-9993-c19ab6518f54",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "39cdd7a5d517d956028c7800288c1eab",
     "grade": true,
     "grade_id": "cell-83357309429acbcd",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac21b47-cf94-4da5-bbeb-a39a7abdaca2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e8a92c0c561551cfe15d0aa2c1d00ae0",
     "grade": true,
     "grade_id": "cell-3ed323aa17884686",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ab2f9f-aff6-4258-abe9-7e21c6270ba6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "40a67f2521eabb810cf0e07e83a93bc7",
     "grade": true,
     "grade_id": "cell-4284778ad9d19593",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5a8edc7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "981ecd6b06cd94e02fa1bdab9b1b27a7",
     "grade": false,
     "grade_id": "cell-9b048cd847c57c77",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "<a id='Q2'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Question 1.2</b> Experience Replay Buffer (10 points) </h3> \n",
    "\n",
    "**Hint:** In machine learning, when training a model, we usually assume dataset includes i.i.d(independent and identically distributed) samples from a unknown distribution.\n",
    "    \n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92584a0-e8d6-4a49-86bc-7d14d2c958d0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dd1241216c2e0ee9b94dc815b263ffd8",
     "grade": false,
     "grade_id": "cell-133b0005bee48d98",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### 1.2.1 Question (5 points):\n",
    "Why do we use the experience replay buffer in reinforcement learning? Select **at most** 3 answers, or you will lose **all** points.\n",
    "\n",
    "**Select two:**\n",
    "\n",
    "1. To prioritize the most recent experiences over older ones\n",
    "2. To make the learning process faster by re-using previous data and improving sample efficiency\n",
    "3. To ensure that the agent only learns from highly rewarding experiences\n",
    "4. To prevent the agent from learning negative experiences\n",
    "5. To break the correlation between consecutive experiences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbf9062-d94b-40f2-a486-930af53ae3c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sq_1_2_1 = [0, 0] # Write the appropriate answer number(s) inside the list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f7f3c6-c8f0-487c-bece-850f2fa9e087",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "176aee2c7c0508b69bda1934614ecc44",
     "grade": false,
     "grade_id": "cell-8d7e3c6747ad00c9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### 1.2.2 Question (5 points):\n",
    "Which of the following best describes the benefit of randomly sampling mini-batches from the experience replay buffer?\n",
    "\n",
    "**Select one:**\n",
    "\n",
    "1. It helps the agent to focus on learning from consecutive experiences\n",
    "2. It creates a more diverse and less correlated set of training data, leading to more stable learning\n",
    "3. It allows the agent to forget irrelevant or outdated information more quickly\n",
    "4. It reduces the computational complexity of the learning algorithm\n",
    "5. It simulates exploration by introducing randomness into the learning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f182a02a-15a0-4e09-ac15-f067203d64f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sq_1_2_2 = 0 # Replace with the appropriate answer number"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c53ffaa-6c4f-48b8-bfda-9af54c05b908",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a48de8481907fbcadd19fb3f8a4e5e18",
     "grade": false,
     "grade_id": "cell-b577265fd0011c08",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "The following cells are used for grading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f368aba2-5019-4950-aaa0-94a77e5395a1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "294627b6a6afdbcc7286f01e288ee2c1",
     "grade": true,
     "grade_id": "cell-f26d8fd34a084fd7",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert len(set(sq_1_2_1)) <= 3\n",
    "assert set(sq_1_2_1) < set(range(1, 6))\n",
    "assert sq_1_2_2 in range(1, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91541fa7-bba9-4625-b906-87f9406237a6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "085eefcf84eb13ee19847348fc726993",
     "grade": true,
     "grade_id": "cell-cf8a284ca46cf5bf",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10389db-047b-499f-8d84-19d79287f54f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "434a413418d6a0bcad8db6c32be8b3b6",
     "grade": true,
     "grade_id": "cell-f40898b6f10f1d9b",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f264ce3c-6958-4880-95d2-efa9aef31f11",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d64fb124439dc4825762417ec6d95155",
     "grade": true,
     "grade_id": "cell-0c6e6e9c7d9037fa",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a4793b0",
   "metadata": {},
   "source": [
    "<a id='Q3'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Question 1.3</b> Grid-based vs Function Approximators (10 points) </h3> \n",
    "\n",
    "**Hint:** An algorithm is said to be sample-efficient when it requires less samples (data) to reach\n",
    "an optimal performance\n",
    "    \n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88bdee4-990f-4af4-816d-0fc869e720fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.3.1 Question (10 points):\n",
    "In Exercise 3, we used grid-based Q-learning to balance the Cartpole. Compared to RBF function approximation methods, grid-based methods are:\n",
    "\n",
    "**Select one:**\n",
    "\n",
    "1. More sample-efficient because they take similarities between states into account, making them highly efficient.\n",
    "\n",
    "2. Less sample-efficient because they don't take similarities between states into account and require exploration of all discretized states.\n",
    "\n",
    "3. More robust to noise in the environment and require discretization of continuous state spaces, leading to fewer states to explore.\n",
    "\n",
    "4. Better at generalizing to unseen states because they always simplify the state space, making them more efficient.\n",
    "\n",
    "5. Equally sample-efficient as RBF methods, but less flexible in handling non-uniform state distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e40d8ca-e43a-48e8-8421-11a3c4c82374",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sq_1_3_1 = 0 # Replace with the appropriate answer number"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86df44e-111e-4879-9e61-6cf0ddfe0237",
   "metadata": {},
   "source": [
    "The following cells are used for grading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c91c3fe-5414-4a22-ace9-fd9d4dbbba25",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8d0bf530ec49953542f230deb6eda721",
     "grade": true,
     "grade_id": "cell-e5a1f15f82928287",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert sq_1_3_1 in range(1, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901bf613-0613-4551-8d0d-d91d3b4c13f2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7ef41072134244695dccd85c5ef126f8",
     "grade": true,
     "grade_id": "cell-ac160ae349b95d4c",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "223b9e0c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9b2bd9223d866f85d0b6d9e2a3948187",
     "grade": false,
     "grade_id": "cell-14c2950d3ec37ff1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "<a id='T2'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Task 2.</b> Visualizing the Policy (10 points) </h3> \n",
    "\n",
    "Create a 2D plot of policy (best action in terms of state) learned with RBF in terms of $x$ and Œ∏ for $\\dot{x} = 0$ and $\\dot{\\theta} = 0$, such that Œ∏ is on the horizontal axis and $x$ is on the vertical axis. The plot will be visualized below.\n",
    "\n",
    "**Hint 1 :** You can fix $\\dot{x} = 0$ and $\\dot{\\theta} = 0$ and discretize the state-space for $x$ and $\\theta$.\n",
    "\n",
    "**Hint 2 :** You can directly use the code in ***excercise 1 Task 4*** to visualize the policy.\n",
    "    \n",
    "**Hint 3 :** For CartPole environment state specifications, find them here https://gymnasium.farama.org/environments/classic_control/cart_pole/\n",
    "    \n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797754bd-2a91-4b68-ab12-84dab27f100a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# define and reload the RBF agent, which is trained with RBF feature!!!!\n",
    "with open(Path().cwd()/'cfg'/'cartpole_rbf.yaml', 'r') as f:\n",
    "    cfg = t.Struct(**yaml.safe_load(f))        \n",
    "agent = RBFAgent(n_actions=cfg.n_actions, gamma=cfg.gamma, batch_size=cfg.batch_size)\n",
    "agent.load(Path().cwd()/'results/CartPole-v1/model/')\n",
    "\n",
    "########## Your code starts here ##########\n",
    "\n",
    "\n",
    "\n",
    "########## Your code ends here #############\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25abcff7-fe09-4151-b343-f0e246725125",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8b26591fe5462716bc0054bce44c792d",
     "grade": false,
     "grade_id": "cell-ff09aacaf4dcf0a0",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "Do not remove this cell as it is used for grading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88aa5e4-757d-49c8-9bec-3f23e7bc2f9f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. Submitting <a id='3.'></a>\n",
    "Ensure all tasks and questions (in ```ex4_dpn.ipynb``` and ```ex4_rbf.ipynb```) are answered and the necessary plots are saved in the appropriate locations. The relevant plots and files need to be included for this part of assignment are:\n",
    "\n",
    "\n",
    "- Training performance plots:\n",
    "  - `cartpole_hd.png`: Training performance of handcrafted feature plots in terms of episode and episodic reward\n",
    "  - `cartpole_rbf.png`: training performance of rbf feature plots in terms of episode and episodic reward \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "  \n",
    "\n",
    "- Model files:\n",
    "  - `rbf.kpl`: Trained model\n",
    "\n",
    "\n",
    "Ensure the correct model files and plots are saved in paths:\n",
    "- ```results/CartPole-v1/cartpole_rbf.png``` Training result for Cartpole environment\n",
    "- ```results/CartPole-v1/cartpole_hd.png```  Training result for Cartpole environment\n",
    "- ```results/CartPole-v1/model/rbf.kpl``` Model for Cartpole environment\n",
    "\n",
    "\n",
    "<span style=\"color:red\"> **# IMPORTANT: DO NOT FORGET ANOTHER TASK IN ```ex4_dqn.ipynb```** </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa5830a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0df485c3a518d54d556aa23bfd41b439",
     "grade": true,
     "grade_id": "cell-2c6ecd048a50b678",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Make sure that skip training is set to True before submission\n",
    "assert skip_training == True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbc83ca-dbb0-4024-868e-6718a83e9bb2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3.1 Feedback <a id='3.1'></a>\n",
    "\n",
    "In order to help the staff of the course as well as the forthcoming students, it would be great if you could answer to the following questions in your submission:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee27da8-ac20-488b-9d05-c03ed1f0f837",
   "metadata": {},
   "source": [
    "1) How much time did you spend solving this exercise? (change the ```hrs``` variable below to a floating point number representing the number of hours taken e.g. 5.43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a4e19d-39b6-466a-8fea-b63f91916cf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hrs = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc128a60-a227-41b5-b6e0-531de2c3df1b",
   "metadata": {},
   "source": [
    "2) Difficulty of each task/question from 1-5 (int or float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c797f06b-bf2d-4305-abe2-e2773c6fb3d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "T1_a = None   # Implementing Hand feature \n",
    "T2_b = None   # Implementing RBF feature \n",
    "Q1_1 = None # Question 1.1 Considering linear features (10 points)\n",
    "Q1_2 = None # Question 1.2 Experience replay (10 points)\n",
    "Q1_3 = None # Question 1.3 Grid vs Function (10 points)\n",
    "Q2 = None # Question 2 Policy visualization (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6612842-e130-4658-adef-181218023449",
   "metadata": {},
   "source": [
    "3) How well did you understand the content of the task/question from 1-5? (int or float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5ea255-ca3f-4ee2-860c-147ff88373f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "T1_a = None   # Implementing Hand feature \n",
    "T2_b = None   # Implementing RBF feature \n",
    "Q1_1 = None # Question 1.1 Considering linear features (10 points)\n",
    "Q1_2 = None # Question 1.2 Experience replay (10 points)\n",
    "Q1_3 = None # Question 1.3 Grid vs Function (10 points)\n",
    "Q2 = None # Question 2 Policy visualization (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77831fe1-f18d-4036-97a9-ad6d1adadfa2",
   "metadata": {},
   "source": [
    "4) General feedback. Consider questions like:\n",
    "\n",
    "    - Did the content of the lecture relate well with the assignment?\n",
    "    - To what extent did you find the material to be potentially useful for your research and studies?\n",
    "    \n",
    "Please share any additional feedback, suggestions, or comments you have about the lecture, assignment, or course content. Your input is valuable in helping us improve the learning experience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8038fb-e038-4eac-87f3-ff28e0f94e9d",
   "metadata": {
    "tags": []
   },
   "source": [
    "DOUBLE CLICK HERE TO EDIT, CLEAR THIS TEXT AND ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
