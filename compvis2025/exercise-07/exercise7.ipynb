{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bf56b48d26fdf44036c31e2aa8a098c4",
     "grade": false,
     "grade_id": "cell-b83a8e6d5b8b4f5a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This cell is used for creating a button that hides/unhides code cells to quickly look only the results.\n",
    "# Works only with Jupyter Notebooks.\n",
    "\n",
    "import os\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''<script>\n",
    "code_show=true;\n",
    "function code_toggle() {\n",
    "if (code_show){\n",
    "$('div.input').hide();\n",
    "} else {\n",
    "$('div.input').show();\n",
    "}\n",
    "code_show = !code_show\n",
    "}\n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7f40675d189ba9fc7c27a3af293a5d8a",
     "grade": false,
     "grade_id": "cell-90ac698427e71926",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Description:\n",
    "#   Exercise7 notebook.\n",
    "#\n",
    "# Copyright (C) 2018 Santiago Cortes, Juha Ylioinas\n",
    "#\n",
    "# This software is distributed under the GNU General Public \n",
    "# Licence (version 2 or later); please refer to the file \n",
    "# Licence.txt, included with the software, for details.\n",
    "\n",
    "# Preparations\n",
    "import numpy as np\n",
    "\n",
    "# Select data directory\n",
    "if os.path.isdir('/coursedata'):\n",
    "    # JupyterHub\n",
    "    course_data_dir = '/coursedata'\n",
    "elif os.path.isdir('../../../coursedata'):\n",
    "    # Local installation\n",
    "    course_data_dir = '../../../coursedata'\n",
    "else:\n",
    "    # Docker\n",
    "    course_data_dir = '/home/jovyan/work/coursedata/'\n",
    "\n",
    "print('The data directory is %s' % course_data_dir)\n",
    "data_dir = os.path.join(course_data_dir, 'exercise-07-data')\n",
    "print('Data stored in %s' % data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "43a5c371570bdae1121705f693138889",
     "grade": false,
     "grade_id": "cell-c748757b7f7fc208",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# CS-E4850 Computer Vision Exercise Round 7\n",
    "\n",
    "The exercises should be solved and the solutions submitted via Aalto JupyterHub by the deadline. \n",
    "\n",
    "**Deliverables:**\n",
    "- **Jupyter notebook** (`exercise7.ipynb`) containing your solutions to the programming tasks and answers to the questions. Do not change the name of the notebook file. It may result in 0 points for the exercise. **Only this notebook will be graded.**\n",
    "\n",
    "**Important:**\n",
    "- Fill only the cells marked with `# YOUR CODE HERE`. Do not change function signatures.\n",
    "- You may add extra cells for your own tests, but **do not** overwrite global variables or edit locked cells.\n",
    "- **Never create new cells by menu commands \"Edit/Copy Cells\" and \"Edit/Paste Cells ...\"**. These commands create cells with duplicate ids and make autograding impossible. Use menu commands \"Insert/Insert Cell ...\" or the button with a plus sign to insert new cells.\n",
    "- **All notebooks contain hidden tests** which are used for grading. They are hidden inside read-only cells. Therefore, **the read-only cells should never be removed.** \n",
    "- **Note:** Visible tests mainly check the shapes and data types of your function’s output. Hidden tests check the correctness of your solution more thoroughly. Passing the visible tests does not guarantee full points for the exercise.\n",
    "- **Google Colab warning:** Uploading your assignment notebooks to Colab may cause problems. Colab can overwrite notebook metadata and break the autograding. To avoid this, we recommend copy-pasting your code into the notebooks fetched on JupyterHub. Sorry for the inconvenience.\n",
    "- Be sure that everything that you need to implement should work with the pictures specified by the assignments of this exercise round.\n",
    "- Running the cells in mixed order (which quite often happens while trying different things and debugging) may cause errors. While working on a particular cell be sure that you have freshly run all its preceding cells belonging to the same exercise.\n",
    "- **Before submitting**, simply run all the cells of the notebook (for example, select \"Restart & Run All\" in the menu) and check that all the cells run properly.\n",
    "- **Remember to submit your assignment!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "82104ff5962ec72b50054d7575bce363",
     "grade": false,
     "grade_id": "cell-bf4dd3d61f737c34",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Exercise 1 - Comparing  bags-of-words  with  tf-idf  weighting (10 points)\n",
    "Assume  that  we  have  an  indexed  collection  of  documents  containing  the  five  terms  of the following table where the second row indicates the percentage of documents in which each term appears.<br>\n",
    "\n",
    "| term | cat | dog |mammals | mouse | pet |\n",
    "| --- | :---: | :---: | :---: | :---: | :---: |\n",
    "| **% of documents** | 5 | 20 | 2 | 10 | 60 |\n",
    "\n",
    "Now, given the query $Q=\\{mouse, cat, pet, mammals\\}$, compute the similarity between $Q$ and the following example documents $D1$, $D2$, $D3$, by using the cosine similarity measure and tf-idf weights (i.e. term frequency - inverse document frequency) for the bag-of-words histogram representations of the documents and the query.\n",
    "\n",
    "-  $D1$ = Cat is a pet, dog is a pet, and mouse may be a pet too.\n",
    "-  $D2$ = Cat, dog and mouse are all mammals.\n",
    "-  $D3$ = Cat and dog get along well, but cat may eat a mouse.\n",
    "\n",
    "Ignore other words except the five terms, which are listed in the table above. \n",
    "\n",
    "### Proceed with the following steps:\n",
    "\n",
    "**1.1** Compute the inverse document frequency (idf) for each of the five terms<br>\n",
    "**1.2** Compute the term frequencies for the query and each document. <br>\n",
    "**1.3** Form the tf-idf weighted word occurrence histograms for the query and documents <br>\n",
    "**1.4** Evaluate the cosine similarity between the query and each document<br> \n",
    "**1.5** Report the relative ranking of the documents <br>\n",
    "\n",
    "Complete the tasks in **1.1**-**1.5**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "66c4b6c0fdbf75299c26fc94c0ce19d7",
     "grade": false,
     "grade_id": "cell-f478ee727c131da0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### 1.1 Compute the inverse document frequecy (idf)\n",
    "\n",
    "#### Task:\n",
    "Compute the inverse document frequency (idf) for each of the five terms in the table above.\n",
    "Use the **logarithm with base 2**. (idf is the logarithm term on a slide of Lecture 6 where values $n_i/N$ are given in the table above.)\n",
    "\n",
    "#### Important:\n",
    "- Store the five inverse document frequencies in a variable named `idf`.\n",
    "- `idf` must be a NumPy array of shape (5,).\n",
    "- The values must follow the same order as the terms appear in the table above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fc2b4efb67ae25a14f5538ef3efddf34",
     "grade": false,
     "grade_id": "cell-7b3f4b332812bd3d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "##--your-code-starts-here--##\n",
    "#idf = np.zeros(5) # replace me\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "##--your-code-ends-here--##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "43e6175aa0bd2f891c6092e565918222",
     "grade": true,
     "grade_id": "cell-239ca206a754ede4",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visible tests \n",
    "\n",
    "assert \"idf\" in globals(), \"Missing variable: idf\"\n",
    "assert isinstance(idf, np.ndarray), \"idf must be a NumPy array\"\n",
    "assert idf.shape[0] == 5, \"idf must contain 5 values\"\n",
    "assert idf.ndim == 1, \"idf must be one-dimensional\"\n",
    "\n",
    "assert np.allclose(idf[0], 4.3, atol=1e-1), \"The first idf value is incorrect. Did you remember to use logarithm with base 2?\"\n",
    "\n",
    "print(\"All visible tests passed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "62b80064e2a34a6a073f77b4fd184b45",
     "grade": true,
     "grade_id": "cell-df860198eba30fdf",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# HIDDEN TEST CELL\n",
    "# This cell contains hidden test cases that will be evaluated after the deadline.\n",
    "# Please do not remove or modify this cell, as it is required for grading.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a4963e2bb963ebe0d68c0d6c819e4dfc",
     "grade": false,
     "grade_id": "cell-dbccf5de2efe21cc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### 1.2 Compute the term frequencies for the query and each document\n",
    "\n",
    "#### Task:\n",
    "Compute the term frequencies (tf) for the query and each document.\n",
    "\n",
    "#### Important:\n",
    "- Store the term frequencies as NumPy arrays named:\n",
    "  - `tf_q` for the query\n",
    "  - `tf_d1`, `tf_d2`, and `tf_d3` for the three documents\n",
    "- Each array must have shape (5,).\n",
    "- The term order must be the same as in the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1a5799e6f776a2a977c94a930fd3b23d",
     "grade": false,
     "grade_id": "cell-33062173091b474b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "##--your-code-starts-here--##\n",
    "# tf_q = np.zeros(5)  # replace me\n",
    "# tf_d1 = np.zeros(5)  # replace me\n",
    "# tf_d2 = np.zeros(5)  # replace me\n",
    "# tf_d3 = np.zeros(5)  # replace me\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "##--your-code-ends-here--##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cf290cc51f25a333d6c4c78038eff0bc",
     "grade": true,
     "grade_id": "cell-a0344030bee099e1",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visible tests \n",
    "\n",
    "for name in [\"tf_q\", \"tf_d1\", \"tf_d2\", \"tf_d3\"]:\n",
    "    assert name in globals(), f\"Missing variable: {name}\"\n",
    "    arr = eval(name)\n",
    "    assert isinstance(arr, np.ndarray), f\"{name} must be a NumPy array\"\n",
    "    assert arr.ndim == 1, f\"{name} must be one-dimensional\"\n",
    "    assert arr.shape[0] == 5, f\"{name} must contain 5 values\"\n",
    "\n",
    "print(\"All visible tests passed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a3b38121370911e6559c3ae09cc7dae3",
     "grade": true,
     "grade_id": "cell-9e8231909a1488ed",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# HIDDEN TEST CELL\n",
    "# This cell contains hidden test cases that will be evaluated after the deadline.\n",
    "# Please do not remove or modify this cell, as it is required for grading.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "74e64d1c3fcd6ff9d619a2d3a7b36acd",
     "grade": false,
     "grade_id": "cell-596df511ab9044df",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### 1.3 Form the tf-idf weighted word occurrence histograms for the query and documents\n",
    "\n",
    "#### Task:\n",
    "Compute the tf-idf weighted word occurrence histograms for the query and each document.\n",
    "\n",
    "#### Important:\n",
    "- Store the term frequencies as NumPy arrays named:\n",
    "  - `tf_idf_q` for the query\n",
    "  - `tf_idf_d1`, `tf_idf_d2`, and `tf_idf_d3` for the three documents\n",
    "- Each array must have shape (5,).\n",
    "- The term order must be the same as in the table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dd3c8d0989ccde5c4d1a08d06896cd10",
     "grade": false,
     "grade_id": "cell-e8378b68946dedf6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "##--your-code-starts-here--##\n",
    "# tf_idf_q = np.zeros(5)  # replace me\n",
    "# tf_idf_d1 = np.zeros(5)  # replace me\n",
    "# tf_idf_d2 = np.zeros(5)  # replace me\n",
    "# tf_idf_d3 = np.zeros(5)  # replace me\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "##--your-code-ends-here--##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b4b1e3b216448e755e981f7981167cec",
     "grade": true,
     "grade_id": "cell-e2d24f4efb58b65b",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visible tests \n",
    "for name in [\"tf_idf_q\", \"tf_idf_d1\", \"tf_idf_d2\", \"tf_idf_d3\"]:\n",
    "    assert name in globals(), f\"Missing variable: {name}\"\n",
    "    arr = eval(name)\n",
    "    assert isinstance(arr, np.ndarray), f\"{name} must be a NumPy array\"\n",
    "    assert arr.ndim == 1, f\"{name} must be one-dimensional\"\n",
    "    assert arr.shape[0] == 5, f\"{name} must contain 5 values\"\n",
    "\n",
    "print(\"All visible tests passed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "415374216574437eead754c6e468cf89",
     "grade": true,
     "grade_id": "cell-85d8fd5853e8287f",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# HIDDEN TEST CELL\n",
    "# This cell contains hidden test cases that will be evaluated after the deadline.\n",
    "# Please do not remove or modify this cell, as it is required for grading.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "717523fd8b4e35d8a625e427daf4ba00",
     "grade": false,
     "grade_id": "cell-f9b0743e2a2aa279",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### 1.4 Evaluate the cosine similarity between the query and each document\n",
    "\n",
    "#### Task:\n",
    "Evaluate the cosine similarity between the query and each document i.e. normalized scalar product between the weighted occurrence histograms as shown on the slides.\n",
    "\n",
    "You should get similarities 0.95, 0.64, and 0.63, but you need to determine which corresponds to which document.\n",
    "\n",
    "#### Important:\n",
    "- Store the cosine similarities (scalars) in variables:\n",
    "  - `s1` = similarity(Q, D1)\n",
    "  - `s2` = similarity(Q, D2)\n",
    "  - `s3` = similarity(Q, D3)\n",
    "- Each of `s1`, `s2`, `s3` must be a float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b8fe09a434794f0eb70e4044ea2ff30c",
     "grade": false,
     "grade_id": "cell-79fe04df7d6ccb89",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "##--your-code-starts-here--##\n",
    "# s1 = 0  # replace me\n",
    "# s2 = 0  # replace me\n",
    "# s3 = 0  # replace me\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "##--your-code-ends-here--##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "65c33181285064d2e956fc1fb44aa652",
     "grade": true,
     "grade_id": "cell-a5b4b179298431f5",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visible tests \n",
    "for name in [\"s1\", \"s2\", \"s3\"]:\n",
    "    assert name in globals(), f\"Missing variable: {name}\"\n",
    "    val = eval(name)\n",
    "    assert isinstance(val, (float, np.floating)), f\"{name} must be a float\"\n",
    "\n",
    "print(\"All visible tests passed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ce0f7020e0a02e724ef0da0b736dfe28",
     "grade": true,
     "grade_id": "cell-228bacf44acb38c8",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# HIDDEN TEST CELL\n",
    "# This cell contains hidden test cases that will be evaluated after the deadline.\n",
    "# Please do not remove or modify this cell, as it is required for grading.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eeb1f0b712213d3b3ea8c5e4630f438e",
     "grade": false,
     "grade_id": "cell-796195d0ab72530e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### 1.5 Report the relative ranking of the documents \n",
    "\n",
    "#### Task:\n",
    "Report the relative ranking of the documents. You should have got similarities 0.95, 0.64, and 0.63, but you need to determine which corresponds to which document.\n",
    "\n",
    "#### Important:\n",
    "- Create a Python list named `ranking`.\n",
    "- Use document indices (1, 2, 3) and include each index exactly once.\n",
    "- Report the ranking in descending order of similarity; for example, if document D3 is the most similar to the query, place 3 first in the list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dcb70d4bb08e87f81166b0e50d83098d",
     "grade": false,
     "grade_id": "cell-928ea9061610caa1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "##--your-code-starts-here--##\n",
    "# ranking = np.array(3)  # replace me\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "##--your-code-ends-here--##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a963fe304f4b129cf1aa1153def43ef2",
     "grade": true,
     "grade_id": "cell-b526c39cd5dccdd3",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visible tests \n",
    "# Exists\n",
    "assert \"ranking\" in globals(), \"Missing variable: ranking\"\n",
    "\n",
    "# Allow list or ndarray; normalize to ndarray\n",
    "_r = np.asarray(ranking)\n",
    "\n",
    "# Must be 1D of length 3\n",
    "assert _r.ndim == 1, \"ranking must be one-dimensional\"\n",
    "assert _r.shape[0] == 3, \"ranking must contain 3 document indices\"\n",
    "\n",
    "# Must be integers and a permutation of [1, 2, 3]\n",
    "assert np.issubdtype(_r.dtype, np.integer), \"ranking must be integer indices\"\n",
    "assert set(_r.tolist()) == {1, 2, 3}, \"ranking must be a permutation of [1, 2, 3]\"\n",
    "\n",
    "print(\"All visible tests passed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "424ee587b62a81de1538b2b4c732f5ca",
     "grade": true,
     "grade_id": "cell-75d4e8720abbd4e5",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# HIDDEN TEST CELL\n",
    "# This cell contains hidden test cases that will be evaluated after the deadline.\n",
    "# Please do not remove or modify this cell, as it is required for grading.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "05eece22de96ec8b05b5c990b032e941",
     "grade": false,
     "grade_id": "cell-7d689dd151727a3b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Exercise 2 - Precision  and  recall\n",
    "There is a database of 10000 images and a user, who is only interested in images which contain a car. It is known that there are 500 such images in the database. An  automatic image retrieval system retrieves 300 car images and 50 other images from the database. Determine and report the precision and recall of the retrieval system in this particular case. Also compute the counts of true positives (tp), true negatives (tn), false positives (fp), and false negatives (fn). <br> \n",
    "\n",
    "### Hint: \n",
    "Precision and recall are explained on the slides of Lecture 6. There’s also a good Wikipedia overview (including definitions of true/false positives and negatives): https://en.wikipedia.org/wiki/Precision_and_recall.\n",
    "\n",
    "### Important:\n",
    "Store your answers in the following variables:\n",
    "- `tp` true positives\n",
    "- `tn` true negatives\n",
    "- `fp` false positives\n",
    "- `fn` false negatives\n",
    "- `precision` precision\n",
    "- `recall` recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "28c5773dd9ad69b563a8a7389bcb4b31",
     "grade": false,
     "grade_id": "cell-d12b27859ad96e94",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "##--your-code-starts-here--##\n",
    "# tp = 0         # replace me\n",
    "# tn = 0         # replace me \n",
    "# fp = 0         # replace me\n",
    "# fn = 0         # replace me\n",
    "# precision = 0  # replace me\n",
    "# recall = 0     # replace me\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "##--your-code-ends-here--##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0aa510aa1f5e365434e292d5a1ac0b33",
     "grade": true,
     "grade_id": "cell-7cdfc6837402050c",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visible tests \n",
    "\n",
    "# Variables exist\n",
    "for name in [\"tp\", \"tn\", \"fp\", \"fn\", \"precision\", \"recall\"]:\n",
    "    assert name in globals(), f\"Missing variable: {name}\"\n",
    "\n",
    "# Counts should be integer and non-negative\n",
    "for name in [\"tp\", \"tn\", \"fp\", \"fn\"]:\n",
    "    val = eval(name)\n",
    "    assert isinstance(val, (int, np.integer)), f\"{name} must be an integer\"\n",
    "    assert val >= 0, f\"{name} must be non-negative\"\n",
    "    \n",
    "# precision/recall should be numeric\n",
    "for name in [\"precision\", \"recall\"]:\n",
    "    val = eval(name)\n",
    "    assert isinstance(val, (int, float, np.integer, np.floating)), f\"{name} must be numeric\"\n",
    "\n",
    "print(\"All visible tests passed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6f60926ffbdc4e310aaa56f9310830f3",
     "grade": true,
     "grade_id": "cell-091da5899a9b2d86",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# HIDDEN TEST CELL\n",
    "# This cell contains hidden test cases that will be evaluated after the deadline.\n",
    "# Please do not remove or modify this cell, as it is required for grading.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bf8b52310f3b01a814a28b7315e23c09",
     "grade": true,
     "grade_id": "cell-ef5a1e50304927e0",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# HIDDEN TEST CELL\n",
    "# This cell contains hidden test cases that will be evaluated after the deadline.\n",
    "# Please do not remove or modify this cell, as it is required for grading.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f53076a8aeff6c6256b2af29757db839",
     "grade": true,
     "grade_id": "cell-1bd4738cf29f25a9",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# HIDDEN TEST CELL\n",
    "# This cell contains hidden test cases that will be evaluated after the deadline.\n",
    "# Please do not remove or modify this cell, as it is required for grading.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4b759930e99c2f30073a2c7d10768096",
     "grade": true,
     "grade_id": "cell-bd71ac29e7ea61f5",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# HIDDEN TEST CELL\n",
    "# This cell contains hidden test cases that will be evaluated after the deadline.\n",
    "# Please do not remove or modify this cell, as it is required for grading.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bfcb51e8e8a3364569181b3df30cc1ca",
     "grade": true,
     "grade_id": "cell-ffe99ad1544fc27e",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# HIDDEN TEST CELL\n",
    "# This cell contains hidden test cases that will be evaluated after the deadline.\n",
    "# Please do not remove or modify this cell, as it is required for grading.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "18ade92031de5e675289936495ff569f",
     "grade": true,
     "grade_id": "cell-9f53160c0a5fa409",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# HIDDEN TEST CELL\n",
    "# This cell contains hidden test cases that will be evaluated after the deadline.\n",
    "# Please do not remove or modify this cell, as it is required for grading.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6f33cd966b08992b9c3241a74db69e47",
     "grade": false,
     "grade_id": "cell-5956f622ea5b415f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Exercise 3 - VGG practical on object instance recognition (20 points)\n",
    "See the questions in `part1.ipynb`, `part2.ipynb`, and `part3.ipynb` and write your answers in this notebook, `exercise7.ipynb`.\n",
    "\n",
    "**Part1:** <br>\n",
    "Stage I.A (two questions) <br>\n",
    "Stage I.B (two questions) <br>\n",
    "Stage I.C (one question) <br>\n",
    "\n",
    "**Part2** (one question)\n",
    "\n",
    "**Part3:** <br>\n",
    "Stage III.A (three questions) <br>\n",
    "Stage III.B (one question) <br>\n",
    "Stage III.C (two questions) <br>\n",
    "\n",
    "Answering questions in **Part 1** corresponds to **10 points** and **Parts 2 and 3** together correspond to **10 additional points**. Hence, this Exercise 3 is in total worth of 20 points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e815f5409d3dd9a8736d39797a1ad7c8",
     "grade": false,
     "grade_id": "cell-9c27acc45f709844",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### Part 1 (10 points)\n",
    "Type your answers for **Part 1** (5 questions) below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e9edd81cf1a2b18c4f316ae1c9fdab0e",
     "grade": true,
     "grade_id": "cell-0f20b0d1cb3e0593",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1cb56489eb9aa17321310ed596864a51",
     "grade": false,
     "grade_id": "cell-877db417298d8d18",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### Part 2 and 3 (10 points)\n",
    "Type your answers for **Part 2** (1 question) and **Part 3** (6 questions) below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6fa78b8778ae33f4fcb2ec0402a9254d",
     "grade": true,
     "grade_id": "cell-acf7b8978ca4d602",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
