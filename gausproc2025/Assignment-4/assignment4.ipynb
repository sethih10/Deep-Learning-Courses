{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf278d1f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "2U5gjTQc669x",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "888de136d7f7da297dacc9c5c5b23425",
     "grade": false,
     "grade_id": "cell-121487f0f624e78d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# CS-E4895: Gaussian Processes\n",
    "\n",
    "## Assignment #4: Variational inference for GP classification\n",
    "\n",
    "### Learning objectives\n",
    "\n",
    "After completing the exercise, you should be able to:\n",
    "\n",
    "- Implement variational inference for GP classification and hyperparameter selection.\n",
    "- Know how to apply an automatic differentiation framework, such as Tensorflow, in the GP context.\n",
    "\n",
    "### Important Notes\n",
    "\n",
    "- In the first part of this assignment, we will use an automatic differentiation framework to optimize the ELBO objective that arises in variational inference. Most standard optimization methods require access to the gradients of the optimized function. If you were able to derive them by hand for this particular task, you could optimize using standard numpy/scipy. Nowadays, however, these automatic differentiation frameworks make things quite a lot easier, providing easy gradients and integrating them with different standard optimizers. This assignment is designed with Tensorflow, but if you wish, you can try implementing everything with a different framework, such as Pytorch or Jax (we suggest to stick with Tensorflow unless you feel sure about yourself). \n",
    "- All exercises must be solved using only basic mathematical operations (exp, erf, ...) and linear algebra routines (solve, matrix-vector products, ...)\n",
    "- Common gotcha: getting the shape of your arrays wrong e.g. (N, 1) vs (1, N) vs (N,); check that all your functions get and return arrays of the correct shapes!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58004f4-0992-48fc-a1a0-7e1a17907d5b",
   "metadata": {},
   "source": [
    "## Enter your student number\n",
    "\n",
    "**STUDENT_NUMBER** = XXX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136d0aec-131e-4c47-ac18-eabba81130c8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a7625dc4ff73fc01a49c8f2d4fbecac3",
     "grade": false,
     "grade_id": "cell-1bc64e0b800f6314",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Warnings:\n",
    "\n",
    "- Don’t copy and paste cells within a notebook. This will mess up the tracking metadata and prevent autograding from working.\n",
    "- Only add new cells using the '+' button in the upper toolbar, and do not split cells.\n",
    "- Be cautious about things such as copying the whole notebook to Colab to work on it. This has sometimes resulted in removing all notebook metadata, making autograding impossible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1c33c8-cc52-40b7-ac6b-ba2c50e7fbf3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "52f06854c3f9b8051d268be233a1986b",
     "grade": false,
     "grade_id": "cell-8ac32e04dee0720f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Do not move or delete the below block. It is used for automatic grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f05cda-6191-412b-ba76-e9825b2bf836",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3abbc488cd9b30fe8fc6e32db58de0ed",
     "grade": true,
     "grade_id": "cell-54e1ce0fba2bfb14",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d7a4392-9fe4-4ff3-a272-fc2cb86d8d78",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a5ac6fbb28749438e1cf6f6de1210fcb",
     "grade": false,
     "grade_id": "cell-c13ade857f60bf29",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-26 22:02:53.605324: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import matplotlib.pyplot as plt\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "980abada-fc73-42d7-83c7-9fe1f4e14947",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "automatic_grading = True  # Set this to True after completing the notebook and before submitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631f2672-859b-488f-b180-bf8b33095a12",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cc8acc0f4c94dfb79f56f1a3f454c811",
     "grade": false,
     "grade_id": "cell-e2590d511cc86cd8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Do not move or delete the below block. It is used for automatic grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f941198-7a5f-49cc-a3ff-e80db267feb4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "84dc2747753ef0ad3652706f67374520",
     "grade": true,
     "grade_id": "cell-c3bf6f709f27a888",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0d7ec7d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "C37jb-UxOKue",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "655259af9dbfb35d21a27d0d3ec4a4ba",
     "grade": false,
     "grade_id": "cell-c3c9a43926283aa2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### A mini tutorial on automatic differentiation\n",
    "\n",
    "When using an automatic differentiation framework to optimize a function $f: \\theta \\to f(\\theta)$, the variable $\\theta$ and/or the operations mapping from $\\theta$ to $f(\\theta)$ must be defined using operators from the framework.\n",
    "\n",
    "For example to optimize $e^{\\theta}+e^{-\\theta}$ with respect to $\\theta$ with TensorFlow, you need to proceed as follows: (See tutorial on gradient calculation in TensorFlow: https://www.tensorflow.org/api_docs/python/tf/GradientTape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcf4dbf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6QFiiuNGOL3z",
    "outputId": "d5af91e5-dbd5-436b-fbbe-7de974d238ff"
   },
   "outputs": [],
   "source": [
    "# define the theta variable\n",
    "theta = tf.Variable(1.0, dtype=tf.float64)\n",
    "\n",
    "# define the function\n",
    "f = lambda x: tf.exp(x) + tf.exp(-x) # note the use of the tf.exp operation (not np.exp)\n",
    "\n",
    "# run the optimization\n",
    "for t in range(1000):\n",
    "    # at each step, compute the gradients\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(theta)\n",
    "        loss = f(theta)\n",
    "    \n",
    "    gradient = tape.gradient(loss, theta)\n",
    "    \n",
    "    # apply the variable update (gradient descent)\n",
    "    theta.assign(theta - 0.01*gradient)\n",
    "    \n",
    "    if t % 100 == 0:\n",
    "        print(t, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1476064",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f294269d38bd5964dd27eda850e9f1dd",
     "grade": false,
     "grade_id": "cell-5a0a91ca61ff777d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 1: ELBO components (3 points)\n",
    "\n",
    "We are interested in the problem of Gaussian Process classification. We have some data $\\mathcal{D} = \\left\\lbrace {\\bf x}_n, y_n \\right\\rbrace_{n=1}^N$, with $y_n \\in \\{-1,1\\}$.\n",
    "\n",
    "We want to perform inference in the following generative model:\n",
    "$$\n",
    "\\begin{align*}\n",
    "f(\\cdot) &\\sim GP(0, k) \\\\\n",
    "f_n &= f({\\bf x}_n) \\qquad &&(f_n\\in \\mathbb{R}) \\\\\n",
    "\\theta_n &= \\phi(f_n) \\qquad &&(\\theta_n\\in (0, 1)) \\\\\n",
    "\\{-1,1\\} \\ni y_n  &\\sim \\text{Bernoulli}(p=\\theta_n)\n",
    "\\end{align*}\n",
    "$$\n",
    "with $\\phi$ the normal cumulative distribution function $\\phi(x)=\\int_{-\\infty}^x {\\cal N}(u; 0,1)\\,{\\rm d}u$.\n",
    "\n",
    "We can compactly write the likelihood as\n",
    "$$ p(y_n|f(\\cdot),{\\bf x}_n) = p(y_n|f_n) = \\phi(y_n \\cdot f_n).$$\n",
    "\n",
    "\n",
    "We will here use a Squared Exponential kernel, with two parameters: lengthscale $\\ell$ and variance $\\sigma^2$.\n",
    "\n",
    "\n",
    "The posterior is $p({\\bf f}|{\\bf y}) \\propto p({\\bf y}\\mid{\\bf f})p({\\bf f})$ is intractable, hence we resort to an approximate inference scheme called **variational inference**.\n",
    "\n",
    "This turns inference into optimization. We optimize the distance $d(q) = \\operatorname{KL}[q({\\bf f})\\|p({\\bf f}|{\\bf y})] \\geq 0$, with respect to a distribution $q({\\bf f})$.\n",
    "\n",
    "We parameterize $q$ through the mean vector $m$ and the Cholesky factor of the covariance $L$: i.e. $q({\\bf f})={\\cal N}({\\bf f}\\mid \\bf{m}, \\bf{S}=\\bf{L}\\bf{L}^\\top)$\n",
    "\n",
    "In practice we optimize the ELBO:\n",
    "$${\\cal L}(q) = \\log p({\\bf y})-d(q) = \n",
    "\\underbrace{\\mathbb{E}_q\\big[ \\log p({\\bf y}|{\\bf f}) \\big]}_{\\text{VE}} \n",
    "- \\underbrace{\\operatorname{KL}[q({\\bf f})\\|p({\\bf f})]}_{\\text{KL}}$$\n",
    "\n",
    "We split the ELBO into two terms\n",
    "* variational expectations (VE), or data fit term,\n",
    "* Kullback-Leibler (KL) between the prior and the approximate posterior, or regularization term.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7359d29",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "s6XskkCP66-X",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1f06255f7f759b48ccb01d171a28d518",
     "grade": false,
     "grade_id": "cell-ae3dd116c88d1004",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Task 1a**: For a prior $p({\\bf f})={\\cal N}({\\bf f}\\mid 0,K)$ and a variational distribution $q({\\bf f})={\\cal N}({\\bf f}\\mid \\bf{m}, \\bf{S}=\\bf{L}\\bf{L}^\\top)$, compute the KL divergence $\\operatorname{KL}[q({\\bf f})\\|p({\\bf f})]$\n",
    "\n",
    "\n",
    "You can use the formula:\n",
    "$$\n",
    "\\begin{align*}\n",
    "&\\operatorname{KL}\\left[\\mathcal{N}(\\bf\\mu_0,\\bf\\Sigma_0) \\| \\mathcal{N}(\\bf\\mu_1,\\bf\\Sigma_1)\\right] \\\\ \n",
    " &= \\frac{1}{2}\\left(\n",
    "    \\operatorname{tr}\\left(\\bf\\Sigma_1^{-1}\\bf\\Sigma_0\\right) +\n",
    "    \\left(\\bf\\mu_1 - \\bf\\mu_0\\right)^\\top \\bf\\Sigma_1^{-1}\\left(\\bf\\mu_1 - \\bf\\mu_0\\right) - k +\n",
    "    \\ln\\frac{|\\bf\\Sigma_1|}{|\\bf\\Sigma_0|}\n",
    "  \\right),\\; \\text{(source: Wikipedia)}\\\\\n",
    "  &= \\dots \\quad \\text{ (Bonus: can you fill the gap?)}\\\\\n",
    "    &=\n",
    "  \\frac{1}{2}\\left(\n",
    "    \\sum_{ij} (L_1^{-1} L_0)^2_{ij} +\n",
    "    \\|\\bf \\bf L_1^{-1}\\left(\\bf \\mu_1 - \\bf\\mu_0\\right)\\|^2 - k + 2\\sum_{i}\n",
    "    \\big(\\ln |\\bf L_{1,ii}|- \\ln|L_{0,ii}|\\big)\n",
    "  \\right),\n",
    "  \\end{align*}\n",
    "  $$\n",
    "where we have adapted it to the (mean, Cholesky) parameterization of the multivariate Gaussian distributions, *i.e.*, $\\bf \\Sigma_0 = \\bf L_0 \\bf L_0^\\top$ and $\\bf \\Sigma_1 = \\bf L_1 \\bf L_1^\\top$.\n",
    "\n",
    "**Note:** In the last line of the equation above we simplified the computation of the log-determinant to a sum over the logarithms of the diagonal elements. This is specific to the parameterization using Cholesky factors of the covariances, which implies that $\\bf L_0$ and $\\bf L_1$ are *lower-triangular* matrices with non-negative elements on the diagonal! If not, we cannot expect this function to perform correctly. \n",
    "\n",
    "In the function below, you can assume that the inputs are valid Cholesky factors, but later on we will need to ensure this explicitly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c04ff7",
   "metadata": {
    "deletable": false,
    "id": "sW7T2ux_66-Y",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e5c04eaabe51379be0fa24a0fcfef2f6",
     "grade": false,
     "grade_id": "cell-c6e159d0042402b0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def KL(m0, L0, m1, L1) -> float:\n",
    "    \"\"\" returns the KL divergence between N(m0, S0) and N(m1, S1)\n",
    "    \n",
    "    arguments:\n",
    "    m0, m1   -- N × 1, mean vector\n",
    "    L0, L1   -- N × N, Cholesky factor of a covariance matrix \n",
    "    \n",
    "    returns the KL value (should be a tf.Tensor).\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed23e08c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "33m5cg2UIl-F",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "112570617a58be62a42b91fccef0137d",
     "grade": false,
     "grade_id": "cell-eef1aa2fc6bfd63d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Let's test the above function\n",
    "\n",
    "For instance, noting $q_0(f) = N(f|0, I)$ and $q_1(f) = N(f|0, 2I)$, we should have:\n",
    "* $\\operatorname{KL}[q_0\\|q_0] = 0$\n",
    "* $\\operatorname{KL}[q_0\\|q_1] > 0$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adf104d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "7tl6dFfAJQz5",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "345faff7f56403ec7f04048adf966df3",
     "grade": false,
     "grade_id": "cell-bfb72508c0719f3f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "m_0 = np.random.randn(10, 1)\n",
    "m_1 = np.random.randn(10, 1)\n",
    "L_0 = np.tril(np.random.randn(10, 10)) + 4 * np.eye(10)  # Cholesky matrices should be lower-triangular\n",
    "L_1 = np.tril(np.random.randn(10, 10)) + 4 * np.eye(10)  # with positive elements on the diagonal\n",
    "\n",
    "assert abs(KL(m_0, L_0, m_0, L_0)) < 1e-10  # approx. == 0\n",
    "assert KL(m_0, L_0, m_1, L_1) > 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a915db9d-0e73-4639-957d-e20e58add67d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4b5d7fecea47ee8c36f01f9eb7448513",
     "grade": false,
     "grade_id": "cell-011096764e663a8a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Do not move or delete the below block. It is used for automatic grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999403c5-125b-40fc-90f4-e6bf947ee525",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1303909a668936c510d3ffe787612f2b",
     "grade": true,
     "grade_id": "cell-06ba6d9586f29bba",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b608fe7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "w9zp6Q-W66-a",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "40810f41d5e41e031025772f692c7b97",
     "grade": false,
     "grade_id": "cell-6d7ccf13620bbce2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Task 1b**: We compute the variational expectations, $\\mathbb{E}_{q(f_n)}\\big[\\log p(y_n|f_n)\\big]$. For this, first we will need to compute the marginal distribution $q(f_n)$ and then compute the expectation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216c27a8",
   "metadata": {
    "deletable": false,
    "id": "onU4NYnp66-b",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a4b7b4624ac9c192fbd996afad24c873",
     "grade": false,
     "grade_id": "cell-c5ad879b9e9b63cf",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def q_marginals(m, L):\n",
    "    \"\"\" \n",
    "    returns the vectors of marginal means and marginal variances i.e, the means and variances of q(f_n).\n",
    "    \n",
    "    Arguments:\n",
    "    m   -- N × 1, mean vector\n",
    "    L   -- N × N, Cholesky factor of a covariance matrix \n",
    "    \n",
    "    returns two N × 1 vectors\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return m, v\n",
    "\n",
    "def phi(x):\n",
    "    r\"\"\" Cumulative distribution function for the standard normal distribution \n",
    "    \n",
    "    Hint: \n",
    "    1) you may want to use the error function. (tf.math.erf if using Tensorflow)\n",
    "    2) You may want to use a small epsilon parameter to ensure that the output  is strictly between 0 and 1\n",
    "\n",
    "    phi(x) = int_{-\\infty, x} N(u| 0, 1) du    \n",
    "    \"\"\"\n",
    "    jitter = 1e-3  # Use this to ensure the output is strictly between 0 and 1.\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "def classif_log_likelihood(f, y):\n",
    "    \"\"\" log p(y|f) for classification using the normal cdf \n",
    "        log p(y|f) = log phi(y * f)\n",
    "        \n",
    "    Arguments:\n",
    "    f -- The GP value at the observation\n",
    "    y -- The observed value of y, either -1 or 1\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbc4c19-eb56-4104-88f1-c86ed6a9383b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "qq0NMMtr66-d",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "40af8818b1c1b9af4625de764ce499dd",
     "grade": false,
     "grade_id": "cell-dac3dd227f746c3b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Let's test the above function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d372a061-7c9f-4f2f-a023-48ce09c49cdb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f9a574e9af5076ed0c2696ae3eb8740f",
     "grade": false,
     "grade_id": "cell-6af88e9377ca69f2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "m1_test = np.array([[0.7052], [0.5225]])\n",
    "L1_test = np.array([[4.0515, 0.],[0.8740, 5.1058]])\n",
    "\n",
    "# Testing q marginals\n",
    "expected_m = np.array([[0.7052], [0.5225]])\n",
    "expected_S = np.array([[16.415], [26.833]])\n",
    "m_test, S_test = q_marginals(m1_test, L1_test)\n",
    "np.testing.assert_allclose(m_test, expected_m, rtol=1e-4, atol=1e-4)\n",
    "np.testing.assert_allclose(S_test, expected_S, rtol=1e-4, atol=1e-4)\n",
    "\n",
    "# Testing phi\n",
    "phi_val_test = phi(m1_test)\n",
    "expected_phi = np.array([[0.7591], [0.6989]])\n",
    "np.testing.assert_allclose(expected_phi, phi_val_test, rtol=1e-4, atol=1e-4)\n",
    "\n",
    "# Testing classif_log_likelihood\n",
    "x_test = np.array([[0.7052], [0.5225]])\n",
    "y_test = np.ones_like(x_test)\n",
    "expected_ll = np.array([[-0.2755],[-0.3582]])\n",
    "ll_val = classif_log_likelihood(x_test, y_test)\n",
    "np.testing.assert_allclose(ll_val, expected_ll, rtol=1e-4, atol=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908ad10f-be6a-4bdd-b3d3-54d0b3de24e3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a4dada6773acc165ef94d13d276dc024",
     "grade": false,
     "grade_id": "cell-b4ebd9ee3e5ffe1c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Do not move or delete the below three blocks. It is used for automatic grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d250d1c-d7d8-4a8f-a575-b31d7da02bdc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9735268059ec5254e88d99b94c806da9",
     "grade": true,
     "grade_id": "cell-595d8017bf1e9afb",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df8007f-3d68-4517-bbbd-beb1f107f2eb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "31305966c88402632aeb21d7334dcb2e",
     "grade": true,
     "grade_id": "cell-07e45f69cdc618dd",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b3d34f-1ae0-4c87-b36e-d314cae6b2ad",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "21ff4e1f0a537195273adc95a64faeef",
     "grade": true,
     "grade_id": "cell-e228dfeedb7ed357",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8b501a9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "qq0NMMtr66-d",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3b119ed39ea031d75995782bf581c751",
     "grade": false,
     "grade_id": "cell-122a73086e92b9c7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The next function is given to you. It approximates $\\mathbb{E}_{q(f_n)}\\big[\\log p(y_n|f_n)\\big]$ via [Gauss-Hermite quadrature](https://en.wikipedia.org/wiki/Gauss%E2%80%93Hermite_quadrature)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c911317",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "qq0NMMtr66-d",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "12a380b6b45f9caecaf339dbab835100",
     "grade": false,
     "grade_id": "cell-34d49e4e8c9cb9c4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This function is complete.\n",
    "def expected_log_likelihood(\n",
    "    means, mvars, llh, y, n_gh=20):\n",
    "    \"\"\" returns the expected log-likelihood terms\n",
    "    \n",
    "    E_q(f_n)[log p(y_n|f_n)]\n",
    "    \n",
    "    This is a quadrature approximation, \n",
    "    turning the integral into a sum.\n",
    "        \n",
    "    Arguments:\n",
    "    means  -- N × 1, vector of means\n",
    "    mvars  -- N × 1, vector of marginal variances\n",
    "    llh    -- log-likelihood function\n",
    "    y      -- N × 1, vector of observed labels\n",
    "    \"\"\"\n",
    "    z, dz = np.polynomial.hermite.hermgauss(n_gh)\n",
    "    weights = (dz / np.sqrt(np.pi)).reshape(1, -1) # 1 × n_gh\n",
    "    inputs = means + np.sqrt(2) * tf.sqrt(mvars) * z.reshape(1, -1) # N × n_gh\n",
    "    llh_quad = weights * llh(inputs, y) # N × n_gh\n",
    "\n",
    "    return tf.reduce_sum(llh_quad, axis=1) # N,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a259fe97",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "JUCd3aeN66-e",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2607f8b90b300c666d21cfb6b3ac04e7",
     "grade": false,
     "grade_id": "cell-320d86bcdf8b3b4c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Task 1c**: We are now ready to implement the ELBO.\n",
    "\n",
    "$${\\cal L}(q) = \n",
    "\\underbrace{\\mathbb{E}_q\\big[ \\log p({\\bf y}|{\\bf f}) \\big]}_{\\text{VE}} \n",
    "- \\underbrace{\\operatorname{KL}[q({\\bf f})\\|p({\\bf f})]}_{\\text{KL}}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ed0430",
   "metadata": {
    "deletable": false,
    "id": "D_XH9m0U66-e",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9b07ef6f42d96d07d638e852b3fab3b8",
     "grade": false,
     "grade_id": "cell-77d2e628d10b348b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def elbo(m_p, L_p, m_q, L_q, y):\n",
    "    \"\"\" returns ELBO\n",
    "    \n",
    "    ℒ = ∑ₙ₌₁ᴺ 𝔼_q(fₙ)[log p(yₙ|fₙ)] - KL[q(f)∥p(f)]\n",
    "        \n",
    "    Arguments:\n",
    "    \n",
    "    L_p, L_q  -- N × N, Cholesky factors of the covariances of p and q\n",
    "    m_p, m_q  -- N × 1, mean vector of p and q\n",
    "    \n",
    "    returns: a scalar\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a73a4b-7393-4403-955a-27b0af3c7af0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a384cbab45c50c63b5397d7d6b159cc7",
     "grade": false,
     "grade_id": "cell-ee0de109d1ed4ab1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Let's test the above function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36168869-3ba8-48b5-9337-ba9756ec7abd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4a56368920ab21f4a3c4e97f1db0555a",
     "grade": false,
     "grade_id": "cell-8a6b12d19420903e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "m1_test = np.array([[0.62108864], [0.81181053]])\n",
    "L1_test = np.array([[3.51847402, 0.], [0.88272004, 5.01573244]])\n",
    "m2_test = np.array([[0.09114978], [0.5079975 ]])\n",
    "L2_test = np.array([[ 2.81332478, 0.],[-0.3736283 ,  5.06633749]])\n",
    "y_test = np.ones_like(m1_test)\n",
    "\n",
    "val_test = elbo(m1_test, L1_test, m2_test, L2_test, y_test)\n",
    "\n",
    "np.testing.assert_allclose(-4.25904, val_test, rtol=1e-4, atol=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32feb044-391e-4d0d-afd3-0f4e417fb4a3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f65a906e2a89798fc2faab30ad3eabd1",
     "grade": false,
     "grade_id": "cell-56fc51a26df2f889",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Do not move or delete the below block. It is used for automatic grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ddd21d-e590-4a47-b161-847503643c8c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d9302544d3a04ef1972576252a9f8856",
     "grade": true,
     "grade_id": "cell-2a62089dd23b1404",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5fd7e467",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "3fRRI2op66-f",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8dda2865378ffb31af83dc0dee3ebd02",
     "grade": false,
     "grade_id": "cell-edb764d7125a2b4a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 2: Inference as optimization (0.5 points)\n",
    "\n",
    "We are now ready to optimize the ELBO. We will first load some data and select a subset of it for the full VGP inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed698fc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 324
    },
    "deletable": false,
    "editable": false,
    "id": "jT-nKs_w66-g",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ae1e5b281fee9edef9c6e07bffbc9271",
     "grade": false,
     "grade_id": "cell-08395367c9ce443e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "e796e8c9-e51c-4b65-ae90-b37ae9729e5c"
   },
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "XY = []\n",
    "with open(\"banana.csv\") as csvfile:\n",
    "    reader = csv.reader(csvfile, quoting=csv.QUOTE_NONNUMERIC) # change contents to floats\n",
    "    for row in reader: # each row is a list\n",
    "        XY.append(row)\n",
    "XY = np.array(XY)\n",
    "\n",
    "# Here we select a subset of the data. (remember, computation scales as N^3)\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(XY)\n",
    "N = 50\n",
    "X, Y = XY[:N,:-1], XY[:N,-1:]\n",
    "Y = (Y-1.5) * 2 # to be in {-1, 1}\n",
    "N = X.shape[0]\n",
    "\n",
    "# Plotting the data\n",
    "\n",
    "plt.scatter(XY[:,0], XY[:,1], c=XY[:,2], s=1, alpha=0.3)\n",
    "plt.scatter(X[:,0], X[:,1], c=Y)\n",
    "plt.xlabel('$x_1$', fontsize=15)\n",
    "plt.ylabel('$x_2$', fontsize=15)\n",
    "plt.title('Classification data', fontsize=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90e5f3f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "tVmiRlQm66-g",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e2439182eda55173bd0bb63c8a2abc1e",
     "grade": false,
     "grade_id": "cell-49f27e9ee8d9eac1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Task 2a**: To fully specify the model, we need the prior covariance $K_p = K_{\\bf ff}$ and its Cholesky factor $L_p = chol(K_{\\bf ff})$. As usual, the prior mean is zero. Your task here is to implement the standard RBF kernel, parametrized by the length scale $l$ and the kernel variance $\\sigma^2$. \n",
    "\n",
    "**Note:** The x variables are 2D, meaning that some implementation details are different from the previous assignment.\n",
    "\n",
    "**Hyperparameter optimization.** Note that in addition to approximating $p(f|y)$, the ELBO can (optionally) be used to optimize the kernel hyperparameters $\\{l,\\sigma^2\\}$ at the same time, because it is a lower bound to the marginal likelihood $p(y)$. To do this, the kernel has to be defined with Tensorflow functions to enable autodifferentiation and calculating gradients for the hyperparameters, which are defined as Tensorflow Variables. In addition, the two hyperparameters need to be parametrized so that they can not get negative values, which is achieved by optimizing their log-values instead (since, e.g., length scale should be in $[0,\\infty]$, log length scale is unconstrained in $[-\\infty,\\infty]$). We will try out optimizing the hyperparameters later on, which is why the implementation below takes the hyperparameters in log-space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1fcb3c",
   "metadata": {
    "deletable": false,
    "id": "IqSkvWnW66-h",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cbd877902c8991d87e643a741c01e65b",
     "grade": false,
     "grade_id": "cell-eac568648c1ddf5b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def create_se_kernel(X1, X2, log_ell, log_sigma_squared):\n",
    "    \"\"\"\n",
    "    Create the kernel matrix using the Squared Exponential kernel.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "    X1    -- NxD matrix.\n",
    "    X2    -- MxD matrix.\n",
    "    log_ell -- lengthscale value in log scale. \n",
    "    log_sigma_squared -- variance value in log scale. \n",
    "\n",
    "    Returns a NxM kernel matrix. \n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4421041-398f-41a1-98af-491f63952fce",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "174e8ee786712b07f805a556107d1d5f",
     "grade": false,
     "grade_id": "cell-032a975aa8227f5c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Let's test the above function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8b0b35-b1c5-407b-9e94-626736069aa3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f6d8bf59fd9d41f4b84fc0fba035ca68",
     "grade": false,
     "grade_id": "cell-52df60cf2cc5cff1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X1 = np.array([[ 1.30, -1.17], [ 0.34, -0.47], [ 0.07, -0.33]])\n",
    "X2 = np.array([[ 1.28,  1.08], [ 0.44, -1.15], [-0.71, -1.02]])\n",
    "log_ell_test = np.log(1.5)\n",
    "log_sigma_squared_test = np.log(2)\n",
    "\n",
    "assert create_se_kernel(X1, X2, log_ell_test, log_sigma_squared_test).shape == (3, 3)\n",
    "np.testing.assert_allclose(create_se_kernel(X1, X2, log_ell_test, log_sigma_squared_test), np.array([[0.649, 1.696, 0.810], \n",
    "                                                                                           [0.963, 1.800, 1.463], \n",
    "                                                                                           [0.928, 1.670 , 1.571]]), atol=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99855f00-b4b1-4f26-a0c8-8b4a9e4ab4ef",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "549ed48fc8e54a29338cd4affe53ec4c",
     "grade": false,
     "grade_id": "cell-83eb51166fc5a9b4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Do not move or delete the below block. It is used for automatic grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff688108-deb4-4708-ac9a-cc2c2a12d4a0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "296f50a4b5e44e9177965989a819fa87",
     "grade": true,
     "grade_id": "cell-50312b5f14a1720b",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f4d6523",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "qqktFNQO66-j",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b95df5f8a6c031eff08e96e5d7b3307e",
     "grade": false,
     "grade_id": "cell-ebb20bb8139c2861",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Optimizing the ELBO**\n",
    "\n",
    "Here we initialize the variational distribution to $q({\\bf f})={\\cal N}({\\bf f};0, I)$,\n",
    "then optimize it to maximize the ELBO using gradient based optimization. Gradient based optimization refers to optimization schemes where a function $f(\\theta)$ is optimized with respect to $\\theta$ by following the gradient $\\nabla_{\\theta} f(\\theta)$. For example, gradient descent constructs a sequence of values $\\theta_t$ following\n",
    "$$\\theta_{t+1 } = \\theta_t - \\eta \\nabla_{\\theta} f(\\theta)|_{\\theta=\\theta_t}$$\n",
    "where $\\eta$ is the learning rate. Note that while we want to maximize the ELBO, the standard optimization helpers provided by Tensorflow are designed to minimize functions. Thus, ELBO maximization is implemented as negative ELBO minimization. \n",
    "\n",
    "**Note on autodiff.** When using an automatic differentiation framework, one does not need to manually derive the gradient (hence the 'automatic'). As discussed earlier, these frameworks include Tensorflow, Jax and Pytorch.\n",
    "\n",
    "**Parameterization.** Note that the following code treats the $L_q$ parameters in a special way: Instead of optimizing over all of the $N\\times N$ matrix elements, we only optimize over the lower triangular part. In addition, the diagonal elements are constrained to be positive by parameterizing the diagonal in log-space. The reason for this is that Cholesky factors are lower triangular with positive elements on the diagonal, and without forcing these properties, some elements in the upper diagonal could also be optimized to non-zero values or some diagonal elements could become negative. Thus, some of the ELBO calculations would not be quite correct as they assume valid Cholesky factors. This parametrization is achieved by having the $L_q$ lower triangular part in a vector and using a \"bijector\" from the tensorflow_probability package to transform into a lower triangular matrix in a way that preserves gradients. In addition, and exp transform is applied to the diagonal elements to define the final $L_q$ matrix, forcing positivity. See https://www.tensorflow.org/probability/api_docs/python/tfp/bijectors/FillTriangular, https://www.tensorflow.org/probability/api_docs/python/tfp/bijectors/TransformDiagonal and https://www.tensorflow.org/probability/api_docs/python/tfp/bijectors/Bijector if you are interested to learn more.\n",
    "\n",
    "**Task 2b:** Try to get familiar with the optimization function below and fill out the missing details. Then try running the optimization for $q(\\mathbf f)$. At this stage, do not optimize hyperparameters yet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf735f2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "K2VBthKh66-k",
    "lines_to_next_cell": 2,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "35438ea37bad8f2fe0c40516b6513b0f",
     "grade": false,
     "grade_id": "cell-effec73beae3db80",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "d308dca4-4994-4456-f592-d592d75717db"
   },
   "outputs": [],
   "source": [
    "def optimize_elbo(ell, sigma_squared, optimize_hyperparams=False, print_progress=True):\n",
    "    \"\"\" Optimize the variational posterior q(f) and (optionally) the length scale and variance hyperparameters.\n",
    "    \n",
    "    Arguments:\n",
    "    ell -- The kernel effective length scale, or its initial value in case optimize_hyperparameters=True\n",
    "    sigma_squared -- The kernel variance, or its initial value in case optimize_hyperparameters=True\n",
    "    optimize_hyperparams -- Whether to optimize hyperparameters\n",
    "    print_progress -- Whether to print out ELBO values during optimization\n",
    "    \n",
    "    Returns:\n",
    "    m_q -- The mean vector of the optimized variational posterior\n",
    "    L_q -- the Cholesky factor L_q of the covariance of the optimized variational posterior\n",
    "    elbo_values -- The intermediate values of the ELBO during optimization\n",
    "    log_ell -- The log effective length scale after optimization\n",
    "    log_sigma_squared -- The log kernel variance after optimization\n",
    "    \"\"\"\n",
    "    \n",
    "    # Log-hyperparameters:\n",
    "    v_log_ell = tf.Variable(np.log(ell), dtype=tf.float64)\n",
    "    v_log_sigma_squared = tf.Variable(np.log(sigma_squared), dtype=tf.float64)\n",
    "\n",
    "    # You can change the optimizer or learning rate here\n",
    "    opt = tf.optimizers.Adam(learning_rate=0.001) \n",
    "\n",
    "    num_iterations = 30000 # You can change the number of steps here.\n",
    "    \n",
    "    # The prior mean and the Cholesky factor of the prior covariance\n",
    "    K_p = None  # TASK: Complete this\n",
    "    m_p = None  # TASK: Complete this\n",
    "    L_p = None  # TASK: Complete this\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    assert m_p.dtype == tf.float64\n",
    "    assert L_p.dtype == tf.float64\n",
    "    assert K_p.dtype == tf.float64\n",
    "    \n",
    "    # Initial distribution parameters m_q, L_q\n",
    "    m_q = tf.Variable(np.zeros((N, 1)), dtype=tf.float64)\n",
    "    # As Cholesky factors are lower triangular with positive diagonals, we want to parametrize them as such. \n",
    "    # Here we use tfp.bijectors.FillTriangular to map the trainable parameters in L_q_params\n",
    "    # to a lower-triangular matrix and tfp.bijectors.TransformDiagonal to map the diagonals to positive values.\n",
    "    # L_q is initialized at the identity matrix (note the exp transform for the diagonals)\n",
    "    # Could also initialize to np.linalg.cholesky(K_p) for potentially faster convergence.\n",
    "    L_q_params = tf.Variable(tfp.math.fill_triangular_inverse(np.zeros((N,N))), dtype=tf.float64)\n",
    "    filltriangular = tfp.bijectors.FillTriangular(upper=False)\n",
    "    positivediagonal = tfp.bijectors.TransformDiagonal(diag_bijector=tfp.bijectors.Exp())\n",
    "    transform = lambda x: positivediagonal(filltriangular(x))\n",
    "    \n",
    "    # Loss function if we don't do hyperparameter optimization\n",
    "    loss_q = lambda: - elbo(m_p, L_p, m_q, transform(L_q_params), Y)\n",
    "    # Loss function for hyperparameter optimization (the prior Cholesky factor changes at each step)\n",
    "    loss_hp = lambda: - elbo(m_p, tf.linalg.cholesky(create_se_kernel(X, X, v_log_ell, v_log_sigma_squared)), m_q, transform(L_q_params), Y)\n",
    "\n",
    "    # definition of a training step\n",
    "    def create_train_step(loss, opt, trainable_variables):\n",
    "        @tf.function  # this makes the optimisation loop much faster, but restricts what operations you can do within your code\n",
    "        def train_step():\n",
    "            with tf.GradientTape() as tape:\n",
    "                tape.watch(trainable_variables)\n",
    "                loss_value = loss()\n",
    "            gradients = tape.gradient(loss_value, trainable_variables)\n",
    "            opt.apply_gradients(zip(gradients, trainable_variables))\n",
    "            return loss_value\n",
    "        return train_step\n",
    "\n",
    "    loss = loss_hp if optimize_hyperparams else loss_q\n",
    "    trainable_variables = [m_q, L_q_params, v_log_ell, v_log_sigma_squared] if optimize_hyperparams else [m_q, L_q_params]\n",
    "    train_step = create_train_step(loss, opt, trainable_variables)\n",
    "    neg_elbo_values = []\n",
    "    # running the optimization\n",
    "    for t in range(num_iterations):\n",
    "        neg_elbo_value = train_step()\n",
    "        neg_elbo_values.append(neg_elbo_value)\n",
    "        if t % 5000 == 0 and print_progress:\n",
    "            print(t, loss().numpy())\n",
    "    L_q = transform(L_q_params)\n",
    "    return m_q, L_q, neg_elbo_values, v_log_ell, v_log_sigma_squared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30453dc5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "86cf09ad1be178965e5329802f235fef",
     "grade": false,
     "grade_id": "cell-14bc96e903b894d4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Run the model.** Next we run the model with the hyperparameters provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812e6751",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d4d5bc2e3cb9b3b3ff70af887874989c",
     "grade": false,
     "grade_id": "cell-9bb3e8af27c95c75",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# The code is complete and just needs to be executed.\n",
    "if not automatic_grading:\n",
    "    ell = 0.5  # the length scale\n",
    "    sigma_squared = 3.0**2  # the kernel variance\n",
    "    m_q, L_q, neg_elbo_values, log_ell, log_sigma_squared = optimize_elbo(ell, sigma_squared, False)\n",
    "    print(f\"Final Optimized ELBO value: {neg_elbo_values[-1]}\")\n",
    "    assert neg_elbo_values[0] > neg_elbo_values[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4d9ed4-ae70-4589-9e3b-dc2696df2220",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bb262274e6f5378aef0d824d9f3376cd",
     "grade": false,
     "grade_id": "cell-73d08ac35e8127de",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We also plot the evolution of the ELBO as a function of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9aa3aa-b100-4fc1-a170-bf2bc1214219",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c0a88d99b1ebc3d04ee15594a129507f",
     "grade": false,
     "grade_id": "cell-753bae9a2d7dd82d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "if not automatic_grading:\n",
    "    plt.plot(neg_elbo_values)\n",
    "    plt.title(\"negative ELBO\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d45f73",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "t3ZlPddLxMXT",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "35022da881246ce8573a2ef5b4615b7d",
     "grade": false,
     "grade_id": "cell-c83640bfc559565a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 3: Posterior prediction for new data points (2.5 points)\n",
    "\n",
    "Under the hood, the algorithm defines a posterior process for all values of the input space.\n",
    "\n",
    "For a new input $x^*$, the posterior prediction is given by \n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "q(f(x^*)) &= \\int p(f(x^*)|{\\bf f})q({\\bf f})d{\\bf f}\\\\\n",
    " &= {\\cal N}(f(x^*)| K_{f^*{\\bf f} }K_{{\\bf ff}}^{-1} m_q,\n",
    " K_{f^*f^*} - K_{f^*{\\bf f}}K_{{\\bf ff}}^{-1}(K_{{\\bf ff}} - S)K_{{\\bf ff}}^{-1}K_{{\\bf f} f^*})\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "where $S$ is the optimized covariance and $\\mathbf m_q$ is the optimized mean of $q(\\mathbf f)$. Implement this function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b587ee0d-758a-437f-8071-069a6dc57434",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "30bf03e40318a257d3766741b1821c3a",
     "grade": false,
     "grade_id": "cell-2f869fb0c2233cf6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# create new input points on grid\n",
    "n_grid = 100\n",
    "x = np.linspace(XY[:,0].min(), XY[:,1].max(), n_grid)\n",
    "X1new, X2new = np.meshgrid(x, x)\n",
    "Xnew = np.hstack([X1new.reshape(-1,1), X2new.reshape(-1,1)])  # size : n_grid * n_grid x 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc2a5a3-dbf4-49cb-bbab-78867cb090ff",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "adf29e759090431ce04399a9f04f1fca",
     "grade": false,
     "grade_id": "cell-5279f0d70385c9c2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Task 3a:** We should avoid computing the full N_new × N_new covariance matrix $K_{f^{*}f^{*}}$ due to memory limitations. You only need the diagonal values of this matrix for the predictive variances. Implement a function that calculates this. Think of a way of implementing this without using the `create_se_kernel` function.\n",
    "\n",
    "**Hint:** Notice that we don't need the value of $\\ell$ in this function.\n",
    "\n",
    "**Note:** We only calculate the diagonal kernel matrix for the new points *i.e.* $K_{f^{*}f^{*}}$ as $X^{*}$ are high in number as compared to $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e6b58d-44be-49a6-8bf5-737a6984fa88",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7e9a03ec4b193f1a08337a45a0085abe",
     "grade": false,
     "grade_id": "cell-b04c295b3264104f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def create_se_kernel_diag(X, log_sigma_squared):\n",
    "    \"\"\"\n",
    "    Create the diagonal of the kernel matrix using the Squared Exponential kernel.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "    X    -- NxD matrix.\n",
    "    log_sigma_squared -- variance value in log scale. \n",
    "\n",
    "    Returns a Nx1 diagonal of the kernel matrix. \n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ac21a9-a97e-49c4-bf6a-561537d09862",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "60fceda60d52271f1299bf65a0b01f1a",
     "grade": false,
     "grade_id": "cell-1678802edd0b61c2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Let's test the above function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f4c1e3-2692-42bd-ac70-cc4f67de1480",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c409dab0e3126e55e066108647245424",
     "grade": false,
     "grade_id": "cell-a9275dee006c5954",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X1 = np.array([[ 1.30, -1.17], [ 0.34, -0.47], [ 0.07, -0.33]])\n",
    "log_sigma_squared = np.log(2)\n",
    "\n",
    "assert create_se_kernel_diag(X1, log_sigma_squared).shape == (3, 1)\n",
    "np.testing.assert_allclose(create_se_kernel_diag(X1, log_sigma_squared), 2 * np.ones((3, 1)), atol=1e-3) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448853e7-71c4-4d4f-90ec-974d5e50e60f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c088d6bc844d651b51b11b0058f09951",
     "grade": false,
     "grade_id": "cell-a6274f6d67c04a89",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Do not move or delete the below block. It is used for automatic grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a6a528-c43d-4cf5-b049-e10e033b1c7d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bb7630d6d7f774a956c53fcb86c612cf",
     "grade": true,
     "grade_id": "cell-30833afeb795a415",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24b723b2-7488-427d-b7e5-e7cc3e629137",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f029253c47f5655d4b8bf33de4b662f7",
     "grade": false,
     "grade_id": "cell-211c134964f5b68b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Task 3b**: Implement the posterior prediction for new points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8131dce5",
   "metadata": {
    "deletable": false,
    "id": "nIrjqfJNxKDv",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b68126c1df9466d964a0b30ebb58c745",
     "grade": false,
     "grade_id": "cell-07c381195ee389f8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def posterior_marginal_prediction(X_new, X, m_q, L_q, log_ell, log_sigma_squared):\n",
    "    \"\"\" compute the posterior marginal predictions q(f(x*)) independently for all inputs in X_new \n",
    "    \n",
    "    Note: You need to use tensorflow functions.\n",
    "    \n",
    "    arguments:\n",
    "    X_new -- N_new × 2, matrix of new inputs\n",
    "    X     -- N × 2, matrix of training inputs\n",
    "    m_q   -- N × 1, mean vector of q\n",
    "    L_q   -- N × N, Cholesky factor of the covariances of q\n",
    "    log_ell -- log-length-scale of the kernel\n",
    "    log_sigmasquared -- log-variance of the kernel\n",
    "    returns: predictive marginal means and variances (both with size N_new × 1)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return m_new, v_new\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c30b8bb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5901c1ae1807dc090e8c8e412f9879c3",
     "grade": false,
     "grade_id": "cell-adac5a7b895a313a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Let's test the above function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e62414",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8afd0eac4b2e2e6f2b26747ccdd14241",
     "grade": false,
     "grade_id": "cell-923805020b2feeee",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test when X_new == X. \n",
    "\n",
    "x_test = np.random.randn(5, 1)\n",
    "m_q_test = np.random.randn(5, 1)\n",
    "L_q_test = np.random.randn(5, 5)\n",
    "log_ell_test = np.log(1.2)\n",
    "log_sigma_squared_test = np.log(0.4)\n",
    "\n",
    "pred_mean_test, pred_var_test = posterior_marginal_prediction(x_test, x_test, m_q_test, L_q_test, log_ell_test, log_sigma_squared_test)\n",
    "np.testing.assert_allclose(pred_mean_test, m_q_test)\n",
    "S_diag_test = np.diag(L_q_test @ np.transpose(L_q_test))[:, None]\n",
    "np.testing.assert_allclose(pred_var_test, S_diag_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa1f00b-509d-49e2-83d9-c297c9d108b4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a4cb073581b0673a92d48c0d2a26154a",
     "grade": false,
     "grade_id": "cell-dcdbc32c84c05008",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Do not move or delete the below block. It is used for automatic grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90b8036-627a-4515-9ba7-739fdf6de016",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2325f406244ab0e9615de5d9790f430f",
     "grade": true,
     "grade_id": "cell-db3680431aee8891",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db36ce3b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "sfGR_5yG64zo",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0fe9fb4c7fd12e73420d6b8f1f51b995",
     "grade": false,
     "grade_id": "cell-7e0dd2e8fb9b28b9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### For the likelihood\n",
    "$$ p(y_i|f(\\cdot),{\\bf x}_i) = p(y_i|f_i) = \\phi(y_i * f_i),$$\n",
    "where $\\phi$ is the cdf of the standard normal distribution, the integral $ p(y^*=1 | {\\bf y}) \\approx \\int p(y^*=1 | f^*) q(f^*) \\,{\\rm d}f^* $\n",
    "can be solved analytically for Gaussian $q(f^*)$. For $q(f^*) = \\mathcal{N}(\\mu, \\sigma^2)$, we get\n",
    "$$ \\int p(y^* | f^*) q(f^*) \\,{\\rm d}f^* = \\phi(\\mu / \\sqrt{1 + \\sigma^2}) .$$\n",
    "\n",
    "**Task 3c**: Fill out the implementation of the $y$ output probability given the corresponding GP mean and variance in the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7f36bb-4ec8-46f4-82c6-3bebf997ad3b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d849f2bb14ad1406b8c0064e55e851bb",
     "grade": false,
     "grade_id": "cell-f9be25105e7b83ef",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def ynew_probability(m, v):\n",
    "    \"\"\"\n",
    "    return the output probability when the GP mean (m) and variance (v) are given. \n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return y_new\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a140c3b-738c-4ea9-bfb2-5c2f42263734",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "defb537b5ec8bdcaee3fb4bebf390a47",
     "grade": false,
     "grade_id": "cell-1d132cebc42e7963",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Let's test the above function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99df6cb9-2e71-4a1d-93eb-aa64c0ae22d0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dc18180ee321ebaa262fa1139fbccd92",
     "grade": false,
     "grade_id": "cell-7b7ba4cb490370e6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "posterior_m_test = np.array([[0.388], [0.351]])\n",
    "posterior_S_test = np.array([[8.084], [19.940]])\n",
    "\n",
    "expected_y_prob = np.array([[0.5511], [0.5305]])\n",
    "y_prob = ynew_probability(posterior_m_test, posterior_S_test)\n",
    "\n",
    "assert np.allclose(y_prob, expected_y_prob, rtol=1e-4, atol=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7244a3b2-bd6c-49a2-934a-d119856fc7af",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9dc1e460a821dcc0c9e656afb4504ecc",
     "grade": false,
     "grade_id": "cell-dd2999836795cdee",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Do not move or delete the below block. It is used for automatic grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4bd36f-bfae-492c-b0f8-21f4b2c24256",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f9304c0ea41b564bdc5b1a2fe1690d5f",
     "grade": true,
     "grade_id": "cell-f2ad5f2648966006",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d85ee9f5-31ab-4363-b1c8-22b8d8694674",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "59085eaa2d62409bf8823a668be46b81",
     "grade": false,
     "grade_id": "cell-f9690d4db3c70af7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let us plot the posterior now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abe9ba2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "deletable": false,
    "editable": false,
    "id": "4bXbpwLP3Zcv",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "07d769bda930de84597b2cd93e54be70",
     "grade": false,
     "grade_id": "cell-babbc4b1f670b7f0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "a5446e44-ab6d-49b7-babf-d1df3b83faa9"
   },
   "outputs": [],
   "source": [
    "### This function is complete, so nothing needs to be done here.\n",
    "def plot_posterior(m_new, v_new):\n",
    "    \"\"\"\n",
    "    Plots the posterior prediction for the grid in Xnew (global variable)\n",
    "    \n",
    "    arguments:\n",
    "    m_new -- mean at Xnew\n",
    "    v_new -- variance at Xnew\n",
    "    \"\"\"\n",
    "    y_new = ynew_probability(m_new, v_new)\n",
    "    \n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.contour(x, x, m_new.numpy().reshape(n_grid,n_grid),\n",
    "                cmap=plt.cm.RdBu_r, vmin=-np.abs(m_new).max(), vmax=np.abs(m_new).max())\n",
    "    plt.colorbar()\n",
    "    plt.title(\"Posterior mean\")\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.contour(x, x, v_new.numpy().reshape(n_grid,n_grid))\n",
    "    plt.colorbar()\n",
    "    plt.title(\"Posterior variance\")\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.contour(x, x, y_new.numpy().reshape(n_grid,n_grid),\n",
    "                cmap=plt.cm.RdBu_r, vmin=0, vmax=1)\n",
    "    plt.colorbar()\n",
    "    plt.title(\"p(y|f)\")\n",
    "    plt.scatter(X[:,0], X[:,1], c=Y)  # training points\n",
    "\n",
    "    plt.xlim(x.min(), x.max())\n",
    "    plt.ylim(x.min(), x.max())\n",
    "\n",
    "    plt.scatter(XY[:,0], XY[:,1], c=XY[:,2], s=1, alpha=0.3)  # all data points\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810b1e92",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "deletable": false,
    "editable": false,
    "id": "4bXbpwLP3Zcv",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5314e2c2a666ec678b6d02e8f85b3167",
     "grade": false,
     "grade_id": "cell-dfe0ad9548293215",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "a5446e44-ab6d-49b7-babf-d1df3b83faa9"
   },
   "outputs": [],
   "source": [
    "if not automatic_grading:\n",
    "    m_new, v_new = posterior_marginal_prediction(Xnew, X, m_q, L_q, log_ell, log_sigma_squared)\n",
    "    plot_posterior(m_new, v_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50801790",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7cce6b4b5f2b0fcb8094a1d6b5b2f7aa",
     "grade": false,
     "grade_id": "cell-3e11b934e178613f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Task 3d:** Optimize the kernel's hyperparameters, too, by setting `optimize_hyperparameters=True`. Plot the posterior obtained with the optimized hyperparameters. Initialize the hyperparameters with $\\ell=0.5$ and $\\sigma^2=4$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f112e8-1390-49d8-8962-994c425c275f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3d0943bcefa7e03bb15afa9d74391a95",
     "grade": false,
     "grade_id": "cell-c0f3989629e7103b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# NOTE: Write your code inside this if condition.\n",
    "if not automatic_grading:\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f85eef-edbc-40d9-877c-bfa34300399b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cf8a49de39585d57264be592890ac581",
     "grade": false,
     "grade_id": "cell-edde59ba78c31ea5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Task 3e:** Does optimizing the hyperparameters give a better ELBO value? Explain!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66df2ce8-cb88-4508-9dc5-c85fceb51dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_3e = None  # TASK: Replace None with True or False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a22d48-0062-4aa0-b78f-15894906849e",
   "metadata": {},
   "source": [
    "**Explanation:** {Write your answer here}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45bdfb6-01bd-44f4-98db-f362bd84d156",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dafea453d04db237ba5779732a974ed2",
     "grade": false,
     "grade_id": "cell-60792d7b513ee519",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(a_3e, bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abed5c69-1124-4a9b-97ef-e667de0b454a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a8a2566bb625871070603bf3d1c36413",
     "grade": false,
     "grade_id": "cell-108d3161f58b70f5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Do not move or delete the below block. It is used for automatic grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbc479c-cc6c-437e-a861-f25fa25fc78a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c24cf570d4af4fb7e444380cdf0b5e77",
     "grade": true,
     "grade_id": "cell-cfb50951b52bd529",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5771b6c-6d2d-4a7f-871a-f4b8e90feacc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e1a7f2cdacf9f8885cfca6afd829f656",
     "grade": false,
     "grade_id": "cell-b844331f44e51850",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 4: After answering all the questions, kindly set `automatic_grading=True` at the top of the notebook, use the validate option in the toolbar to validate the notebook and be sure that there are no errors."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
